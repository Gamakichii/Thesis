{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE: user_reports_data builder has been moved to follow the Firestore fetch cell.\n",
        "# Run the Firestore fetch cell (which populates `df_user`) first, then run the builder cell located immediately after it.\n",
        "print('See the \"Fetch and Process User Reports\" cell and the subsequent builder cell for `user_reports_data`.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auto-split post IDs into train_post_ids and test_post_ids (stratified by majority post label)\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_post_ids = []\n",
        "test_post_ids = []\n",
        "\n",
        "if 'user_reports_data' in globals() and len(user_reports_data) > 0:\n",
        "    post_to_labels = {}\n",
        "    for r in user_reports_data:\n",
        "        pid = r.get('postId')\n",
        "        if not pid:\n",
        "            continue\n",
        "        post_to_labels.setdefault(pid, []).append(int(r.get('true_label', 0)))\n",
        "\n",
        "    post_ids = []\n",
        "    labels = []\n",
        "    for pid, labs in post_to_labels.items():\n",
        "        # majority label for the post\n",
        "        lab = int(round(sum(labs) / len(labs)))\n",
        "        post_ids.append(pid)\n",
        "        labels.append(lab)\n",
        "\n",
        "    print(f'Found {len(post_ids)} unique posts with labels. Label distribution:', Counter(labels))\n",
        "\n",
        "    if len(post_ids) < 2:\n",
        "        print('Not enough posts to split; assigning all to train.')\n",
        "        train_post_ids = post_ids\n",
        "        test_post_ids = []\n",
        "    else:\n",
        "        try:\n",
        "            train_post_ids, test_post_ids = train_test_split(post_ids, test_size=0.2, stratify=labels, random_state=SEED)\n",
        "        except Exception:\n",
        "            # fallback to unstratified split\n",
        "            train_post_ids, test_post_ids = train_test_split(post_ids, test_size=0.2, random_state=SEED)\n",
        "\n",
        "    # show splits\n",
        "    def label_counts(ids):\n",
        "        cnt = Counter()\n",
        "        for pid in ids:\n",
        "            labs = post_to_labels.get(pid, [])\n",
        "            if labs:\n",
        "                cnt.update([int(round(sum(labs)/len(labs)))])\n",
        "        return dict(cnt)\n",
        "\n",
        "    print('Train posts:', len(train_post_ids), 'label dist:', label_counts(train_post_ids))\n",
        "    print('Test posts :', len(test_post_ids), 'label dist:', label_counts(test_post_ids))\n",
        "\n",
        "else:\n",
        "    print('user_reports_data not found or empty; cannot auto-split post IDs')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig_DMomfTLNT"
      },
      "source": [
        "-----\n",
        "\n",
        "### **AE (with Fine-Tuning) + GCN Training Notebook**\n",
        "  * **Part 1**: A Deep Autoencoder is **pre-trained** on a large public dataset and then **fine-tuned** using user-reported benign URLs from Firestore.\n",
        "  * **Part 2**: The Graph Convolutional Network (GCN) is trained on your user reports from Firestore to learn structural patterns.\n",
        "  * **Part 3**: The fine-tuned AE and the trained GCN are fused and evaluated.\n",
        "  * **Part 4**: All necessary artifacts for the backend API are exported.\n",
        "-----\n",
        "\n",
        "#### **1. Setup and Installations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPzzxLHETLNX",
        "outputId": "1b3c51b2-7b85-4c60-b3bb-8453c77491ac"
      },
      "outputs": [],
      "source": [
        "# Install core data science and cloud libraries\n",
        "!pip -q install pandas scikit-learn==1.7.1 tldextract google-cloud-firestore\n",
        "\n",
        "# Install TensorFlow for the Autoencoder\n",
        "!pip -q install tensorflow==2.19.0 tf-keras==2.19.0 \"protobuf>=5.26.1\"\n",
        "\n",
        "# Install PyTorch and PyTorch Geometric for the GNN\n",
        "!pip -q install torch==2.3.1 torchvision==0.18.1 --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip -q install torch_geometric -f https://data.pyg.org/whl/torch-2.3.1+cpu.html\n",
        "\n",
        "# Install visualization libraries\n",
        "!pip -q install matplotlib seaborn\n",
        "\n",
        "print(\"✅ All libraries installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skg8ndKbTLNY"
      },
      "source": [
        "-----\n",
        "\n",
        "#### **2. Imports and Initial Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aekWiVDvTLNY",
        "outputId": "9611081a-73ff-4a69-e404-979f742bc9e3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tldextract\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ML/DL Frameworks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "# Scikit-learn for preprocessing and metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Google Cloud and Colab\n",
        "from google.colab import files\n",
        "from google.cloud import firestore\n",
        "\n",
        "# --- Reproducibility ---\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkaS3tMQTLNY"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Part 1: Autoencoder Pre-Training and Fine-Tuning**\n",
        "\n",
        "This part now includes both the initial training on public data and the specialized fine-tuning on user's data.\n",
        "\n",
        "#### **3. Load Public Dataset and Authenticate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "SL27DlPsTLNZ",
        "outputId": "0a04d98a-9756-495c-db2a-2369d838b066"
      },
      "outputs": [],
      "source": [
        "# The public dataset contains 111 lexical features extracted from URLs\n",
        "URL = \"https://raw.githubusercontent.com/GregaVrbancic/Phishing-Dataset/master/dataset_full.csv\"\n",
        "print(\"Downloading public dataset...\")\n",
        "df_base = pd.read_csv(URL)\n",
        "\n",
        "# Separate features (X) from the label (y)\n",
        "y_public = df_base[\"phishing\"].astype(int).values\n",
        "X_public = df_base.drop(columns=[\"phishing\"]).astype(np.float32)\n",
        "feature_cols = X_public.columns.tolist()\n",
        "\n",
        "print(f\"Dataset loaded with {X_public.shape[0]} samples and {X_public.shape[1]} features.\")\n",
        "\n",
        "# Split public data for pre-training and getting a baseline\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_public, y_public, test_size=0.30, stratify=y_public, random_state=42)\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_tr, y_tr, test_size=0.20, stratify=y_tr, random_state=42)\n",
        "\n",
        "# --- Feature Scaling ---\n",
        "# Fit the scaler ONLY on benign training data to learn the \"normal\" distribution\n",
        "# Toggle between StandardScaler (for reproducibility per thesis) and RobustScaler (recommended)\n",
        "import os\n",
        "use_standard_scaler = os.environ.get('STANDARD_SCALER', '0').lower() in ('1','true','yes')\n",
        "if use_standard_scaler:\n",
        "    print('Using StandardScaler as requested for thesis reproducibility')\n",
        "    scaler = StandardScaler().fit(X_tr[y_tr==0])\n",
        "    scaler_type = 'StandardScaler'\n",
        "else:\n",
        "    print('Using RobustScaler for improved robustness to outliers')\n",
        "    scaler = RobustScaler().fit(X_tr[y_tr==0])\n",
        "    scaler_type = 'RobustScaler'\n",
        "\n",
        "# Transform all datasets using this scaler\n",
        "X_tr_s = scaler.transform(X_tr)\n",
        "X_val_s = scaler.transform(X_val)\n",
        "X_te_s = scaler.transform(X_te)\n",
        "print('Scaler type:', scaler_type)\n",
        "\n",
        "print(\"Data successfully split and scaled.\")\n",
        "\n",
        "# --- Firebase Authentication ---\n",
        "print(\"\\nPlease upload your Firebase service account JSON key file.\")\n",
        "uploaded = files.upload()\n",
        "sa_path = next(iter(uploaded.keys()))\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
        "print(\"\\n✅ Firebase authentication configured.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Optional: Add external phishing dataset (Mendeley DOI) ---\n",
        "# Tamal, Maruf (2023), \"Phishing Detection Dataset\", Mendeley Data, V1, doi: 10.17632/6tm2d6sz7p.1\n",
        "import requests, zipfile, io, os\n",
        "\n",
        "print('Attempting to fetch Mendeley dataset via DOI. If automated download fails, please manually download the dataset and upload it to Colab.')\n",
        "\n",
        "def try_download_doi(doi, dest_dir='external_dataset'):\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "    doi_url = f'https://doi.org/{doi}'\n",
        "    try:\n",
        "        r = requests.head(doi_url, allow_redirects=True, timeout=10)\n",
        "        final = r.url\n",
        "        print('Resolved DOI to:', final)\n",
        "        # If final is a zip or direct file, download\n",
        "        if final.endswith('.zip') or 'mendeley' in final:\n",
        "            print('Downloading archive...')\n",
        "            rr = requests.get(final, stream=True, timeout=30)\n",
        "            rr.raise_for_status()\n",
        "            z = zipfile.ZipFile(io.BytesIO(rr.content))\n",
        "            z.extractall(dest_dir)\n",
        "            print('Extracted to', dest_dir)\n",
        "            return dest_dir\n",
        "        else:\n",
        "            # Not direct; fallback: ask user to download manually\n",
        "            print('DOI did not resolve to a direct file. Please download the dataset from the Mendeley Data page and upload it here.')\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print('Automated DOI fetch failed:', e)\n",
        "        return None\n",
        "\n",
        "# Try to download; if it fails, prompt the user to upload a dataset file\n",
        "external_dir = try_download_doi('10.17632/6tm2d6sz7p.1')\n",
        "if external_dir is None:\n",
        "    print('\\nIf you have the dataset ZIP/CSV file, upload it now via the Files UI or using:')\n",
        "    print(\"from google.colab import files; files.upload()\")\n",
        "else:\n",
        "    # Discover CSV files in the extracted directory\n",
        "    csv_files = []\n",
        "    for root, dirs, files in os.walk(external_dir):\n",
        "        for f in files:\n",
        "            if f.lower().endswith('.csv'):\n",
        "                csv_files.append(os.path.join(root, f))\n",
        "    print('Found CSV files:', csv_files)\n",
        "    if csv_files:\n",
        "        # Load first CSV and try to integrate\n",
        "        df_ext = pd.read_csv(csv_files[0])\n",
        "        print('External dataset shape:', df_ext.shape)\n",
        "        # If df_ext contains a URL column, compute features and append to df_base\n",
        "        url_cols = [c for c in df_ext.columns if 'url' in c.lower()]\n",
        "        if url_cols:\n",
        "            print('Detected URL column(s):', url_cols)\n",
        "            urls = df_ext[url_cols[0]].astype(str).tolist()\n",
        "            feats_df = pd.DataFrame([features_for_url(u) for u in urls], columns=feature_cols)\n",
        "            # If label exists in df_ext, attach it\n",
        "            label_col = None\n",
        "            for c in df_ext.columns:\n",
        "                if c.lower() in ('phishing','label','is_phishing'):\n",
        "                    label_col = c\n",
        "                    break\n",
        "            if label_col is not None:\n",
        "                feats_df['phishing'] = df_ext[label_col].astype(int).values\n",
        "            else:\n",
        "                feats_df['phishing'] = 0\n",
        "            # Merge into df_base (append)\n",
        "            df_base = pd.concat([df_base, feats_df], ignore_index=True)\n",
        "            print('Appended external dataset; new df_base shape:', df_base.shape)\n",
        "        else:\n",
        "            print('No URL column detected in external CSV; please preprocess externally to provide URLs or feature columns.')\n",
        "\n",
        "print('External dataset integration step complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0xGh7v7TLNZ"
      },
      "source": [
        "-----\n",
        "\n",
        "#### **4. Define and Pre-train the Autoencoder**\n",
        "\n",
        "We train the AE on the large public dataset to give it a strong general foundation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAW6CNiDTLNZ",
        "outputId": "44e10b85-8eaf-46d2-90f7-1ea9be7333da"
      },
      "outputs": [],
      "source": [
        "# Autoencoder hyperparameters (from thesis)\n",
        "AE_LAYER1 = 64\n",
        "AE_LAYER2 = 32\n",
        "AE_BOTTLENECK = 16\n",
        "AE_DROPOUT = 0.1\n",
        "LR_PRETRAIN = 1e-3\n",
        "BATCH_PRETRAIN = 512\n",
        "EPOCHS_PRETRAIN = 30\n",
        "LR_FINETUNE = 1e-5\n",
        "BATCH_FINETUNE = 16\n",
        "EPOCHS_FINETUNE = 50\n",
        "\n",
        "# Define the Autoencoder architecture (as in thesis)\n",
        "inp = keras.Input(shape=(X_tr_s.shape[1],))\n",
        "x = keras.layers.Dense(AE_LAYER1, activation='relu')(inp)\n",
        "x = keras.layers.Dropout(AE_DROPOUT)(x)\n",
        "x = keras.layers.Dense(AE_LAYER2, activation='relu')(x)\n",
        "z = keras.layers.Dense(AE_BOTTLENECK, activation='relu')(x)  # Bottleneck\n",
        "x = keras.layers.Dense(AE_LAYER2, activation='relu')(z)\n",
        "x = keras.layers.Dense(AE_LAYER1, activation='relu')(x)\n",
        "out = keras.layers.Dense(X_tr_s.shape[1])(x)\n",
        "\n",
        "ae_model = keras.Model(inp, out)\n",
        "ae_model.compile(optimizer=keras.optimizers.Adam(LR_PRETRAIN), loss='mse')\n",
        "\n",
        "print('--- Step 1: Pre-training Autoencoder on Public Dataset ---')\n",
        "history = ae_model.fit(\n",
        "    X_tr_s[y_tr == 0], X_tr_s[y_tr == 0],\n",
        "    validation_data=(X_val_s[y_val == 0], X_val_s[y_val == 0]),\n",
        "    epochs=EPOCHS_PRETRAIN,\n",
        "    batch_size=BATCH_PRETRAIN,\n",
        "    verbose=1,\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "print('\\n✅ Pre-training complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ez6FCABYmNo"
      },
      "source": [
        "# 4.5. Pre-train Evaluation\n",
        "\n",
        "This new cell evaluates the performance of the Autoencoder before it has been fine-tuned. This gives us a baseline to measure improvement against."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Updated Pre-train Evaluation (uses current preprocessing + AE->prob mapping) ---\n",
        "print(\"\\n--- Evaluating Pre-trained Autoencoder Performance (using current preprocessing) ---\")\n",
        "\n",
        "# Reconstruct benign portion of public validation using current scaler / features\n",
        "pretrain_val_reconstructed = ae_model.predict(X_val_s[y_val == 0], verbose=0)\n",
        "pretrain_val_error = np.mean((pretrain_val_reconstructed - X_val_s[y_val == 0])**2, axis=1)\n",
        "\n",
        "# Default pretrain threshold: 99th percentile of benign validation errors (keeps high precision)\n",
        "pretrain_ae_threshold = float(np.percentile(pretrain_val_error, 99.0))\n",
        "\n",
        "# Evaluate on the public test set (transform already computed earlier: X_te_s)\n",
        "test_reconstructed = ae_model.predict(X_te_s, verbose=0)\n",
        "test_error = np.mean((test_reconstructed - X_te_s)**2, axis=1)\n",
        "\n",
        "# Convert errors -> AE probability consistently with later code\n",
        "ae_prob_test = np.clip((test_error - pretrain_ae_threshold * 0.5) / (pretrain_ae_threshold * 2), 0, 1)\n",
        "\n",
        "# Binary predictions using the AE probability threshold of 0.5\n",
        "y_pred_pretrain = (ae_prob_test > 0.5).astype(int)\n",
        "\n",
        "# Define local print_stats (ensures availability regardless of cell order)\n",
        "def print_stats(model_name, y_true, y_pred):\n",
        "    stats = {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
        "    }\n",
        "    print(f\"\\n--- {model_name} ---\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"  {key.capitalize():<18}: {value if key == 'confusion_matrix' else f'{value:.4f}'}\")\n",
        "    return stats\n",
        "\n",
        "# Print metrics\n",
        "pretrain_stats = print_stats(\"Pre-trained AE (Baseline on Public Data)\", y_te, y_pred_pretrain)\n",
        "\n",
        "# Also report ROC/PR AUC on continuous AE score for diagnostics\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
        "try:\n",
        "    roc = roc_auc_score(y_te, test_error)  # higher error => positive\n",
        "except Exception:\n",
        "    roc = None\n",
        "prec, rec, _ = precision_recall_curve(y_te, test_error)\n",
        "pr_auc = auc(rec, prec)\n",
        "print(f\"ROC AUC (error as score): {roc}, PR AUC: {pr_auc:.4f}\")\n",
        "\n",
        "# Save the pretrain threshold for reference (optional)\n",
        "try:\n",
        "    with open('pretrain_ae_threshold.txt', 'w') as f:\n",
        "        f.write(str(pretrain_ae_threshold))\n",
        "    print('Saved pretrain_ae_threshold.txt')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print('\\nPretrain threshold (99th percentile benign val) =', pretrain_ae_threshold)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHJ0rwkyYmh3",
        "outputId": "ad010c0d-6755-4c33-e5df-86efbcfc6250"
      },
      "outputs": [],
      "source": [
        "# --- Pre-train Evaluation ---\n",
        "print(\"\\n--- Evaluating Pre-trained Autoencoder Performance ---\")\n",
        "\n",
        "# Determine a temporary threshold based on the pre-trained model\n",
        "pretrain_val_reconstructed = ae_model.predict(X_val_s[y_val==0], verbose=0)\n",
        "pretrain_val_error = np.mean((pretrain_val_reconstructed - X_val_s[y_val==0])**2, axis=1)\n",
        "pretrain_ae_threshold = float(np.percentile(pretrain_val_error, 99.0))\n",
        "\n",
        "# Evaluate on the public test set\n",
        "test_reconstructed = ae_model.predict(X_te_s, verbose=0)\n",
        "test_error = np.mean((test_reconstructed - X_te_s)**2, axis=1)\n",
        "y_pred_pretrain = (test_error > pretrain_ae_threshold).astype(int)\n",
        "\n",
        "# Define the evaluation function\n",
        "def print_stats(model_name, y_true, y_pred):\n",
        "    stats = {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
        "    }\n",
        "    print(f\"\\n--- {model_name} ---\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"  {key.capitalize():<18}: {value if key == 'confusion_matrix' else f'{value:.4f}'}\")\n",
        "    return stats\n",
        "\n",
        "# Print the pre-train stats\n",
        "pretrain_stats = print_stats(\"Pre-trained AE (Baseline on Public Data)\", y_te, y_pred_pretrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Override pretrain AE threshold to 95th percentile (was 99th percentile) — quick fix per request\n",
        "import numpy as np\n",
        "try:\n",
        "    if 'pretrain_val_error' in globals():\n",
        "        pretrain_ae_threshold = float(np.percentile(pretrain_val_error, 95.0))\n",
        "    else:\n",
        "        # compute from current AE and public benign validation if available\n",
        "        if 'ae_model' in globals() and 'X_val_s' in globals():\n",
        "            try:\n",
        "                recon = ae_model.predict(X_val_s[y_val==0], verbose=0)\n",
        "                errors = np.mean((recon - X_val_s[y_val==0])**2, axis=1)\n",
        "                pretrain_ae_threshold = float(np.percentile(errors, 95.0))\n",
        "            except Exception as e:\n",
        "                print('Could not compute pretrain_val_error:', e)\n",
        "                pretrain_ae_threshold = None\n",
        "        else:\n",
        "            pretrain_ae_threshold = None\n",
        "    if pretrain_ae_threshold is not None:\n",
        "        print('Set pretrain_ae_threshold to 95th percentile =', pretrain_ae_threshold)\n",
        "        try:\n",
        "            with open('pretrain_ae_threshold.txt','w') as f:\n",
        "                f.write(str(pretrain_ae_threshold))\n",
        "        except Exception:\n",
        "            pass\n",
        "    else:\n",
        "        print('pretrain_ae_threshold not set (missing data)')\n",
        "except Exception as e:\n",
        "    print('Error overriding pretrain threshold:', e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td5pMqKDTLNa"
      },
      "source": [
        "-----\n",
        "\n",
        "#### **5. Fine-Tune the Autoencoder with User Reports**\n",
        "\n",
        "This is the new, crucial step. We continue training the AE, but only on the benign URLs reported by users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoQIHaKiTLNa",
        "outputId": "6d2e34c3-9f0a-41b3-b678-be3da19ce5b0"
      },
      "outputs": [],
      "source": [
        "# --- Fetch and Process User Reports for Fine-tuning and Evaluation ---\n",
        "APP_ID = \"ads-phishing-link\"\n",
        "REPORTS_PATH = f\"artifacts/{APP_ID}/private_user_reports\"\n",
        "db = firestore.Client()\n",
        "print(f\"\\nFetching all user reports for processing...\")\n",
        "try:\n",
        "    report_docs = list(db.collection(REPORTS_PATH).stream())\n",
        "    print(f\"Found {len(report_docs)} total user reports.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error fetching from Firestore: {e}\")\n",
        "    report_docs = []\n",
        "\n",
        "# --- Create Unified Labeled URL Dataset from All Reports (with aggregation) ---\n",
        "# Aggregate user reports into votes per-URL and per-post, then trust only high-confidence labels\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Map report types to label intents\n",
        "type_to_label = {\n",
        "    'true_positive': 1,\n",
        "    'false_negative': 1,\n",
        "    'true_negative': 0,\n",
        "    'false_positive': 0\n",
        "}\n",
        "\n",
        "# Collect votes\n",
        "url_votes = defaultdict(list)\n",
        "post_votes = defaultdict(list)\n",
        "for doc in report_docs:\n",
        "    d = doc.to_dict()\n",
        "    rtype = d.get('type')\n",
        "    label = type_to_label.get(rtype, None)\n",
        "    payload = d.get('payload') or {}\n",
        "    links = payload.get('links') or []\n",
        "    if isinstance(links, str):\n",
        "        links = [links]\n",
        "    if payload.get('url') and payload.get('url') not in links:\n",
        "        links.append(payload.get('url'))\n",
        "    for u in set([x for x in links if x]):\n",
        "        if label is not None:\n",
        "            url_votes[u].append(label)\n",
        "    if payload.get('postId') and label is not None:\n",
        "        post_votes[payload.get('postId')].append(label)\n",
        "\n",
        "# Decide trust rules\n",
        "MIN_VOTES = 1  # loosen to include single-vote reports for more training signal\n",
        "trusted_url_labels = {}\n",
        "for u, votes in url_votes.items():\n",
        "    if len(votes) >= MIN_VOTES:\n",
        "        c = Counter(votes)\n",
        "        trusted_label = 1 if c[1] > c[0] else 0\n",
        "        trusted_url_labels[u] = {'label': trusted_label, 'votes': len(votes), 'pos': c[1], 'neg': c[0]}\n",
        "\n",
        "# Build df_user from trusted labels; fallback to all reports if none trusted\n",
        "user_url_data = []\n",
        "if len(trusted_url_labels) > 0:\n",
        "    for u, info in trusted_url_labels.items():\n",
        "        user_url_data.append({'url': u, 'label': info['label'], 'votes': info['votes']})\n",
        "    print(f\"Using {len(trusted_url_labels)} trusted URL labels (min_votes={MIN_VOTES}) for fine-tuning.)\")\n",
        "else:\n",
        "    # Fallback: use all reported URLs but mark as low confidence\n",
        "    print(\"No trusted URL labels found (not enough consensus). Falling back to all user reports (low confidence).\")\n",
        "    for doc in report_docs:\n",
        "        d = doc.to_dict()\n",
        "        payload = d.get('payload', {})\n",
        "        links = payload.get('links') or []\n",
        "        if isinstance(links, str): links = [links]\n",
        "        if payload.get('url') and payload.get('url') not in links:\n",
        "            links.append(payload.get('url'))\n",
        "        label = 1 if d.get('type') in ('false_negative','true_positive') else 0\n",
        "        for u in set([x for x in links if x]):\n",
        "            user_url_data.append({'url': u, 'label': label})\n",
        "\n",
        "# Create DataFrame\n",
        "df_user = pd.DataFrame(user_url_data).drop_duplicates().reset_index(drop=True)\n",
        "print(f\"Created a unified dataset of {len(df_user)} unique user-reported URLs (post-aggregation).\")\n",
        "\n",
        "# --- Feature Extraction Function ---\n",
        "feat_means_map = df_base[feature_cols].mean().to_dict()\n",
        "\n",
        "# Helper: normalize URL by removing tracking params (fbclid, utm_*)\n",
        "from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode\n",
        "\n",
        "def normalize_url_for_features(raw_url: str) -> str:\n",
        "    try:\n",
        "        p = urlparse(raw_url or \"\")\n",
        "        if p.query:\n",
        "            q = parse_qsl(p.query, keep_blank_values=True)\n",
        "            q_filtered = [(k, v) for k, v in q if not (k.lower() == 'fbclid' or k.lower().startswith('utm_'))]\n",
        "            p = p._replace(query=urlencode(q_filtered))\n",
        "        return urlunparse(p)\n",
        "    except Exception:\n",
        "        return raw_url or \"\"\n",
        "\n",
        "# Helper: stabilize heavy-tailed features prior to scaling/training\n",
        "import numpy as np\n",
        "\n",
        "def stabilize_features(feat_dict: dict) -> dict:\n",
        "    # Stricter clipping to limit extreme z-scores from rare large counts\n",
        "    feat_dict['qty_questionmark_url'] = min(max(int(feat_dict.get('qty_questionmark_url', 0)), 0), 3)\n",
        "    feat_dict['qty_slash_url'] = min(max(int(feat_dict.get('qty_slash_url', 0)), 0), 10)\n",
        "    feat_dict['qty_dot_url'] = min(max(int(feat_dict.get('qty_dot_url', 0)), 0), 6)\n",
        "    feat_dict['qty_hyphen_url'] = min(max(int(feat_dict.get('qty_hyphen_url', 0)), 0), 10)\n",
        "\n",
        "    # Binarize or clip other count-style features\n",
        "    feat_dict['url_shortened'] = 1.0 if feat_dict.get('url_shortened', 0) else 0.0\n",
        "\n",
        "    # Log-transform length and timing-like numeric features to compress dynamic range\n",
        "    for key in ['file_length','directory_length','params_length','time_domain_activation','time_domain_expiration','ttl_hostname','asn_ip','time_response']:\n",
        "        if key in feat_dict and feat_dict[key] is not None:\n",
        "            try:\n",
        "                feat_dict[key] = float(np.log1p(max(0.0, float(feat_dict[key]))))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # Keep other defaults intact\n",
        "    return feat_dict\n",
        "\n",
        "\n",
        "def features_for_url(url: str) -> np.ndarray:\n",
        "    u = normalize_url_for_features(url)\n",
        "    try:\n",
        "        parts = tldextract.extract(u)\n",
        "        domain = \".\".join(p for p in [parts.subdomain, parts.domain, parts.suffix] if p)\n",
        "        lex_feats = {\n",
        "            'qty_dot_url': u.count('.'), 'qty_hyphen_url': u.count('-'), 'qty_slash_url': u.count('/'),\n",
        "            'qty_questionmark_url': u.count('?'), 'qty_equal_url': u.count('='), 'qty_at_url': u.count('@'), 'length_url': len(u),\n",
        "            'qty_dot_domain': domain.count('.'), 'qty_hyphen_domain': domain.count('-'), 'domain_length': len(domain),\n",
        "            # conservative defaults for other fields; will be filled from feat_means_map when missing\n",
        "            'url_shortened': 1.0 if any(s in (parts.domain or '').lower() for s in ['bit','tinyurl','t','is','cutt','ow','lnkd','buff','goo']) else 0.0\n",
        "        }\n",
        "    except Exception:\n",
        "        lex_feats = {}\n",
        "    # Merge with means to ensure all expected feature keys present\n",
        "    merged = {c: float(feat_means_map.get(c, 0.0)) for c in feature_cols}\n",
        "    # Overwrite with computed lexical features when available\n",
        "    for k, v in lex_feats.items():\n",
        "        if k in merged:\n",
        "            merged[k] = float(v)\n",
        "    # Stabilize heavy-tailed features\n",
        "    merged = stabilize_features(merged)\n",
        "    return np.array([float(merged.get(c, feat_means_map.get(c, 0.0))) for c in feature_cols], dtype=np.float32)\n",
        "\n",
        "# --- Split User Data into Train/Validation and Held-out Test Sets ---\n",
        "if len(df_user) > 10: # Ensure enough data to split\n",
        "    X_user_features = np.stack(df_user['url'].apply(features_for_url).values)\n",
        "    y_user_labels = df_user['label'].values\n",
        "\n",
        "    # 80/20 split for training and final testing\n",
        "    X_user_train_val, X_user_test, y_user_train_val, y_user_test = train_test_split(\n",
        "        X_user_features, y_user_labels, test_size=0.20, stratify=y_user_labels, random_state=42\n",
        "    )\n",
        "    # Split the training data again for fine-tuning and validation (for threshold optimization)\n",
        "    X_user_train, X_user_val, y_user_train, y_user_val = train_test_split(\n",
        "        X_user_train_val, y_user_train_val, test_size=0.25, stratify=y_user_train_val, random_state=42 # 60% train, 20% val\n",
        "    )\n",
        "\n",
        "    X_user_train_s = scaler.transform(X_user_train)\n",
        "    X_user_val_s = scaler.transform(X_user_val)\n",
        "    X_user_test_s = scaler.transform(X_user_test)\n",
        "\n",
        "    print(f\"\\nSplit user data: {len(X_user_train_s)} train, {len(X_user_val_s)} validation, {len(X_user_test_s)} test samples.\")\n",
        "\n",
        "    # --- Fine-Tune Autoencoder on Benign URLs from the User Training Set ---\n",
        "    if len(X_user_train_s[y_user_train == 0]) > 0:\n",
        "        print(\"\\n--- Fine-tuning Autoencoder on User-Reported Training Data ---\")\n",
        "        ae_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss=\"mse\")\n",
        "        ae_model.fit(X_user_train_s[y_user_train == 0], X_user_train_s[y_user_train == 0], epochs=50, batch_size=16, verbose=0)\n",
        "        print(\"✅ Autoencoder fine-tuning complete.\")\n",
        "    else:\n",
        "        print(\"\\n⚠️ No benign user reports in the training set to fine-tune on.\")\n",
        "else:\n",
        "    print(\"\\n⚠️ Not enough user reports for a robust train/test split. Evaluation might not be reliable.\")\n",
        "    # Create empty placeholders if there isn't enough data\n",
        "    X_user_train_s, X_user_val_s, X_user_test_s = [np.array([]) for _ in range(3)]\n",
        "    y_user_train, y_user_val, y_user_test = [np.array([]) for _ in range(3)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Balanced fine-tune + K-fold CV threshold selection + Active-learning hook + Telemetry ===\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score\n",
        "import time, json, csv\n",
        "\n",
        "# Parameters\n",
        "K_FOLDS = 5\n",
        "RECALL_TARGET = float(os.environ.get('AE_THRESHOLD_RECALL_TARGET', '0.25'))\n",
        "USER_WEIGHT = float(os.environ.get('AE_USER_WEIGHT', '5.0'))\n",
        "PHISH_SAMPLE_MAX = int(os.environ.get('PHISH_SAMPLE_MAX', '500'))\n",
        "\n",
        "print('Running balanced fine-tune + K-fold thresholding (K=%d) ...' % K_FOLDS)\n",
        "\n",
        "# --- Balanced fine-tune to avoid catastrophic forgetting ---\n",
        "# Collect user benign URLs (label==0) if present\n",
        "user_benign_urls = []\n",
        "if 'df_user' in globals() and not df_user.empty:\n",
        "    try:\n",
        "        user_benign_urls = df_user[df_user['label']==0]['url'].dropna().unique().tolist()\n",
        "    except Exception:\n",
        "        user_benign_urls = []\n",
        "\n",
        "# Sample phishing examples from public dataset\n",
        "phish_indices = np.where(y_public == 1)[0] if 'y_public' in globals() else np.array([])\n",
        "phish_sample_idx = phish_indices\n",
        "if phish_indices.size > PHISH_SAMPLE_MAX:\n",
        "    rng = np.random.RandomState(SEED)\n",
        "    phish_sample_idx = rng.choice(phish_indices, size=PHISH_SAMPLE_MAX, replace=False)\n",
        "\n",
        "# Build features for balanced fine-tune\n",
        "X_bal = []\n",
        "sw = []\n",
        "if len(user_benign_urls) > 0:\n",
        "    # user benign features\n",
        "    for u in user_benign_urls:\n",
        "        try:\n",
        "            feat = features_for_url(u)\n",
        "            X_bal.append(feat)\n",
        "            sw.append(USER_WEIGHT)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# add sampled phishing public examples (use public X_public rows corresponding to indices)\n",
        "if phish_sample_idx is not None and len(phish_sample_idx) > 0 and 'X_public' in globals():\n",
        "    # Use the numeric X_public directly (it matches feature_cols)\n",
        "    for idx in phish_sample_idx:\n",
        "        try:\n",
        "            X_bal.append(X_public.iloc[idx].values.astype(np.float32))\n",
        "            sw.append(1.0)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "if len(X_bal) > 0:\n",
        "    X_bal = np.vstack(X_bal)\n",
        "    try:\n",
        "        X_bal_s = scaler.transform(X_bal)\n",
        "        print('Balanced fine-tune dataset:', X_bal_s.shape, ' (user_benign=%d, phish=%d)' % (len(user_benign_urls), len(phish_sample_idx)))\n",
        "        # Fine-tune AE to reconstruct inputs but emphasize user-benign via sample_weight\n",
        "        try:\n",
        "            ae_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='mse')\n",
        "            ae_model.fit(X_bal_s, X_bal_s, sample_weight=np.array(sw), epochs=20, batch_size=32, verbose=1)\n",
        "            print('✅ Balanced fine-tune complete')\n",
        "        except Exception as e:\n",
        "            print('Balanced fine-tune failed:', e)\n",
        "    except Exception as e:\n",
        "        print('Could not scale balanced dataset:', e)\n",
        "else:\n",
        "    print('No balanced fine-tune data available. Skipping.')\n",
        "\n",
        "# --- K-fold CV threshold selection using PR curve per fold ---\n",
        "# Build labeled URL dataset from df_user\n",
        "if 'df_user' in globals() and not df_user.empty:\n",
        "    urls = df_user['url'].values\n",
        "    labels = df_user['label'].astype(int).values\n",
        "    n_samples = len(urls)\n",
        "    if n_samples >= 6:\n",
        "        k = min(K_FOLDS, n_samples)\n",
        "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED)\n",
        "        thresholds = []\n",
        "        fold_stats = []\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(urls, labels)):\n",
        "            X_val = np.stack([features_for_url(urls[i]) for i in val_idx])\n",
        "            X_val_s = scaler.transform(X_val)\n",
        "            # compute AE errors on validation fold\n",
        "            recon = ae_model.predict(X_val_s, verbose=0)\n",
        "            errors = np.mean((recon - X_val_s)**2, axis=1)\n",
        "            # precision-recall curve expects score where larger => positive\n",
        "            precision, recall, pr_thresholds = precision_recall_curve(labels[val_idx], errors)\n",
        "            # Find threshold achieving at least RECALL_TARGET with maximum precision\n",
        "            cand_threshold = None\n",
        "            cand_prec = -1\n",
        "            for p, r, t in zip(precision, recall, np.append(pr_thresholds, pr_thresholds[-1] if len(pr_thresholds)>0 else 0)):\n",
        "                if r >= RECALL_TARGET and p > cand_prec:\n",
        "                    cand_prec = p\n",
        "                    cand_threshold = t\n",
        "            # fallback: choose threshold that maximizes F1 on this fold\n",
        "            if cand_threshold is None:\n",
        "                f1s = (2 * precision * recall) / (precision + recall + 1e-12)\n",
        "                best_i = np.nanargmax(f1s)\n",
        "                cand_threshold = pr_thresholds[best_i] if len(pr_thresholds)>0 else np.percentile(errors, 95)\n",
        "            thresholds.append(cand_threshold)\n",
        "            # record fold stats at chosen threshold\n",
        "            ypred = (errors > cand_threshold).astype(int)\n",
        "            f1 = f1_score(labels[val_idx], ypred)\n",
        "            prec = precision_score(labels[val_idx], ypred, zero_division=0)\n",
        "            rec = recall_score(labels[val_idx], ypred, zero_division=0)\n",
        "            fold_stats.append({'fold': fold, 'threshold': float(cand_threshold), 'f1': float(f1), 'precision': float(prec), 'recall': float(rec)})\n",
        "        # Aggregate thresholds\n",
        "        thresholds = np.array(thresholds)\n",
        "        chosen_threshold = float(np.median(thresholds))\n",
        "        print('K-fold thresholds per fold:', thresholds)\n",
        "        print('Median chosen threshold:', chosen_threshold)\n",
        "        # Set ae_threshold (used later) to chosen_threshold\n",
        "        ae_threshold = chosen_threshold\n",
        "        try:\n",
        "            with open('autoencoder_threshold.txt','w') as f:\n",
        "                f.write(str(ae_threshold))\n",
        "        except Exception:\n",
        "            pass\n",
        "        # Save fold_stats for provenance\n",
        "        with open('ae_threshold_cv_stats.json','w') as f:\n",
        "            json.dump({'folds': fold_stats, 'chosen_threshold': ae_threshold, 'recall_target': RECALL_TARGET}, f, indent=2)\n",
        "        print('Saved ae_threshold_cv_stats.json')\n",
        "    else:\n",
        "        print('Not enough user-labeled URLs for K-fold CV thresholding (need >=6). Using existing ae_threshold or pretrain_ae_threshold if available.')\n",
        "        if 'pretrain_ae_threshold' in globals() and 'ae_threshold' not in globals():\n",
        "            ae_threshold = pretrain_ae_threshold\n",
        "else:\n",
        "    print('df_user missing or empty; skipping K-fold CV threshold selection')\n",
        "\n",
        "# --- Active-learning hook: save borderline candidates for review ---\n",
        "# Borderline definition: fused score between 0.35 and 0.65 OR top-N uncertain\n",
        "BORDER_LOW= float(os.environ.get('AL_BORDER_LOW','0.35'))\n",
        "BORDER_HIGH= float(os.environ.get('AL_BORDER_HIGH','0.65'))\n",
        "AL_FILE = 'active_learning_candidates.csv'\n",
        "\n",
        "def add_active_candidates(urls_list, scores_list, reason='borderline'):\n",
        "    rows = []\n",
        "    ts = int(time.time())\n",
        "    for u,s in zip(urls_list, scores_list):\n",
        "        rows.append([u, float(s), reason, ts])\n",
        "    # append to CSV\n",
        "    header = not os.path.exists(AL_FILE)\n",
        "    with open(AL_FILE, 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        if header:\n",
        "            writer.writerow(['url','score','reason','ts'])\n",
        "        writer.writerows(rows)\n",
        "    print('Appended', len(rows), 'active-learning candidates to', AL_FILE)\n",
        "\n",
        "# Build fused scores for all user-reported URLs (if possible)\n",
        "try:\n",
        "    fused_scores_all = []\n",
        "    urls_all = []\n",
        "    for report in user_reports_data:\n",
        "        urls = report.get('urls') or []\n",
        "        if not urls: continue\n",
        "        # compute per-url AE probs and take max\n",
        "        feats = np.stack([features_for_url(u) for u in urls if u], axis=0)\n",
        "        feats_s = scaler.transform(feats)\n",
        "        recon = ae_model.predict(feats_s, verbose=0)\n",
        "        errors = np.mean((recon - feats_s)**2, axis=1)\n",
        "        ae_prob = float(np.clip((errors - ae_threshold * 0.5) / (ae_threshold * 2), 0, 1).max())\n",
        "        # gcn prob\n",
        "        g = 0.5\n",
        "        if 'post_node_map' in globals() and 'gnn_probs' in globals():\n",
        "            idx = post_node_map.get(str(report['postId']))\n",
        "            if idx is not None and 0 <= idx < len(gnn_probs):\n",
        "                g = float(gnn_probs[idx])\n",
        "        fused = 0.6 * ae_prob + 0.4 * g\n",
        "        # record per-URL as well (use first URL)\n",
        "        fused_scores_all.append(fused)\n",
        "        urls_all.append(urls[0] if len(urls)>0 else None)\n",
        "    # select borderline ones\n",
        "    borderline_urls = [u for u,s in zip(urls_all, fused_scores_all) if u and BORDER_LOW <= s <= BORDER_HIGH]\n",
        "    borderline_scores = [s for s in fused_scores_all if BORDER_LOW <= s <= BORDER_HIGH]\n",
        "    if borderline_urls:\n",
        "        add_active_candidates(borderline_urls, borderline_scores, reason='borderline_fused')\n",
        "except Exception as e:\n",
        "    print('Active-learning candidate extraction failed:', e)\n",
        "\n",
        "# --- Telemetry: save label counts + report counts ---\n",
        "telemetry = {}\n",
        "try:\n",
        "    telemetry['timestamp'] = int(time.time())\n",
        "    telemetry['num_user_reports'] = len(report_docs) if 'report_docs' in globals() else 0\n",
        "    telemetry['num_unique_user_urls'] = int(len(df_user)) if 'df_user' in globals() and not df_user.empty else 0\n",
        "    telemetry['label_counts'] = {}\n",
        "    if 'df_user' in globals() and not df_user.empty:\n",
        "        telemetry['label_counts'] = df_user['label'].value_counts().to_dict()\n",
        "    # report types counts from report_docs\n",
        "    if 'report_docs' in globals() and report_docs:\n",
        "        types = [ (d.to_dict() or {}).get('type') for d in report_docs ]\n",
        "        from collections import Counter\n",
        "        telemetry['report_type_counts'] = dict(Counter([t for t in types if t is not None]))\n",
        "    # Save telemetry\n",
        "    with open('training_telemetry.json','w') as f:\n",
        "        json.dump(telemetry, f, indent=2)\n",
        "    print('Saved training_telemetry.json', telemetry)\n",
        "except Exception as e:\n",
        "    print('Telemetry capture failed:', e)\n",
        "\n",
        "print('Balanced fine-tune + K-fold CV + active-learning + telemetry complete.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure `user_reports_data` exists before fusion grid-search\n",
        "if 'user_reports_data' not in globals():\n",
        "    user_reports_data = []\n",
        "    # Prefer df_user if available (constructed earlier from trusted labels)\n",
        "    if 'df_user' in globals() and not df_user.empty:\n",
        "        for _, r in df_user.iterrows():\n",
        "            pid = r.get('post_id') or r.get('postId') or None\n",
        "            urls = r.get('urls') or []\n",
        "            if isinstance(urls, str):\n",
        "                try:\n",
        "                    import ast\n",
        "                    urls = ast.literal_eval(urls)\n",
        "                except Exception:\n",
        "                    urls = [urls]\n",
        "            # infer label\n",
        "            lbl = None\n",
        "            try:\n",
        "                if 'label' in r.index and (int(r['label']) in (0,1)):\n",
        "                    lbl = int(r['label'])\n",
        "                else:\n",
        "                    typ = r.get('type') if 'type' in r.index else None\n",
        "                    lbl = 1 if typ in ('false_negative','true_positive') else 0\n",
        "            except Exception:\n",
        "                lbl = 0\n",
        "            if pid:\n",
        "                user_reports_data.append({'postId': pid, 'true_label': int(lbl), 'urls': urls})\n",
        "    # Fallback: use report_docs from Firestore if present\n",
        "    elif 'report_docs' in globals() and len(report_docs) > 0:\n",
        "        for doc in report_docs:\n",
        "            d = doc.to_dict() or {}\n",
        "            payload = d.get('payload') or {}\n",
        "            postId = payload.get('postId') or payload.get('post_id') or None\n",
        "            links = []\n",
        "            if isinstance(payload.get('links'), list):\n",
        "                links = list(payload.get('links'))\n",
        "            if payload.get('url') and payload.get('url') not in links:\n",
        "                links.append(payload.get('url'))\n",
        "            rtype = d.get('type')\n",
        "            true_label = 1 if (rtype in ('false_negative', 'true_positive')) else 0\n",
        "            if postId:\n",
        "                user_reports_data.append({'postId': postId, 'true_label': true_label, 'urls': links})\n",
        "    else:\n",
        "        print('No df_user or report_docs available to build user_reports_data; grid-search will be skipped.')\n",
        "\n",
        "print('user_reports_data length:', len(user_reports_data))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Weighted fine-tune and fusion-weight grid search ---\n",
        "print('\\n--- Weighted fine-tune on user training data and fusion weight grid-search ---')\n",
        "\n",
        "# Build sample weights for user training set (use vote counts when available)\n",
        "# df_user may contain 'votes' if it came from trusted_url_labels; fallback to 1\n",
        "if 'df_user' in globals() and 'url' in df_user.columns:\n",
        "    # Create mapping url -> votes (default 1)\n",
        "    url_to_votes = {}\n",
        "    if 'trusted_url_labels' in globals() and len(trusted_url_labels) > 0:\n",
        "        for u, info in trusted_url_labels.items():\n",
        "            url_to_votes[u] = info.get('votes', 1)\n",
        "    else:\n",
        "        # fallback: all votes = 1\n",
        "        for u in df_user['url'].unique():\n",
        "            url_to_votes[u] = 1\n",
        "\n",
        "    # Build X_user_train_s and sample weights aligned to X_user_train rows\n",
        "    try:\n",
        "        # Recompute X_user_train & y_user_train if needed\n",
        "        X_user_train = np.stack(df_user['url'].apply(features_for_url).values)[:len(df_user)]\n",
        "        y_user_train = df_user['label'].values[:len(df_user)]\n",
        "        X_user_train_s = scaler.transform(X_user_train)\n",
        "        sample_weights = np.array([url_to_votes.get(u, 1) for u in df_user['url'].values])\n",
        "    except Exception as e:\n",
        "        print('Could not build weighted dataset automatically:', e)\n",
        "        X_user_train_s = None\n",
        "        sample_weights = None\n",
        "else:\n",
        "    X_user_train_s = None\n",
        "    sample_weights = None\n",
        "\n",
        "# If we have user training data and weights, fine-tune using them\n",
        "if X_user_train_s is not None and X_user_train_s.shape[0] > 0:\n",
        "    print('Fine-tuning AE using sample weights (if labels exist)...')\n",
        "    try:\n",
        "        ae_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='mse')\n",
        "        # Only fine-tune on benign (label==0) examples as before, but weighted\n",
        "        benign_mask = (y_user_train == 0)\n",
        "        if benign_mask.sum() > 0:\n",
        "            sw = sample_weights[benign_mask] if sample_weights is not None else None\n",
        "            ae_model.fit(X_user_train_s[benign_mask], X_user_train_s[benign_mask], epochs=30, batch_size=16, sample_weight=sw, verbose=1)\n",
        "            print('✅ Weighted fine-tuning complete.')\n",
        "        else:\n",
        "            print('No benign user examples for weighted fine-tune.')\n",
        "    except Exception as e:\n",
        "        print('Weighted fine-tune failed:', e)\n",
        "else:\n",
        "    print('No user train data available for weighted fine-tune.')\n",
        "\n",
        "# --- Fusion weight grid-search on user validation set ---\n",
        "if 'X_user_val_s' in globals() and X_user_val_s.shape[0] > 0 and 'y_user_val' in globals() and len(y_user_val) > 0:\n",
        "    print('\\nPerforming fusion-weight grid-search on user validation set...')\n",
        "    # Compute AE post-level probabilities for user validation\n",
        "    # We need post-level AE probs as in notebook (postid_to_ae_prob)\n",
        "    # For simplicity, compute per-url AE prob and average per post\n",
        "    val_post_ids = []\n",
        "    val_ae_probs = []\n",
        "    val_gcn_probs = []\n",
        "    val_true = []\n",
        "    for report in [r for r in user_reports_data if r['postId'] in train_post_ids or r['postId'] in test_post_ids]:\n",
        "        pid = report['postId']\n",
        "        urls = report.get('urls') or []\n",
        "        if not urls: continue\n",
        "        feats = np.stack([features_for_url(u) for u in urls if u], axis=0)\n",
        "        feats_s = scaler.transform(feats)\n",
        "        recon = ae_model.predict(feats_s, verbose=0)\n",
        "        errors = np.mean((recon - feats_s)**2, axis=1)\n",
        "        ae_probs = np.clip((errors - ae_threshold * 0.5) / (ae_threshold * 2), 0, 1)\n",
        "        post_ae_prob = float(ae_probs.max())\n",
        "        gcn_prob = 0.5\n",
        "        if post_node_map is not None and gnn_probs is not None:\n",
        "            idx = post_node_map.get(str(pid))\n",
        "            if idx is not None and 0 <= idx < len(gnn_probs):\n",
        "                gcn_prob = float(gnn_probs[idx])\n",
        "        val_post_ids.append(pid)\n",
        "        val_ae_probs.append(post_ae_prob)\n",
        "        val_gcn_probs.append(gcn_prob)\n",
        "        val_true.append(report['true_label'])\n",
        "\n",
        "    val_ae_probs = np.array(val_ae_probs)\n",
        "    val_gcn_probs = np.array(val_gcn_probs)\n",
        "    val_true = np.array(val_true)\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_w = None\n",
        "    for w in np.linspace(0,1,21):\n",
        "        fused = w * val_ae_probs + (1-w) * val_gcn_probs\n",
        "        ypred = (fused > 0.5).astype(int)\n",
        "        f1 = f1_score(val_true, ypred)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_w = w\n",
        "    print('Best fusion weight (AE):', best_w, 'F1:', best_f1)\n",
        "else:\n",
        "    print('Not enough user validation data for fusion grid-search.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CScMteJoc9BR",
        "outputId": "179c273c-b49b-4a1f-e2df-16b846adf7ca"
      },
      "outputs": [],
      "source": [
        "# --- Find Optimal Anomaly Threshold using the User Validation Set ---\n",
        "if len(X_user_val_s) > 0:\n",
        "    print(\"\\n--- Optimizing Autoencoder Anomaly Threshold ---\")\n",
        "    val_reconstructed = ae_model.predict(X_user_val_s, verbose=0)\n",
        "    val_errors = np.mean((val_reconstructed - X_user_val_s)**2, axis=1)\n",
        "\n",
        "    # Test a range of percentile-based thresholds to find the best F1-score\n",
        "    best_f1 = -1\n",
        "    optimal_threshold = float(np.percentile(val_errors[y_user_val == 0], 95)) # Default fallback\n",
        "\n",
        "    for p in np.arange(80, 100, 0.5):\n",
        "        threshold = np.percentile(val_errors[y_user_val == 0], p) if len(val_errors[y_user_val == 0]) > 0 else optimal_threshold\n",
        "        y_pred = (val_errors > threshold).astype(int)\n",
        "        f1 = f1_score(y_user_val, y_pred)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            optimal_threshold = float(threshold)\n",
        "\n",
        "    ae_threshold = optimal_threshold\n",
        "    print(f\"✅ Optimal AE threshold found: {ae_threshold:.8f} (Yields F1-score of {best_f1:.4f} on user validation data)\")\n",
        "else:\n",
        "    print(\"\\n⚠️ No user validation data. Falling back to public data for threshold.\")\n",
        "    val_reconstructed = ae_model.predict(X_val_s[y_val==0], verbose=0)\n",
        "    val_error = np.mean((val_reconstructed - X_val_s[y_val==0])**2, axis=1)\n",
        "    ae_threshold = float(np.percentile(val_error, 99.0))\n",
        "    print(f\"Calculated anomaly threshold from public data: {ae_threshold:.8f}\")\n",
        "\n",
        "# Save the final optimized artifacts\n",
        "print(\"\\nSaving fine-tuned Autoencoder artifacts...\")\n",
        "ae_model.save(\"phishing_autoencoder_model.keras\")\n",
        "with open(\"scaler.pkl\", \"wb\") as f: pickle.dump(scaler, f)\n",
        "with open(\"autoencoder_threshold.txt\", \"w\") as f: f.write(str(ae_threshold))\n",
        "print(\"✅ Fine-tuned AE model, scaler, and optimized threshold saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Auto-select AE threshold using validation set (FPR-constrained search) ---\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Desired maximum false positive rate on validation (adjustable)\n",
        "desired_fpr = 0.05\n",
        "\n",
        "# Determine which validation set to use (prefer user validation if available)\n",
        "use_user_val = ('X_user_val_s' in globals() and hasattr(X_user_val_s, 'shape') and X_user_val_s.shape[0] > 0\n",
        "                and 'y_user_val' in globals() and len(y_user_val) > 0)\n",
        "\n",
        "if use_user_val:\n",
        "    print(\"Using user validation set to pick threshold (preferred).\")\n",
        "    val_X = X_user_val_s\n",
        "    val_y = y_user_val\n",
        "    val_reconstructed = ae_model.predict(val_X, verbose=0)\n",
        "    val_errors = np.mean((val_reconstructed - val_X)**2, axis=1)\n",
        "else:\n",
        "    print(\"No user validation labels available. Falling back to public benign validation for percentile-based threshold.\")\n",
        "    # Use benign public validation errors for percentile fallback\n",
        "    val_X = X_val_s[y_val==0]\n",
        "    val_reconstructed = ae_model.predict(val_X, verbose=0)\n",
        "    val_errors = np.mean((val_reconstructed - val_X)**2, axis=1)\n",
        "    val_y = None\n",
        "\n",
        "# Compute diagnostics\n",
        "if use_user_val:\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(val_y, val_errors)\n",
        "    except Exception:\n",
        "        roc_auc = None\n",
        "    prec, rec, thr = precision_recall_curve(val_y, val_errors)\n",
        "    pr_auc = auc(rec, prec)\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}  PR AUC: {pr_auc:.4f}\")\n",
        "\n",
        "# Candidate thresholds (percentiles of benign errors)\n",
        "benign_errors = val_errors if (not use_user_val) else val_errors[val_y==0]\n",
        "percentiles = np.linspace(50, 99.9, 500)\n",
        "candidates = np.percentile(benign_errors, percentiles)\n",
        "\n",
        "best_thr = None\n",
        "best_f1 = -1\n",
        "best_stats = None\n",
        "\n",
        "if use_user_val:\n",
        "    for thr in candidates:\n",
        "        y_pred = (val_errors > thr).astype(int)\n",
        "        f1 = f1_score(val_y, y_pred)\n",
        "        fp = ((y_pred==1) & (val_y==0)).sum()\n",
        "        nneg = max(1, (val_y==0).sum())\n",
        "        fp_rate = fp / nneg\n",
        "        if (fp_rate <= desired_fpr and f1 > best_f1):\n",
        "            best_f1 = f1\n",
        "            best_thr = thr\n",
        "            best_stats = (f1, precision_score(val_y, y_pred), recall_score(val_y, y_pred), fp_rate)\n",
        "\n",
        "    # If no candidate satisfies FPR constraint, fallback to max-F1 candidate\n",
        "    if best_thr is None:\n",
        "        for thr in candidates:\n",
        "            y_pred = (val_errors > thr).astype(int)\n",
        "            f1 = f1_score(val_y, y_pred)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_thr = thr\n",
        "                best_stats = (f1, precision_score(val_y, y_pred), recall_score(val_y, y_pred), ((y_pred==1)&(val_y==0)).sum()/max(1,(val_y==0).sum()))\n",
        "\n",
        "    print(\"Selected threshold from user validation:\")\n",
        "    print(\"threshold=\", best_thr)\n",
        "    print(\"f1, precision, recall, fp_rate =\", best_stats)\n",
        "    ae_threshold = float(best_thr)\n",
        "    # Save updated threshold file\n",
        "    try:\n",
        "        with open('autoencoder_threshold.txt', 'w') as f:\n",
        "            f.write(str(ae_threshold))\n",
        "        print('Saved new autoencoder_threshold.txt')\n",
        "    except Exception as e:\n",
        "        print('Failed to save threshold file:', e)\n",
        "\n",
        "    # Plot PR and ROC curves for inspection\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(1-prec, rec, label=f'PR AUC={pr_auc:.3f}')\n",
        "    plt.xlabel('False Positive Rate (1 - precision)')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.title('Precision-Recall (approx via thresholding)')\n",
        "    plt.legend()\n",
        "\n",
        "    try:\n",
        "        plt.subplot(1,2,2)\n",
        "        from sklearn.metrics import roc_curve\n",
        "        fpr, tpr, rthr = roc_curve(val_y, val_errors)\n",
        "        plt.plot(fpr, tpr, label=f'ROC AUC={roc_auc:.3f}')\n",
        "        plt.xlabel('FPR')\n",
        "        plt.ylabel('TPR')\n",
        "        plt.title('ROC')\n",
        "        plt.legend()\n",
        "    except Exception:\n",
        "        pass\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    # Fallback: choose 99th percentile on public benign validation\n",
        "    ae_threshold = float(np.percentile(val_errors, 99.0))\n",
        "    print('No labeled validation available; using 99th percentile on public benign validation to set threshold:', ae_threshold)\n",
        "    try:\n",
        "        with open('autoencoder_threshold.txt', 'w') as f:\n",
        "            f.write(str(ae_threshold))\n",
        "        print('Saved new autoencoder_threshold.txt')\n",
        "    except Exception as e:\n",
        "        print('Failed to save threshold file:', e)\n",
        "\n",
        "print('\\nFinal ae_threshold =', ae_threshold)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBfCMD87TLNa"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Part 2: GNN for Structural Detection**\n",
        "\n",
        "Builds the graph and trains the GCN on all user reports (both positive and negative).\n",
        "\n",
        "#### **6. Load All Graph Data from Firestore**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qgxuTin-LNj",
        "outputId": "3736bc2b-0996-46ce-8e3c-48744b2e3a18"
      },
      "outputs": [],
      "source": [
        "# --- Construct `user_reports_data` expected by GCN ---\n",
        "# This normalizes the Firestore report documents into the format used later:\n",
        "# {'postId': <postId>, 'true_label': 0|1, 'urls': [list of urls]}\n",
        "user_reports_data = []\n",
        "for doc in report_docs:\n",
        "    d = doc.to_dict()\n",
        "    payload = (d.get('payload') or {})\n",
        "    postId = payload.get('postId')\n",
        "    # Collect links: prefer 'links' list, otherwise include single 'url' if present\n",
        "    links = []\n",
        "    if isinstance(payload.get('links'), list):\n",
        "        links = list(payload.get('links'))\n",
        "    if payload.get('url') and payload.get('url') not in links:\n",
        "        links.append(payload.get('url'))\n",
        "    rtype = d.get('type')\n",
        "    # Map user report types to label: phishing=1, benign=0\n",
        "    true_label = 1 if (rtype in ('false_negative', 'true_positive')) else 0\n",
        "    # Only include entries that have a postId (GCN nodes are keyed by post:<postId>)\n",
        "    if postId:\n",
        "        user_reports_data.append({'postId': postId, 'true_label': true_label, 'urls': links})\n",
        "\n",
        "print(f\"Prepared user_reports_data with {len(user_reports_data)} entries (post-linked reports).\")\n",
        "# Optional: quick sanity counts\n",
        "from collections import Counter\n",
        "print('label distribution:', Counter([r['true_label'] for r in user_reports_data]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4NTYlzuTLNb",
        "outputId": "6169cbae-0fb2-4119-c953-f847d1a937ba"
      },
      "outputs": [],
      "source": [
        "# --- Firestore Configuration for Graph Data ---\n",
        "NODES_PATH = f\"artifacts/{APP_ID}/private/graph/nodes\"\n",
        "EDGES_PATH = f\"artifacts/{APP_ID}/private/graph/edges\"\n",
        "print(f\"\\nFetching graph data from Firestore for app: '{APP_ID}'...\")\n",
        "try:\n",
        "    node_docs = list(db.collection(NODES_PATH).stream())\n",
        "    edge_docs = list(db.collection(EDGES_PATH).stream())\n",
        "    print(f\"Found {len(node_docs)} nodes and {len(edge_docs)} edges.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error fetching from Firestore: {e}\")\n",
        "    node_docs, edge_docs = [], []\n",
        "\n",
        "# --- Graph Construction ---\n",
        "node_key_to_idx, idx_to_key, x_rows = {}, [], []\n",
        "def add_node(key, doc):\n",
        "  if key in node_key_to_idx: return\n",
        "  idx = len(idx_to_key)\n",
        "  node_key_to_idx[key] = idx\n",
        "  idx_to_key.append(key)\n",
        "  node_type = (doc or {}).get(\"type\")\n",
        "  feats = [1.0, 0.0, 0.0] if node_type == \"user\" else [0.0, 1.0, 0.0] if node_type == \"domain\" else [0.0, 0.0, 1.0]\n",
        "  x_rows.append(feats)\n",
        "\n",
        "for d in node_docs: add_node(d.id, d.to_dict())\n",
        "edges = []\n",
        "for d in edge_docs:\n",
        "  e = d.to_dict()\n",
        "  src, dst = e.get(\"src\"), e.get(\"dst\")\n",
        "  if src and dst and src in node_key_to_idx and dst in node_key_to_idx:\n",
        "    edges.append([node_key_to_idx[src], node_key_to_idx[dst]])\n",
        "    edges.append([node_key_to_idx[dst], node_key_to_idx[src]]) # Undirected\n",
        "\n",
        "x = torch.tensor(np.array(x_rows, dtype=np.float32)) if x_rows else torch.empty((0, 3))\n",
        "edge_index = torch.tensor(np.array(edges, dtype=np.int64)).t().contiguous() if edges else torch.empty((2, 0), dtype=torch.long)\n",
        "\n",
        "# --- Split User Data for GCN Train/Test ---\n",
        "# Extract postIds and labels from user_reports_data\n",
        "post_ids = [report['postId'] for report in user_reports_data]\n",
        "true_labels = [report['true_label'] for report in user_reports_data]\n",
        "\n",
        "# Split postIds into train and test sets\n",
        "if len(post_ids) > 0:\n",
        "    train_post_ids, test_post_ids, _, _ = train_test_split(\n",
        "        post_ids, true_labels, test_size=0.20, stratify=true_labels, random_state=42\n",
        "    )\n",
        "    print(f\"\\nSplit user post IDs for GCN: {len(train_post_ids)} train, {len(test_post_ids)} test.\")\n",
        "else:\n",
        "    train_post_ids, test_post_ids = [], []\n",
        "    print(\"\\n⚠️ No user post IDs found for GCN train/test split.\")\n",
        "\n",
        "\n",
        "# --- Label Nodes and Create Masks from the User Data Splits ---\n",
        "y_nodes = torch.full((len(idx_to_key),), -1, dtype=torch.long)\n",
        "for report in user_reports_data:\n",
        "    post_key = f\"post:{report['postId']}\"\n",
        "    if post_key in node_key_to_idx:\n",
        "        y_nodes[node_key_to_idx[post_key]] = report[\"true_label\"]\n",
        "\n",
        "train_indices = [node_key_to_idx[f\"post:{pid}\"] for pid in train_post_ids if f\"post:{pid}\" in node_key_to_idx]\n",
        "test_indices = [node_key_to_idx[f\"post:{pid}\"] for pid in test_post_ids if f\"post:{pid}\" in node_key_to_idx]\n",
        "\n",
        "train_mask = torch.zeros(len(idx_to_key), dtype=torch.bool); train_mask[torch.tensor(train_indices)] = True\n",
        "test_mask = torch.zeros(len(idx_to_key), dtype=torch.bool); test_mask[torch.tensor(test_indices)] = True\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index, y=y_nodes, train_mask=train_mask, test_mask=test_mask)\n",
        "print(\"\\nGraph constructed and data prepared for GCN.\")\n",
        "\n",
        "# --- Define and Train GCN with Class Imbalance Handling ---\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, num_features, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(num_features, 16)\n",
        "        self.conv2 = GCNConv(16, num_classes)\n",
        "    def forward(self, d):\n",
        "        x, ei = d.x, d.edge_index\n",
        "        x = F.relu(self.conv1(x, ei))\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv2(x, ei)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "if data.x.size(0) > 0 and train_mask.any():\n",
        "    # **NEW**: Calculate class weights to handle imbalance\n",
        "    train_labels = data.y[train_mask]\n",
        "    class_counts = torch.bincount(train_labels[train_labels >= 0])\n",
        "    class_weights = 1. / class_counts.float()\n",
        "    class_weights = class_weights / class_weights.sum()\n",
        "    print(f\"\\nCalculated GCN class weights: {class_weights.numpy()}\")\n",
        "\n",
        "    gnn_model = GCN(num_features=data.x.size(1))\n",
        "    optimizer = optim.Adam(gnn_model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
        "\n",
        "    print(\"Training GCN with class weights...\")\n",
        "    for epoch in range(50): # Increased epochs for better learning\n",
        "      gnn_model.train()\n",
        "      optimizer.zero_grad()\n",
        "      out = gnn_model(data)\n",
        "      # **NEW**: Use weights in the loss function\n",
        "      loss = F.nll_loss(out[train_mask], data.y[train_mask], weight=class_weights)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print(\"GNN Training complete.\")\n",
        "\n",
        "    gnn_model.eval()\n",
        "    with torch.no_grad():\n",
        "      logits = gnn_model(data)\n",
        "      gcn_prob_all_nodes = logits.exp()[:, 1].cpu().numpy()\n",
        "    print(\"✅ GNN probabilities calculated for all nodes.\")\n",
        "else:\n",
        "    print(\"Skipping GNN training due to insufficient labeled data.\")\n",
        "    gnn_model = None\n",
        "    gcn_prob_all_nodes = np.full(len(idx_to_key), 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e43YQ_6nTLNb"
      },
      "source": [
        "-----\n",
        "\n",
        "#### **7. Define and Train the GNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpQ5j61iTLNb",
        "outputId": "0428c165-75cb-48e4-e012-ef2cce62d669"
      },
      "outputs": [],
      "source": [
        "class GCN(nn.Module):\n",
        "  def __init__(self, num_features, num_classes=2):\n",
        "    super().__init__()\n",
        "    self.conv1 = GCNConv(num_features, 16)\n",
        "    self.conv2 = GCNConv(16, num_classes)\n",
        "\n",
        "  def forward(self, d):\n",
        "    x, ei = d.x, d.edge_index\n",
        "    x = F.relu(self.conv1(x, ei))\n",
        "    x = F.dropout(x, p=0.2, training=self.training)\n",
        "    x = self.conv2(x, ei)\n",
        "    return F.log_softmax(x, dim=1)\n",
        "\n",
        "if data.x.size(0) > 0 and train_mask.any():\n",
        "    gnn_model = GCN(num_features=data.x.size(1))\n",
        "    optimizer = optim.Adam(gnn_model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
        "\n",
        "    print(\"Training GNN...\")\n",
        "    for epoch in range(30):\n",
        "      gnn_model.train()\n",
        "      optimizer.zero_grad()\n",
        "      out = gnn_model(data)\n",
        "      loss = F.nll_loss(out[train_mask], data.y[train_mask])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print(\"GNN Training complete.\")\n",
        "\n",
        "    gnn_model.eval()\n",
        "    with torch.no_grad():\n",
        "      logits = gnn_model(data)\n",
        "      gcn_prob_all_nodes = logits.exp()[:, 1].cpu().numpy()\n",
        "    print(\"✅ GNN probabilities calculated for all nodes.\")\n",
        "else:\n",
        "    print(\"Skipping GNN training due to insufficient labeled data.\")\n",
        "    gnn_model = None\n",
        "    gcn_prob_all_nodes = np.full(len(idx_to_key), 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu3-F4IvTLNb"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Part 3: Hybrid Fusion and Final Evaluation**\n",
        "\n",
        "#### **8. Align Data for Fusion and Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BNEnHPKTLNb",
        "outputId": "e2a83957-135d-48b3-a3e5-01d13e3015b5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Define the evaluation function to handle probability thresholds\n",
        "def print_thesis_stats(model_name, y_true, y_pred_prob, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Prints evaluation statistics for a model given true labels and predicted probabilities.\n",
        "    Includes metrics suitable for thesis reporting.\n",
        "    \"\"\"\n",
        "    # Convert probabilities to binary predictions based on the threshold\n",
        "    y_pred = (y_pred_prob > threshold).astype(int)\n",
        "\n",
        "    stats = {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
        "    }\n",
        "    print(f\"\\n--- {model_name} ---\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"  {key.capitalize():<18}: {value if key == 'confusion_matrix' else f'{value:.4f}'}\")\n",
        "    return stats\n",
        "\n",
        "\n",
        "idx_to_postid = {i: k.split(\"post:\", 1)[1] for i, k in enumerate(idx_to_key) if k.startswith(\"post:\")}\n",
        "\n",
        "# Get the post IDs and true labels for the held-out user test set\n",
        "test_indices_np = test_mask.nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
        "user_test_post_indices_in_graph = [i for i in test_indices_np if idx_to_key[i].startswith(\"post:\")]\n",
        "user_test_post_ids = [idx_to_key[i].split(\"post:\", 1)[1] for i in user_test_post_indices_in_graph]\n",
        "\n",
        "# Filter true labels to match the labeled test post nodes\n",
        "user_test_true_labels = data.y[user_test_post_indices_in_graph].cpu().numpy()\n",
        "\n",
        "\n",
        "post_to_urls = {}\n",
        "for d in report_docs:\n",
        "  report = d.to_dict()\n",
        "  payload = report.get(\"payload\", {})\n",
        "  pid = payload.get(\"postId\")\n",
        "  links = (report.get(\"payload\") or {}).get(\"links\", [])\n",
        "  url = (report.get(\"payload\") or {}).get(\"url\")\n",
        "  if pid:\n",
        "    all_links = set(links if isinstance(links, list) else [])\n",
        "    if isinstance(url, str):\n",
        "        all_links.add(url)\n",
        "    post_to_urls.setdefault(pid, set()).update(all_links)\n",
        "\n",
        "# --- Prepare Labeled User Data for AE Threshold Optimization ---\n",
        "print(\"\\nPreparing labeled user data for AE threshold optimization...\")\n",
        "\n",
        "# Filter user_reports_data to create user_test_data based on user_test_post_ids\n",
        "user_test_data = [report for report in user_reports_data if report['postId'] in user_test_post_ids]\n",
        "\n",
        "user_test_urls_labeled = []\n",
        "user_test_labels = []\n",
        "\n",
        "for report in user_test_data:\n",
        "    true_label = report[\"true_label\"]\n",
        "    urls = report[\"urls\"]\n",
        "    for url in urls:\n",
        "        user_test_urls_labeled.append(url)\n",
        "        user_test_labels.append(true_label)\n",
        "\n",
        "if user_test_urls_labeled:\n",
        "    X_user_test_labeled = np.stack([features_for_url(u) for u in user_test_urls_labeled if u], axis=0)\n",
        "    X_user_test_labeled_s = scaler.transform(X_user_test_labeled)\n",
        "    y_user_test_labeled = np.array(user_test_labels)\n",
        "\n",
        "    print(f\"Prepared {len(user_test_urls_labeled)} labeled URLs from user test data for threshold optimization.\")\n",
        "else:\n",
        "    print(\"⚠️ No labeled URLs in user test data. Skipping AE threshold optimization.\")\n",
        "    X_user_test_labeled_s = np.array([])\n",
        "    y_user_test_labeled = np.array([])\n",
        "\n",
        "\n",
        "print(\"\\nCalculating AE probabilities for held-out user test posts...\")\n",
        "postid_to_ae_prob = {}\n",
        "if user_test_post_ids:\n",
        "    for pid in user_test_post_ids:\n",
        "        urls = post_to_urls.get(pid)\n",
        "        if not urls:\n",
        "            postid_to_ae_prob[pid] = 0.5\n",
        "            continue\n",
        "\n",
        "        X_urls = np.stack([features_for_url(u) for u in urls if u], axis=0)\n",
        "        if X_urls.ndim == 2:\n",
        "            X_urls_s = scaler.transform(X_urls)\n",
        "            reconstructed = ae_model.predict(X_urls_s, verbose=0)\n",
        "            errors = np.mean((reconstructed - X_urls_s)**2, axis=1)\n",
        "            # Using a simple scaling for probability based on AE error\n",
        "            # Error higher than threshold * 2 is prob 1, error lower than threshold * 0.5 is prob 0\n",
        "            prob = np.clip((errors - ae_threshold * 0.5) / (ae_threshold * 2), 0, 1)\n",
        "            postid_to_ae_prob[pid] = float(prob.max())\n",
        "        else:\n",
        "            postid_to_ae_prob[pid] = 0.5\n",
        "\n",
        "# Ensure probabilities are ordered correctly based on user_test_post_ids\n",
        "ae_post_prob_user_test = np.array([postid_to_ae_prob.get(pid, 0.5) for pid in user_test_post_ids])\n",
        "gcn_prob_user_test = gcn_prob_all_nodes[user_test_post_indices_in_graph] if user_test_post_indices_in_graph else np.array([0.5] * len(user_test_post_ids))\n",
        "fusion_prob_user_test = 0.6 * ae_post_prob_user_test + 0.4 * gcn_prob_user_test\n",
        "\n",
        "\n",
        "print(\"✅ Data aligned for final evaluation on user test set.\")\n",
        "\n",
        "# --- Final Evaluation on Held-out User Test Set ---\n",
        "print(\"\\n--- Evaluating Fine-Tuned AE on Held-out User Test Set ---\")\n",
        "stats_ae_user_test = print_thesis_stats(\"Fine-Tuned AE (User Test Data)\", user_test_true_labels, ae_post_prob_user_test)\n",
        "\n",
        "print(\"\\n--- Evaluating GCN on Held-out User Test Set ---\")\n",
        "stats_gcn_user_test = print_thesis_stats(\"GCN (User Test Data)\", user_test_true_labels, gcn_prob_user_test)\n",
        "\n",
        "print(\"\\n--- Evaluating Hybrid Fusion Model on Held-out User Test Set ---\")\n",
        "stats_fusion_user_test = print_thesis_stats(\"Hybrid Fusion Model (User Test Data)\", user_test_true_labels, fusion_prob_user_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Reproducibility, provenance, validation, and artifact manifest (ISO-friendly) ---\n",
        "import os, json, hashlib, subprocess\n",
        "from datetime import datetime\n",
        "\n",
        "print('Running provenance + artifact manifest cell')\n",
        "\n",
        "# A. Deterministic setup (best-effort)\n",
        "SEED = int(os.environ.get('TRAIN_SEED', '42'))\n",
        "try:\n",
        "    import random\n",
        "    random.seed(SEED)\n",
        "    import numpy as _np\n",
        "    _np.random.seed(SEED)\n",
        "    import tensorflow as _tf\n",
        "    _tf.random.set_seed(SEED)\n",
        "    try:\n",
        "        _tf.config.experimental.enable_op_determinism()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        _tf.config.threading.set_intra_op_parallelism_threads(1)\n",
        "        _tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "    except Exception:\n",
        "        pass\n",
        "except Exception as e:\n",
        "    print('Deterministic setup partial failure:', e)\n",
        "\n",
        "# B. Capture environment & git commit\n",
        "reqs_file = 'requirements_used.txt'\n",
        "try:\n",
        "    subprocess.check_call([\"python\", \"-m\", \"pip\", \"freeze\"], stdout=open(reqs_file, 'w'))\n",
        "except Exception:\n",
        "    try:\n",
        "        # fallback\n",
        "        with open(reqs_file, 'w') as f:\n",
        "            f.write('\\n'.join([f\"{k}=={v}\" for k,v in {}.items()]))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "git_commit = None\n",
        "try:\n",
        "    git_commit = subprocess.check_output(['git','rev-parse','--short','HEAD']).decode().strip()\n",
        "except Exception:\n",
        "    git_commit = None\n",
        "\n",
        "training_provenance = {\n",
        "    'seed': SEED,\n",
        "    'python_packages_file': reqs_file,\n",
        "    'git_commit': git_commit,\n",
        "    'timestamp': datetime.utcnow().isoformat() + 'Z'\n",
        "}\n",
        "with open('training_provenance.json','w') as f:\n",
        "    json.dump(training_provenance, f, indent=2)\n",
        "\n",
        "# C. Data/schema validation (simple checks)\n",
        "try:\n",
        "    assert 'df_base' in globals(), 'df_base not found'\n",
        "    assert not df_base.isnull().any().any(), 'Nulls present in base dataset'\n",
        "    assert X_public.shape[1] == len(feature_cols), 'Feature column count mismatch'\n",
        "    print('Basic data checks passed')\n",
        "except AssertionError as e:\n",
        "    print('Data check failed:', e)\n",
        "\n",
        "# Helper: file checksum\n",
        "def file_sha256(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path,'rb') as f:\n",
        "        while True:\n",
        "            b = f.read(8192)\n",
        "            if not b: break\n",
        "            h.update(b)\n",
        "    return h.hexdigest()\n",
        "\n",
        "# D. Save artifacts (if model and scaler exist) and record checksums\n",
        "artifact_manifest = {'provenance': training_provenance, 'artifacts': {}}\n",
        "try:\n",
        "    # Save AE model, scaler, threshold if not already saved\n",
        "    if 'ae_model' in globals():\n",
        "        try:\n",
        "            ae_model.save('phishing_autoencoder_model.keras')\n",
        "        except Exception:\n",
        "            # some Keras versions require different save; ignore\n",
        "            pass\n",
        "    if 'scaler' in globals():\n",
        "        with open('scaler.pkl','wb') as f: pickle.dump(scaler, f)\n",
        "    if 'ae_threshold' in globals():\n",
        "        with open('autoencoder_threshold.txt','w') as f: f.write(str(ae_threshold))\n",
        "\n",
        "    for p in ['phishing_autoencoder_model.keras', 'scaler.pkl', 'autoencoder_threshold.txt']:\n",
        "        if os.path.exists(p):\n",
        "            artifact_manifest['artifacts'][p] = {'path': p, 'sha256': file_sha256(p)}\n",
        "except Exception as e:\n",
        "    print('Artifact save error:', e)\n",
        "\n",
        "# F. Model card\n",
        "model_card = f\"\"\"\n",
        "# Model Card\n",
        "name: dakugumen-phishing-ae\n",
        "trained_at: {training_provenance['timestamp']}\n",
        "git_commit: {training_provenance['git_commit']}\n",
        "notes: Sanitized URLs, RobustScaler, AE+GCN fusion\n",
        "\"\"\"\n",
        "with open('MODEL_CARD.md','w') as f: f.write(model_card)\n",
        "artifact_manifest['artifacts']['MODEL_CARD.md'] = {'path':'MODEL_CARD.md', 'sha256': file_sha256('MODEL_CARD.md')}\n",
        "\n",
        "# G. Unit test for features\n",
        "try:\n",
        "    sample = 'https://example.com/path?utm_source=x&fbclid=123'\n",
        "    fv = features_for_url(sample)\n",
        "    assert isinstance(fv, (list, tuple, np.ndarray))\n",
        "    print('features_for_url test passed')\n",
        "except Exception as e:\n",
        "    print('features_for_url test failed:', e)\n",
        "\n",
        "# H. Save manifest\n",
        "with open('artifact_manifest.json','w') as f:\n",
        "    json.dump(artifact_manifest, f, indent=2)\n",
        "print('Saved artifact_manifest.json and MODEL_CARD.md')\n",
        "\n",
        "# I. Optionally register in Firestore (if db present)\n",
        "try:\n",
        "    if 'db' in globals() and db is not None:\n",
        "        reg = db.collection(f\"artifacts/{APP_ID}/models_registry\")\n",
        "        reg.add({'manifest': artifact_manifest, 'timestamp': firestore.SERVER_TIMESTAMP})\n",
        "        print('Registered artifact in Firestore registry')\n",
        "except Exception as e:\n",
        "    print('Registry write failed (ok):', e)\n",
        "\n",
        "print('\\nProvenance and manifest generation complete')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Train a supervised classifier combining AE + GCN + lexical features ---\n",
        "print('\\n--- Training supervised classifier (RandomForest) to boost detection ---')\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Build dataset from df_user (use all available labels)\n",
        "X_rows = []\n",
        "y_rows = []\n",
        "feature_names = []\n",
        "\n",
        "expected_cols = list(getattr(scaler, 'feature_names_in_', []))\n",
        "if not expected_cols:\n",
        "    expected_cols = [f'f{i}' for i in range(111)]\n",
        "\n",
        "for idx, row in df_user.iterrows():\n",
        "    u = row['url']\n",
        "    lbl = int(row['label'])\n",
        "    try:\n",
        "        feats = features_for_url(u)\n",
        "        scaled_feats = scaler.transform(feats.reshape(1, -1))[0]\n",
        "        # AE recon and content score\n",
        "        recon = ae_model.predict(scaled_feats.reshape(1, -1), verbose=0)\n",
        "        err = float(np.mean(np.square(scaled_feats - recon[0])))\n",
        "        content_score = float(min(err / (ae_threshold * 2), 1.0))\n",
        "        # GCN prob if available (use 0.5 default)\n",
        "        gcn_p = 0.5\n",
        "        # try to map postId via df_user if present in df_user; we don't have postId mapping here\n",
        "        # Build feature vector: [content_score, gcn_p] + scaled_feats\n",
        "        vec = np.concatenate(([content_score, gcn_p], scaled_feats))\n",
        "        X_rows.append(vec)\n",
        "        y_rows.append(lbl)\n",
        "    except Exception as e:\n",
        "        print('Skipping URL in classifier dataset due to error:', e)\n",
        "\n",
        "if len(X_rows) >= 10:\n",
        "    X = np.vstack(X_rows)\n",
        "    y = np.array(y_rows)\n",
        "    # Train/test split\n",
        "    X_tr_c, X_te_c, y_tr_c, y_te_c = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "    clf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)\n",
        "    clf.fit(X_tr_c, y_tr_c)\n",
        "    y_pred = clf.predict(X_te_c)\n",
        "    print('Classifier evaluation on held-out fold:')\n",
        "    print(classification_report(y_te_c, y_pred))\n",
        "    # Save classifier and meta\n",
        "    import pickle, json\n",
        "    clf_path = 'phishing_classifier.pkl'\n",
        "    meta = {\n",
        "        'feature_names': ['content_score', 'gcn_prob'] + expected_cols,\n",
        "        'trained_on': len(y),\n",
        "        'timestamp': __import__('datetime').datetime.utcnow().isoformat() + 'Z'\n",
        "    }\n",
        "    with open(clf_path, 'wb') as f:\n",
        "        pickle.dump(clf, f)\n",
        "    with open('classifier_meta.json', 'w') as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "    print('Saved classifier and meta to disk.')\n",
        "else:\n",
        "    print('Not enough data to train classifier; need >=10 samples. Got', len(X_rows))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Ablation evaluation, bootstrap CIs, dataset summary, and deploy helper ---\n",
        "import numpy as np, json\n",
        "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "print('\\n--- Ablation & bootstrap analysis')\n",
        "\n",
        "def eval_probs(name, y_true, y_prob, threshold=0.5):\n",
        "    y_pred = (y_prob > threshold).astype(int)\n",
        "    p,r,f,_ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "    try:\n",
        "        roc = roc_auc_score(y_true, y_prob)\n",
        "    except Exception:\n",
        "        roc = None\n",
        "    prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
        "    pr_auc = auc(rec, prec)\n",
        "    return {'name':name, 'precision':p, 'recall':r, 'f1':f, 'roc_auc':roc, 'pr_auc':pr_auc}\n",
        "\n",
        "results = []\n",
        "# Public test set ablations (if available)\n",
        "if 'y_te' in globals():\n",
        "    # AE-only\n",
        "    if 'ae_prob_test_finetuned' in globals():\n",
        "        results.append(eval_probs('AE-only-public', y_te, ae_prob_test_finetuned))\n",
        "    # GCN-only\n",
        "    gcn_prob_test_public_local = gcn_prob_test_public if 'gcn_prob_test_public' in globals() else np.full(len(y_te), 0.5)\n",
        "    results.append(eval_probs('GCN-only-public', y_te, gcn_prob_test_public_local))\n",
        "    # Hybrid\n",
        "    if 'fusion_prob_test_public' in globals():\n",
        "        results.append(eval_probs('Hybrid-public', y_te, fusion_prob_test_public))\n",
        "\n",
        "# Save ablation results\n",
        "with open('ablation_results.json','w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print('Saved ablation_results.json')\n",
        "print(results)\n",
        "\n",
        "# Bootstrap CI for a metric (recall) on fused public test if present\n",
        "def bootstrap_ci(y_true, y_prob, metric_fn, n=1000, alpha=0.05):\n",
        "    N = len(y_true)\n",
        "    vals = []\n",
        "    for _ in range(n):\n",
        "        idx = np.random.choice(N, N, replace=True)\n",
        "        vals.append(metric_fn(y_true[idx], (y_prob[idx]>0.5).astype(int)))\n",
        "    lo = np.percentile(vals, 100*alpha/2)\n",
        "    hi = np.percentile(vals, 100*(1-alpha/2))\n",
        "    return lo, np.median(vals), hi\n",
        "\n",
        "if 'fusion_prob_test_public' in globals() and 'y_te' in globals():\n",
        "    lo, med, hi = bootstrap_ci(y_te, fusion_prob_test_public, lambda y,t: precision_recall_fscore_support(y,t,average='binary',zero_division=0)[1])\n",
        "    print(f'Bootstrap recall CI (hybrid public): {lo:.3f}, {med:.3f}, {hi:.3f}')\n",
        "    with open('bootstrap_recall_public.json','w') as f:\n",
        "        json.dump({'ci':[lo,med,hi]}, f)\n",
        "\n",
        "# Dataset summary\n",
        "summary = {}\n",
        "try:\n",
        "    summary['public_stats'] = {'n': int(len(df_base))}\n",
        "    if 'y_public' in globals():\n",
        "        summary['public_label_dist'] = dict(Counter(list(y_public)))\n",
        "except Exception:\n",
        "    pass\n",
        "try:\n",
        "    summary['user_reports_count'] = int(len(df_user))\n",
        "    summary['user_label_dist'] = dict(Counter(df_user['label'].tolist()))\n",
        "    # Save some example URLs\n",
        "    summary['user_examples'] = df_user['url'].head(20).tolist()\n",
        "except Exception:\n",
        "    pass\n",
        "with open('dataset_summary.json','w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print('Saved dataset_summary.json')\n",
        "\n",
        "# Deploy helper: zip artifacts for manual download\n",
        "import os\n",
        "artifacts = ['phishing_autoencoder_model.keras','scaler.pkl','autoencoder_threshold.txt','phishing_classifier.pkl','classifier_meta.json','artifact_manifest.json','MODEL_CARD.md']\n",
        "zip_name = 'artifacts_for_deploy.zip'\n",
        "# Build zip (works in Colab)\n",
        "import shutil\n",
        "with shutil.ZipFile(zip_name, 'w') as zf:\n",
        "    for a in artifacts:\n",
        "        if os.path.exists(a):\n",
        "            if os.path.isdir(a):\n",
        "                # add directory contents\n",
        "                for root, dirs, files in os.walk(a):\n",
        "                    for fname in files:\n",
        "                        path = os.path.join(root, fname)\n",
        "                        zf.write(path)\n",
        "            else:\n",
        "                zf.write(a)\n",
        "print('Built', zip_name, '— download and copy to server, then POST /reload_models')\n",
        "\n",
        "print('\\nDone: ablation, bootstrap, dataset summary, deploy zip created.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry2_dvAsTLNc"
      },
      "source": [
        "-----\n",
        "\n",
        "#### **Part 4: Export Final Artifacts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "OpLDmjDwTLNc",
        "outputId": "4b25252b-81ca-43af-c1ba-8ffc7d60e11d"
      },
      "outputs": [],
      "source": [
        "print(\"\\nSaving final artifacts for backend deployment...\")\n",
        "post_node_map = {k.split(\"post:\", 1)[1]: i for i, k in enumerate(idx_to_key) if k.startswith(\"post:\")}\n",
        "with open(\"post_node_map.json\", \"w\") as f:\n",
        "    json.dump(post_node_map, f)\n",
        "np.save(\"gnn_probs.npy\", gcn_prob_all_nodes)\n",
        "if gnn_model:\n",
        "    torch.save(gnn_model.state_dict(), \"gnn_model.pth\")\n",
        "\n",
        "print(\"✅ All artifacts saved.\")\n",
        "\n",
        "print(\"\\nDownloading all artifacts...\")\n",
        "backend_files = [\n",
        "    \"phishing_autoencoder_model.keras\", \"scaler.pkl\", \"autoencoder_threshold.txt\",\n",
        "    \"post_node_map.json\", \"gnn_probs.npy\"\n",
        "]\n",
        "if gnn_model:\n",
        "    backend_files.append(\"gnn_model.pth\")\n",
        "\n",
        "for f in backend_files:\n",
        "  try:\n",
        "    files.download(f)\n",
        "  except Exception as e:\n",
        "    print(f\"Could not download {f}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83e795bb"
      },
      "source": [
        "* * *\n",
        "\n",
        "### **Part 3: Hybrid Fusion and Final Evaluation (Continued)**\n",
        "\n",
        "#### **9. Final Evaluation on the Public Test Set**\n",
        "\n",
        "Now, evaluate the fine-tuned AE and the hybrid model on the public test set to see the overall performance, especially compared to the pre-trained AE baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce71eb69",
        "outputId": "5b94238e-5573-4fd0-edd9-692c1cbf8452"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Evaluating Fine-Tuned AE on Public Test Set ---\")\n",
        "# Get AE Content Scores for the public test set using the fine-tuned model\n",
        "test_reconstructed_finetuned = ae_model.predict(X_te_s, verbose=0)\n",
        "test_error_finetuned = np.mean((test_reconstructed_finetuned - X_te_s)**2, axis=1)\n",
        "\n",
        "# Calculate AE probabilities based on the fine-tuned and optimized threshold\n",
        "# Ensure ae_threshold is defined from previous steps (optimization on user data)\n",
        "ae_prob_test_finetuned = np.clip((test_error_finetuned - ae_threshold * 0.5) / (ae_threshold * 2), 0, 1)\n",
        "\n",
        "# Evaluate the fine-tuned AE on the public test set\n",
        "stats_ae_finetuned_public = print_thesis_stats(\"Fine-Tuned AE (Public Test Data)\", y_te, ae_prob_test_finetuned)\n",
        "\n",
        "print(\"\\n--- Evaluating Hybrid Model on Public Test Set ---\")\n",
        "# Assign a neutral GCN score (0.5) for the public test set as it's not part of the user-generated graph data\n",
        "gcn_prob_test_public = np.full(len(y_te), 0.5)\n",
        "\n",
        "# Fuse the AE and GCN scores using the fixed weights (0.6 for AE, 0.4 for GCN)\n",
        "fusion_prob_test_public = 0.6 * ae_prob_test_finetuned + 0.4 * gcn_prob_test_public\n",
        "\n",
        "# Evaluate the hybrid model on the public test set\n",
        "stats_fusion_public = print_thesis_stats(\"Optimized Hybrid Model (Public Test Data)\", y_te, fusion_prob_test_public)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb1f7c75"
      },
      "source": [
        "* * *\n",
        "\n",
        "#### **10. Visualize Confusion Matrices**\n",
        "\n",
        "Visualize the confusion matrices for the baseline, fine-tuned AE, and hybrid model evaluations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "c8950b6c",
        "outputId": "a3a52049-7721-4671-f5f3-0760cc0da6bd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, model_name):\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Predicted Benign', 'Predicted Phishing'],\n",
        "                yticklabels=['True Benign', 'True Phishing'])\n",
        "    plt.title(f'Confusion Matrix: {model_name}')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- Visualizing Confusion Matrices ---\")\n",
        "\n",
        "# Plot Pre-trained AE (Baseline) Confusion Matrix\n",
        "plot_confusion_matrix(pretrain_stats[\"confusion_matrix\"], \"Pre-trained AE (Baseline on Public Data)\")\n",
        "\n",
        "# Plot Optimized Hybrid Model (User Test) Confusion Matrix\n",
        "if \"confusion_matrix\" in stats_fusion_user_test:\n",
        "    plot_confusion_matrix(stats_fusion_user_test[\"confusion_matrix\"], \"Optimized Hybrid Model (User Test Data)\")\n",
        "else:\n",
        "    print(\"\\nSkipping Optimized Hybrid Model (User Test Data) confusion matrix visualization due to missing data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9d3cf16"
      },
      "source": [
        "* * *\n",
        "\n",
        "#### **11. Visualize ROC Curves**\n",
        "\n",
        "Visualize the ROC curves for the different models to compare their performance across various thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2e55a5d1",
        "outputId": "6148f564-3625-4814-e9f0-5119cc7e34dd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def plot_roc_curve(y_true, y_pred_prob, model_name):\n",
        "    if len(np.unique(y_true)) < 2:\n",
        "        print(f\"\\nSkipping ROC curve for {model_name}: Only one class present in true labels.\")\n",
        "        return\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'Receiver Operating Characteristic (ROC) Curve: {model_name}')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- Visualizing ROC Curves ---\")\n",
        "\n",
        "# Plot ROC for Fine-Tuned AE (User Test Data)\n",
        "if 'ae_post_prob_user_test' in locals() and 'user_test_true_labels' in locals() and len(user_test_true_labels) > 0:\n",
        "    plot_roc_curve(user_test_true_labels, ae_post_prob_user_test, \"Fine-Tuned AE (User Test Data)\")\n",
        "else:\n",
        "    print(\"\\nSkipping Fine-Tuned AE (User Test Data) ROC curve visualization due to missing data.\")\n",
        "\n",
        "# Plot ROC for GCN (User Test Data)\n",
        "if 'gcn_prob_user_test' in locals() and 'user_test_true_labels' in locals() and len(user_test_true_labels) > 0:\n",
        "    plot_roc_curve(user_test_true_labels, gcn_prob_user_test, \"GCN (User Test Data)\")\n",
        "else:\n",
        "    print(\"\\nSkipping GCN (User Test Data) ROC curve visualization due to missing data.\")\n",
        "\n",
        "# Plot ROC for Hybrid Fusion Model (User Test Data)\n",
        "if 'fusion_prob_user_test' in locals() and 'user_test_true_labels' in locals() and len(user_test_true_labels) > 0:\n",
        "     plot_roc_curve(user_test_true_labels, fusion_prob_user_test, \"Hybrid Fusion Model (User Test Data)\")\n",
        "else:\n",
        "     print(\"\\nSkipping Hybrid Fusion Model (User Test Data) ROC curve visualization due to missing data.\")\n",
        "\n",
        "# Optionally, keep plotting on Public Test data as a secondary benchmark\n",
        "print(\"\\n--- Visualizing ROC Curves on Public Test Data (Secondary Benchmark) ---\")\n",
        "\n",
        "# Plot ROC for Fine-Tuned AE (Public Test Data)\n",
        "if 'ae_prob_test_finetuned' in locals() and 'y_te' in locals() and len(y_te) > 0:\n",
        "    plot_roc_curve(y_te, ae_prob_test_finetuned, \"Fine-Tuned AE (Public Test Data)\")\n",
        "else:\n",
        "    print(\"\\nSkipping Fine-Tuned AE (Public Test Data) ROC curve visualization due to missing data.\")\n",
        "\n",
        "# Plot ROC for Hybrid Fusion Model (Public Test Data)\n",
        "if 'fusion_prob_test_public' in locals() and 'y_te' in locals() and len(y_te) > 0:\n",
        "     plot_roc_curve(y_te, fusion_prob_test_public, \"Hybrid Fusion Model (Public Test Data)\")\n",
        "else:\n",
        "     print(\"\\nSkipping Hybrid Fusion Model (Public Test Data) ROC curve visualization due to missing data.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
