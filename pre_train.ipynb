{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Robust Autoencoder Trainer for Phishing Detection (v2)**\n",
        "\n",
        "This notebook implements a revised, more robust training pipeline for the phishing detection Autoencoder, conforming to the thesis proposal. It addresses key challenges identified in the previous version, primarily data scarcity and unreliable threshold optimization.\n",
        "\n",
        "**Key Improvements:**\n",
        "1.  **Cross-Validation (`StratifiedKFold`):** Replaces the single train/validation/test split with k-fold cross-validation to provide a statistically reliable estimate of model performance, which is critical for small user datasets.\n",
        "2.  **Balanced Fine-Tuning:** The fine-tuning process now uses a mix of user-reported benign URLs and a sample of phishing URLs from the public dataset. This prevents the model from \"catastrophically forgetting\" what phishing looks like.\n",
        "3.  **Intelligent Threshold Optimization:** Instead of using a fixed percentile, this version finds the optimal anomaly threshold by analyzing the Precision-Recall curve on each validation fold, maximizing the F1-score.\n",
        "4.  **Robust Performance Metrics:** Final performance is reported as the average and standard deviation across all cross-validation folds, giving a much clearer picture of real-world effectiveness.\n",
        "5.  **Simplified Focus (AE First):** The GCN components are temporarily commented out to focus on perfecting the core content-based detector first. They can be re-enabled once a larger user dataset is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "#### **1. Setup and Installations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install pandas scikit-learn tensorflow tldextract google-cloud-firestore matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "#### **2. Imports and Initial Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tldextract\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# TensorFlow for Autoencoder\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Scikit-learn for preprocessing, cross-validation, and metrics\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
        "                             f1_score, confusion_matrix, roc_auc_score, \n",
        "                             precision_recall_curve, auc)\n",
        "\n",
        "# Google Cloud for data fetching\n",
        "from google.colab import files\n",
        "from google.cloud import firestore\n",
        "\n",
        "# --- Reproducibility ---\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"Libraries imported and seed set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "### **Part 1: Data Loading and Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **3. Load Public Dataset and Authenticate with Firebase**\n",
        "First, we load the large public dataset of URL features. This will be used for pre-training our Autoencoder to give it a general understanding of benign vs. phishing URLs. We also set up our connection to Firestore to fetch user-reported data later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The public dataset contains 111 lexical features extracted from URLs\n",
        "URL = \"https://raw.githubusercontent.com/GregaVrbancic/Phishing-Dataset/master/dataset_full.csv\"\n",
        "print(\"Downloading public dataset...\")\n",
        "df_public = pd.read_csv(URL)\n",
        "\n",
        "# Separate features (X) from the label (y)\n",
        "y_public = df_public[\"phishing\"].astype(int)\n",
        "X_public = df_public.drop(columns=[\"phishing\"]).astype(np.float32)\n",
        "feature_cols = X_public.columns.tolist()\n",
        "\n",
        "print(f\"Public dataset loaded with {X_public.shape[0]} samples and {X_public.shape[1]} features.\")\n",
        "\n",
        "# --- Firebase Authentication ---\n",
        "print(\"\\nPlease upload your Firebase service account JSON key file.\")\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "    sa_path = next(iter(uploaded.keys()))\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
        "    db = firestore.Client()\n",
        "    print(\"\\n✅ Firebase authentication configured.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Firebase authentication failed: {e}\")\n",
        "    db = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **4. Feature Engineering and Scaling**\n",
        "Here we define the functions to extract lexical features from a URL. Crucially, we also define our feature scaler (`RobustScaler` is preferred for its resilience to outliers). This scaler will be fitted **only on the benign data** from the public training set to learn the distribution of \"normal\" URLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lexical_features(url: str) -> dict:\n",
        "    \"\"\"Extracts basic lexical features from a URL string.\"\"\"\n",
        "    features = {}\n",
        "    try:\n",
        "        ext = tldextract.extract(url)\n",
        "        domain = f\"{ext.domain}.{ext.suffix}\"\n",
        "        hostname = f\"{ext.subdomain}.{domain}\" if ext.subdomain else domain\n",
        "        \n",
        "        features['qty_dot_url'] = url.count('.')\n",
        "        features['qty_hyphen_url'] = url.count('-')\n",
        "        features['qty_underline_url'] = url.count('_')\n",
        "        features['qty_slash_url'] = url.count('/')\n",
        "        features['qty_questionmark_url'] = url.count('?')\n",
        "        features['qty_equal_url'] = url.count('=')\n",
        "        features['qty_at_url'] = url.count('@')\n",
        "        features['qty_and_url'] = url.count('&')\n",
        "        features['qty_exclamation_url'] = url.count('!')\n",
        "        features['qty_space_url'] = url.count(' ')\n",
        "        features['qty_tilde_url'] = url.count('~')\n",
        "        features['qty_comma_url'] = url.count(',')\n",
        "        features['qty_plus_url'] = url.count('+')\n",
        "        features['qty_asterisk_url'] = url.count('*')\n",
        "        features['qty_hashtag_url'] = url.count('#')\n",
        "        features['qty_dollar_url'] = url.count('$')\n",
        "        features['qty_percent_url'] = url.count('%')\n",
        "        features['qty_dot_domain'] = domain.count('.')\n",
        "        features['qty_hyphen_domain'] = domain.count('-')\n",
        "        features['qty_underline_domain'] = domain.count('_')\n",
        "        features['qty_at_domain'] = domain.count('@')\n",
        "        features['qty_vowels_domain'] = sum(1 for char in domain if char in 'aeiouAEIOU')\n",
        "        features['domain_length'] = len(domain)\n",
        "        features['domain_in_ip'] = 1 if all(part.isdigit() for part in domain.split('.')) else 0\n",
        "        features['server_client_domain'] = 1 if 'server' in domain or 'client' in domain else 0\n",
        "        \n",
        "    except Exception:\n",
        "        # Return empty dict on error\n",
        "        return {}\n",
        "    return features\n",
        "\n",
        "def create_feature_vector(url: str, all_feature_columns: list) -> np.ndarray:\n",
        "    \"\"\"Creates a full feature vector for a URL, filling missing values with 0.\"\"\"\n",
        "    lexical_feats = get_lexical_features(url)\n",
        "    feature_vector = np.array([lexical_feats.get(col, 0.0) for col in all_feature_columns], dtype=np.float32)\n",
        "    return feature_vector\n",
        "\n",
        "# Split public data to get a set for fitting the scaler\n",
        "X_public_train, _, y_public_train, _ = train_test_split(\n",
        "    X_public, y_public, test_size=0.3, random_state=SEED, stratify=y_public\n",
        ")\n",
        "\n",
        "# --- Feature Scaling ---\n",
        "print(\"Fitting RobustScaler on benign public training data...\")\n",
        "# Using RobustScaler is better for data with outliers, which is common in lexical features.\n",
        "scaler = RobustScaler().fit(X_public_train[y_public_train == 0])\n",
        "\n",
        "print(\"✅ Scaler fitted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "### **Part 2: Autoencoder Pre-training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **5. Define and Pre-train the Autoencoder Model**\n",
        "We define the Autoencoder architecture as specified in the thesis. Then, we pre-train it **only on the benign samples** from the public dataset. This teaches the model to accurately reconstruct \"normal\" URLs, establishing a strong baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Autoencoder hyperparameters (from thesis)\n",
        "AE_LAYER1 = 64\n",
        "AE_LAYER2 = 32\n",
        "AE_BOTTLENECK = 16\n",
        "AE_DROPOUT = 0.1\n",
        "LR_PRETRAIN = 1e-3\n",
        "BATCH_PRETRAIN = 512\n",
        "EPOCHS_PRETRAIN = 30\n",
        "\n",
        "def build_autoencoder(input_shape):\n",
        "    \"\"\"Builds the Keras Autoencoder model.\"\"\"\n",
        "    inp = keras.Input(shape=(input_shape,))\n",
        "    x = keras.layers.Dense(AE_LAYER1, activation='relu')(inp)\n",
        "    x = keras.layers.Dropout(AE_DROPOUT)(x)\n",
        "    x = keras.layers.Dense(AE_LAYER2, activation='relu')(x)\n",
        "    z = keras.layers.Dense(AE_BOTTLENECK, activation='relu', name='bottleneck')(x)  # Bottleneck\n",
        "    x = keras.layers.Dense(AE_LAYER2, activation='relu')(z)\n",
        "    x = keras.layers.Dense(AE_LAYER1, activation='relu')(x)\n",
        "    out = keras.layers.Dense(input_shape)(x)\n",
        "    \n",
        "    model = keras.Model(inp, out)\n",
        "    return model\n",
        "\n",
        "# Build and compile the model for pre-training\n",
        "pretrain_ae_model = build_autoencoder(X_public.shape[1])\n",
        "pretrain_ae_model.compile(optimizer=keras.optimizers.Adam(LR_PRETRAIN), loss='mse')\n",
        "\n",
        "print('--- Pre-training Autoencoder on Public Dataset ---')\n",
        "\n",
        "# Scale the training data\n",
        "X_public_train_scaled = scaler.transform(X_public_train)\n",
        "\n",
        "# Train only on benign data\n",
        "X_benign_public_train = X_public_train_scaled[y_public_train == 0]\n",
        "\n",
        "history = pretrain_ae_model.fit(\n",
        "    X_benign_public_train, X_benign_public_train,\n",
        "    epochs=EPOCHS_PRETRAIN,\n",
        "    batch_size=BATCH_PRETRAIN,\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    validation_split=0.2, # Use a portion of training data for validation\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "print('\\n✅ Pre-training complete.')\n",
        "\n",
        "# Save the pre-trained weights for use in cross-validation\n",
        "pretrain_ae_model.save_weights('pretrained_ae_weights.weights.h5')\n",
        "print('Pre-trained model weights saved.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "### **Part 3: User Data Processing and Fine-Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **6. Fetch and Process User-Reported URLs**\n",
        "Now we fetch the labeled data provided by your application's users from Firestore. We process these reports to create a clean dataset of unique URLs and their corresponding labels (0 for benign, 1 for phishing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_user_reports(db_client):\n",
        "    \"\"\"Fetches and processes user reports from Firestore.\"\"\"\n",
        "    if not db_client:\n",
        "        print(\"Firestore client not initialized. Skipping user data fetch.\")\n",
        "        return pd.DataFrame(columns=['url', 'label'])\n",
        "\n",
        "    APP_ID = \"ads-phishing-link\"  # As per your app.py\n",
        "    REPORTS_PATH = f\"artifacts/{APP_ID}/private_user_reports\"\n",
        "    print(f\"Fetching user reports from: {REPORTS_PATH}...\")\n",
        "\n",
        "    try:\n",
        "        report_docs = list(db_client.collection(REPORTS_PATH).stream())\n",
        "        print(f\"Found {len(report_docs)} total user reports.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error fetching from Firestore: {e}\")\n",
        "        return pd.DataFrame(columns=['url', 'label'])\n",
        "\n",
        "    # Process reports into a list of {'url': ..., 'label': ...} dicts\n",
        "    processed_reports = []\n",
        "    for doc in report_docs:\n",
        "        d = doc.to_dict()\n",
        "        report_type = d.get('type')\n",
        "        payload = d.get('payload', {})\n",
        "        url = payload.get('url')\n",
        "\n",
        "        if not url or not report_type:\n",
        "            continue\n",
        "\n",
        "        # Map report type to binary label (as per your thesis)\n",
        "        # 1 = Phishing (true_positive, false_negative)\n",
        "        # 0 = Benign (false_positive, true_negative)\n",
        "        label = 1 if report_type in ('true_positive', 'false_negative') else 0\n",
        "        processed_reports.append({'url': url, 'label': label})\n",
        "\n",
        "    if not processed_reports:\n",
        "        print(\"No valid reports found.\")\n",
        "        return pd.DataFrame(columns=['url', 'label'])\n",
        "        \n",
        "    # Create DataFrame and drop duplicates to get unique labeled URLs\n",
        "    df_user = pd.DataFrame(processed_reports).drop_duplicates().reset_index(drop=True)\n",
        "    print(f\"Created a dataset of {len(df_user)} unique user-reported URLs.\")\n",
        "    print(\"User data label distribution:\")\n",
        "    print(df_user['label'].value_counts())\n",
        "    return df_user\n",
        "\n",
        "# Fetch the data\n",
        "df_user_reports = fetch_user_reports(db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **7. Prepare Data for Cross-Validation and Fine-Tuning**\n",
        "This is a critical step. We create the final feature matrix (`X_user`) and label vector (`y_user`) from the user reports. \n",
        "\n",
        "More importantly, to prevent **catastrophic forgetting**, we create a balanced fine-tuning dataset. For each cross-validation fold, the training set will consist of:\n",
        "1.  All **benign** URLs from the user training fold.\n",
        "2.  All **phishing** URLs from the user training fold.\n",
        "3.  An equal number of **phishing** URLs sampled from the original public dataset.\n",
        "\n",
        "This ensures that while the model learns from your specific user data, it doesn't forget the general features of phishing URLs it learned during pre-training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df_user_reports.empty:\n",
        "    # Create feature vectors for all user-reported URLs\n",
        "    X_user = np.array([create_feature_vector(url, feature_cols) for url in df_user_reports['url']])\n",
        "    y_user = df_user_reports['label'].values\n",
        "\n",
        "    # Get phishing examples from the public dataset to prevent forgetting\n",
        "    X_public_phish = X_public[y_public == 1]\n",
        "    \n",
        "    print(f\"User data prepared for cross-validation: X_user shape {X_user.shape}, y_user shape {y_user.shape}\")\n",
        "    print(f\"Found {len(X_public_phish)} phishing examples in public data for balanced fine-tuning.\")\n",
        "else:\n",
        "    print(\"User reports DataFrame is empty. Cannot proceed with fine-tuning.\")\n",
        "    # Create empty arrays to avoid errors in subsequent cells if run out of order\n",
        "    X_user, y_user, X_public_phish = np.array([]), np.array([]), np.array([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "### **Part 4: Cross-Validation, Fine-Tuning, and Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **8. The Cross-Validation and Fine-Tuning Loop**\n",
        "This is the core of the robust training process. We use `StratifiedKFold` to split our small user dataset into multiple folds, ensuring each fold has a similar ratio of benign to phishing URLs. \n",
        "\n",
        "For each fold, we:\n",
        "1.  **Load** the pre-trained AE weights.\n",
        "2.  **Fine-tune** the model on the balanced training set for that fold.\n",
        "3.  **Evaluate** on the validation set to find the best anomaly threshold.\n",
        "4.  **Store** the performance metrics and the optimized threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(X_user) > 0:\n",
        "    # --- Cross-Validation Setup ---\n",
        "    N_SPLITS = 5  # Use 5 folds for a good balance of computation and reliability\n",
        "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
        "\n",
        "    # --- Fine-Tuning Hyperparameters ---\n",
        "    LR_FINETUNE = 1e-5\n",
        "    BATCH_FINETUNE = 16\n",
        "    EPOCHS_FINETUNE = 50\n",
        "\n",
        "    # --- Storage for results across folds ---\n",
        "    fold_results = []\n",
        "    best_thresholds = []\n",
        "\n",
        "    # --- The Loop ---\n",
        "    for fold_idx, (train_indices, val_indices) in enumerate(skf.split(X_user, y_user)):\n",
        "        print(f\"\\n--- Starting Fold {fold_idx + 1}/{N_SPLITS} ---\")\n",
        "\n",
        "        # 1. Get data for this fold\n",
        "        X_train_fold, X_val_fold = X_user[train_indices], X_user[val_indices]\n",
        "        y_train_fold, y_val_fold = y_user[train_indices], y_user[val_indices]\n",
        "\n",
        "        # 2. Create the balanced fine-tuning dataset\n",
        "        num_user_phish = np.sum(y_train_fold == 1)\n",
        "        if num_user_phish > 0:\n",
        "            # Sample from public phishing data to balance\n",
        "            sample_indices = np.random.choice(len(X_public_phish), size=num_user_phish, replace=False)\n",
        "            X_public_phish_sample = X_public_phish.iloc[sample_indices]\n",
        "            \n",
        "            # Combine user data with public phishing data\n",
        "            X_finetune = np.vstack([X_train_fold, X_public_phish_sample])\n",
        "            y_finetune = np.concatenate([y_train_fold, np.ones(num_user_phish)])\n",
        "        else: # Handle case with no phishing examples in user training fold\n",
        "            X_finetune = X_train_fold\n",
        "            y_finetune = y_train_fold\n",
        "        \n",
        "        # 3. Scale the fine-tuning and validation data\n",
        "        X_finetune_scaled = scaler.transform(X_finetune)\n",
        "        X_val_scaled = scaler.transform(X_val_fold)\n",
        "\n",
        "        # 4. Build a fresh model and load pre-trained weights\n",
        "        fine_tune_ae = build_autoencoder(X_user.shape[1])\n",
        "        fine_tune_ae.load_weights('pretrained_ae_weights.weights.h5')\n",
        "        fine_tune_ae.compile(optimizer=keras.optimizers.Adam(LR_FINETUNE), loss='mse')\n",
        "\n",
        "        # 5. Fine-tune the model ONLY on BENIGN data from the balanced set\n",
        "        X_benign_finetune = X_finetune_scaled[y_finetune == 0]\n",
        "        if len(X_benign_finetune) > 0:\n",
        "            print(f\"Fine-tuning on {len(X_benign_finetune)} benign samples...\")\n",
        "            fine_tune_ae.fit(\n",
        "                X_benign_finetune, X_benign_finetune,\n",
        "                epochs=EPOCHS_FINETUNE,\n",
        "                batch_size=BATCH_FINETUNE,\n",
        "                shuffle=True,\n",
        "                verbose=0,\n",
        "                callbacks=[keras.callbacks.EarlyStopping(patience=5, monitor='loss', restore_best_weights=True)]\n",
        "            )\n",
        "        else:\n",
        "            print(\"No benign samples in this fold for fine-tuning.\")\n",
        "\n",
        "        # 6. Evaluate on the validation fold to find the best threshold\n",
        "        if len(X_val_fold) > 0:\n",
        "            val_reconstructed = fine_tune_ae.predict(X_val_scaled, verbose=0)\n",
        "            val_errors = np.mean(np.square(X_val_scaled - val_reconstructed), axis=1)\n",
        "\n",
        "            # --- Intelligent Threshold Optimization via PR Curve ---\n",
        "            precision, recall, thresholds = precision_recall_curve(y_val_fold, val_errors)\n",
        "            # Calculate F1 score for each threshold, avoiding division by zero\n",
        "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "            \n",
        "            best_f1_idx = np.argmax(f1_scores)\n",
        "            best_threshold = thresholds[best_f1_idx]\n",
        "            best_f1 = f1_scores[best_f1_idx]\n",
        "            best_thresholds.append(best_threshold)\n",
        "            print(f\"Best Threshold found: {best_threshold:.4f} (with F1 score: {best_f1:.4f})\")\n",
        "\n",
        "            # 7. Calculate and store performance metrics for this fold\n",
        "            y_pred_val = (val_errors > best_threshold).astype(int)\n",
        "            fold_metrics = {\n",
        "                'accuracy': accuracy_score(y_val_fold, y_pred_val),\n",
        "                'precision': precision_score(y_val_fold, y_pred_val, zero_division=0),\n",
        "                'recall': recall_score(y_val_fold, y_pred_val, zero_division=0),\n",
        "                'f1': f1_score(y_val_fold, y_pred_val, zero_division=0),\n",
        "                'roc_auc': roc_auc_score(y_val_fold, val_errors) if len(np.unique(y_val_fold)) > 1 else 0.5\n",
        "            }\n",
        "            fold_results.append(fold_metrics)\n",
        "            print(f\"Validation Metrics for Fold {fold_idx + 1}: {fold_metrics}\")\n",
        "        else:\n",
        "            print(\"Validation fold is empty, skipping evaluation for this fold.\")\n",
        "else:\n",
        "    print(\"Cannot run cross-validation as there is no user data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **9. Aggregate and Display Final Performance**\n",
        "After completing all folds, we calculate the average and standard deviation of the performance metrics. This gives us a much more robust and realistic understanding of how the model is expected to perform on new, unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if fold_results:\n",
        "    df_results = pd.DataFrame(fold_results)\n",
        "    print(\"\\n--- Cross-Validation Results Summary ---\")\n",
        "    print(df_results)\n",
        "\n",
        "    print(\"\\n--- Average Performance Metrics (± Std Dev) ---\")\n",
        "    mean_metrics = df_results.mean()\n",
        "    std_metrics = df_results.std()\n",
        "    summary_df = pd.concat([mean_metrics, std_metrics], axis=1)\n",
        "    summary_df.columns = ['Mean', 'Std Dev']\n",
        "    print(summary_df)\n",
        "    \n",
        "    # Determine the final threshold, e.g., by taking the mean of the best thresholds\n",
        "    final_threshold = np.mean(best_thresholds)\n",
        "    print(f\"\\n✅ Final Optimized Anomaly Threshold (Mean across folds): {final_threshold:.6f}\")\n",
        "else:\n",
        "    print(\"\\nNo results to aggregate.\")\n",
        "    final_threshold = 0.5 # Default fallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "### **Part 5: Final Model Training and Artifact Export**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **10. Retrain Final Model on All User Data**\n",
        "With the hyperparameters validated and an optimal thresholding strategy established, we now train the final model. We use the **entire user dataset** for this final fine-tuning step to ensure the deployed model has learned from all available information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(X_user) > 0:\n",
        "    print(\"\\n--- Training Final Model on ALL User Data ---\")\n",
        "    \n",
        "    # Create the final balanced fine-tuning dataset using all user data\n",
        "    num_user_phish_total = np.sum(y_user == 1)\n",
        "    if num_user_phish_total > 0:\n",
        "        sample_indices_final = np.random.choice(len(X_public_phish), size=num_user_phish_total, replace=False)\n",
        "        X_public_phish_sample_final = X_public_phish.iloc[sample_indices_final]\n",
        "        X_final_train = np.vstack([X_user, X_public_phish_sample_final])\n",
        "        y_final_train = np.concatenate([y_user, np.ones(num_user_phish_total)])\n",
        "    else:\n",
        "        X_final_train = X_user\n",
        "        y_final_train = y_user\n",
        "\n",
        "    # Scale the data\n",
        "    X_final_train_scaled = scaler.transform(X_final_train)\n",
        "    X_benign_final_train = X_final_train_scaled[y_final_train == 0]\n",
        "\n",
        "    # Build and compile the final model\n",
        "    final_ae_model = build_autoencoder(X_user.shape[1])\n",
        "    final_ae_model.load_weights('pretrained_ae_weights.weights.h5') # Start from pre-trained state\n",
        "    final_ae_model.compile(optimizer=keras.optimizers.Adam(LR_FINETUNE), loss='mse')\n",
        "\n",
        "    # Fine-tune on all available benign user data\n",
        "    if len(X_benign_final_train) > 0:\n",
        "        print(f\"Fine-tuning final model on {len(X_benign_final_train)} benign samples...\")\n",
        "        final_ae_model.fit(\n",
        "            X_benign_final_train, X_benign_final_train,\n",
        "            epochs=EPOCHS_FINETUNE,\n",
        "            batch_size=BATCH_FINETUNE,\n",
        "            shuffle=True,\n",
        "            verbose=1,\n",
        "            callbacks=[keras.callbacks.EarlyStopping(patience=5, monitor='loss', restore_best_weights=True)]\n",
        "        )\n",
        "        print(\"\\n✅ Final model training complete.\")\n",
        "    else:\n",
        "        print(\"No benign user data available for final training. Using pre-trained model as final.\")\n",
        "        final_ae_model = pretrain_ae_model # Fallback to the pre-trained model\n",
        "else:\n",
        "    print(\"\\nNo user data. The final model will be the pre-trained public model.\")\n",
        "    final_ae_model = pretrain_ae_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **11. Export Artifacts for Application**\n",
        "Finally, we save the three essential artifacts required by your backend application:\n",
        "1.  `phishing_autoencoder_model.keras`: The trained model itself.\n",
        "2.  `scaler.pkl`: The fitted scaler.\n",
        "3.  `autoencoder_threshold.txt`: The optimized anomaly threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Exporting Final Artifacts ---\")\n",
        "\n",
        "# 1. Save the final trained model\n",
        "final_ae_model.save(\"phishing_autoencoder_model.keras\")\n",
        "print(\"Saved phishing_autoencoder_model.keras\")\n",
        "\n",
        "# 2. Save the scaler\n",
        "with open(\"scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(\"Saved scaler.pkl\")\n",
        "\n",
        "# 3. Save the optimized threshold\n",
        "with open(\"autoencoder_threshold.txt\", \"w\") as f:\n",
        "    f.write(str(final_threshold))\n",
        "print(f\"Saved autoencoder_threshold.txt with value: {final_threshold}\")\n",
        "\n",
        "print(\"\\n✅ All artifacts exported successfully. You can now download them from the Colab files panel.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
