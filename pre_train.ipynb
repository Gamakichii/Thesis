{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyqYacJ-AzA5"
   },
   "source": [
    "### **Robust Hybrid Phishing Detection Trainer (Live Datasets) v7**\n",
    "\n",
    "This notebook implements the complete training pipeline using dynamic, real-world data sources. It downloads the latest phishing URLs from the `Phishing.Database` project and uses the Tranco Top 1 Million list for benign examples.\n",
    "\n",
    "**Key Features & Changes in this Version:**\n",
    "1.  **Fully Unified Feature Extraction:** The feature extraction logic now perfectly mirrors the live backend. It cleans tracking parameters (e.g., `fbclid`), resolves URL shorteners, and then extracts features, eliminating data mismatch.\n",
    "2.  **Full Data Utilization:** All undersampling has been removed. The model now trains on all available public and user-reported data for maximum learning.\n",
    "3.  **Live Data Sources:** Ingests data directly from the Phishing.Database project and the Tranco list, ensuring the model is trained on current threats.\n",
    "4.  **Full Hybrid System:** Implements the complete AE + GCN training and evaluation pipeline using a robust cross-validation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYE1VTrCAzA6"
   },
   "source": [
    "## 1. Setup and Installations âš™ï¸\n",
    "This first code block prepares the environment. It runs installation commands to get all the special tools (software libraries) needed for the project. This includes `requests` for resolving URL shorteners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvyeFGt9AzA7",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Install core data science, deep learning, and cloud libraries\n",
    "!pip -q install pandas scikit-learn tensorflow tldextract google-cloud-firestore matplotlib seaborn requests\n",
    "\n",
    "# Install PyTorch and PyTorch Geometric for the GNN\n",
    "!pip -q install torch torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZB9Sh9aAzA7"
   },
   "source": [
    "## 2. Imports and Initial Configuration ðŸ“š\n",
    "This cell unpacks the project's toolbox. It **imports** all the specific functions and classes from the installed libraries so they are ready to use, including `urllib.parse` for cleaning URLs. It also sets a **random seed**, a number that ensures if the notebook is run again, the results will be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EpjWEaXAzA7",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tldextract\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode\n",
    "\n",
    "# TensorFlow for Autoencoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# PyTorch for GCN\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Scikit-learn for preprocessing, cross-validation, and metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    ")\n",
    "\n",
    "# Google Cloud for data fetching\n",
    "from google.colab import files\n",
    "from google.cloud import firestore\n",
    "# Corrected import for Query\n",
    "from google.cloud.firestore_v1 import Query\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Libraries imported and seed set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnKTf2pXAzA8"
   },
   "source": [
    "## 3. Fetch Live Phishing and Benign URL Datasets â˜ï¸\n",
    "This cell downloads the latest data files. For phishing examples, it gets a list of currently active phishing URLs from `Phishing.Database`. For safe examples, it downloads the Tranco Top 1 Million list, which contains the most popular (and generally safe) websites on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgpV-AUGAzA8",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Fetch Phishing URLs by direct download\n",
    "print(\"Downloading latest active phishing links...\")\n",
    "!wget https://phish.co.za/latest/phishing-links-ACTIVE.txt -O phishing-links-ACTIVE.txt\n",
    "\n",
    "# Fetch Benign URLs (Tranco Top 1 Million)\n",
    "print(\"\\nDownloading Tranco Top 1 Million list...\")\n",
    "!wget https://tranco-list.eu/top-1m.csv.zip -O top-1m.csv.zip\n",
    "!unzip -o top-1m.csv.zip\n",
    "\n",
    "print(\"\\nâœ… Datasets downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHtk07y_AzA8"
   },
   "source": [
    "## 4. Parse and Combine Datasets ðŸ“‹\n",
    "This code reads the raw data from the downloaded files and organizes it into a single dataset. This version uses all available data without undersampling to provide the model with the maximum amount of information for pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgjDCdo7AzA8",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Parse Phishing URLs from the downloaded text file\n",
    "try:\n",
    "    with open('phishing-links-ACTIVE.txt', 'r', encoding='utf-8') as f:\n",
    "        phishing_urls = [line.strip() for line in f if line.strip()]\n",
    "    df_phish = pd.DataFrame(phishing_urls, columns=['url'])\n",
    "    df_phish['label'] = 1\n",
    "    df_phish.drop_duplicates(inplace=True)\n",
    "    print(f\"Parsed {len(df_phish)} unique active phishing URLs.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"phishing-links-ACTIVE.txt not found. Creating an empty phishing dataframe.\")\n",
    "    df_phish = pd.DataFrame(columns=['url', 'label'])\n",
    "\n",
    "# Parse Benign URLs from Tranco list\n",
    "df_benign_full = pd.read_csv('top-1m.csv', names=['rank', 'domain'])\n",
    "df_benign_full['url'] = 'http://' + df_benign_full['domain']\n",
    "df_benign_full['label'] = 0\n",
    "print(f\"Parsed {len(df_benign_full)} benign domains from Tranco list.\")\n",
    "\n",
    "# CORRECTED: Combine all available public data without undersampling\n",
    "df_public_raw = pd.concat([df_phish, df_benign_full[['url', 'label']]], ignore_index=True)\n",
    "df_public_raw = df_public_raw.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nCreated a combined dataset with {len(df_public_raw)} total URLs.\")\n",
    "print(df_public_raw['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlCMrFLHAzA9"
   },
   "source": [
    "## 5. Feature Engineering and Scaling ðŸ“\n",
    "This is a computationally intensive step that **turns each URL into a list of numbers** a machine can understand. This function is now fully unified with the live backend's logic: it first **cleans tracking parameters** (like `fbclid`), then **resolves URL shorteners**, and finally extracts the numerical features from the final, processed URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj-2m8bVAzA9",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Define the list of lexical features we will extract.\n",
    "LEXICAL_FEATURE_COLUMNS = [\n",
    "    'qty_dot_url', 'qty_hyphen_url', 'qty_underline_url', 'qty_slash_url',\n",
    "    'qty_questionmark_url', 'qty_equal_url', 'qty_at_url', 'qty_and_url',\n",
    "    'qty_exclamation_url', 'qty_space_url', 'qty_tilde_url', 'qty_comma_url',\n",
    "    'qty_plus_url', 'qty_asterisk_url', 'qty_hashtag_url', 'qty_dollar_url',\n",
    "    'qty_percent_url', 'qty_dot_domain', 'qty_hyphen_domain', 'qty_underline_domain',\n",
    "    'qty_at_domain', 'qty_vowels_domain', 'domain_length', 'domain_in_ip',\n",
    "    'server_client_domain', 'url_shortened'\n",
    "]\n",
    "\n",
    "# Using a session object for connection pooling is more efficient for many requests\n",
    "session = requests.Session()\n",
    "\n",
    "def get_lexical_features(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts lexical features from a URL, including cleaning tracking parameters \n",
    "    and resolving shorteners to match the backend logic.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    raw_url = str(url)\n",
    "    \n",
    "    try:\n",
    "        # 1. NORMALIZE URL: Remove common tracking query params (logic from app.py)\n",
    "        normalized_url = raw_url\n",
    "        try:\n",
    "            p = urlparse(raw_url)\n",
    "            if p.query:\n",
    "                q = parse_qsl(p.query, keep_blank_values=True)\n",
    "                q_filtered = [(k, v) for k, v in q if not (k.lower() == 'fbclid' or k.lower().startswith('utm_'))]\n",
    "                new_query = urlencode(q_filtered)\n",
    "                p = p._replace(query=new_query)\n",
    "                normalized_url = urlunparse(p)\n",
    "        except Exception:\n",
    "            normalized_url = raw_url # Fallback to original if parsing fails\n",
    "\n",
    "        # 2. RESOLVE SHORTENERS: Use the normalized URL for this step\n",
    "        final_url = normalized_url\n",
    "        shorteners = {\"bit.ly\",\"tinyurl.com\",\"t.co\",\"goo.gl\",\"ow.ly\",\"is.gd\",\"cutt.ly\",\"lnkd.in\",\"buff.ly\"}\n",
    "        ext_original = tldextract.extract(normalized_url)\n",
    "        original_domain_suffix = f\"{ext_original.domain}.{ext_original.suffix}\"\n",
    "        is_shortened = 1 if original_domain_suffix in shorteners else 0\n",
    "        features['url_shortened'] = is_shortened\n",
    "\n",
    "        if is_shortened:\n",
    "            try:\n",
    "                res = session.head(normalized_url, allow_redirects=True, timeout=2)\n",
    "                final_url = res.url\n",
    "            except requests.exceptions.RequestException:\n",
    "                final_url = normalized_url # Fallback if resolution fails\n",
    "\n",
    "        # 3. EXTRACT FEATURES: Use the final, processed URL\n",
    "        ext = tldextract.extract(final_url)\n",
    "        domain = f\"{ext.domain}.{ext.suffix}\"\n",
    "\n",
    "        features['qty_dot_url'] = final_url.count('.')\n",
    "        features['qty_hyphen_url'] = final_url.count('-')\n",
    "        features['qty_underline_url'] = final_url.count('_')\n",
    "        features['qty_slash_url'] = final_url.count('/')\n",
    "        features['qty_questionmark_url'] = final_url.count('?')\n",
    "        features['qty_equal_url'] = final_url.count('=')\n",
    "        features['qty_at_url'] = final_url.count('@')\n",
    "        features['qty_and_url'] = final_url.count('&')\n",
    "        features['qty_exclamation_url'] = final_url.count('!')\n",
    "        features['qty_space_url'] = final_url.count(' ')\n",
    "        features['qty_tilde_url'] = final_url.count('~')\n",
    "        features['qty_comma_url'] = final_url.count(',')\n",
    "        features['qty_plus_url'] = final_url.count('+')\n",
    "        features['qty_asterisk_url'] = final_url.count('*')\n",
    "        features['qty_hashtag_url'] = final_url.count('#')\n",
    "        features['qty_dollar_url'] = final_url.count('$')\n",
    "        features['qty_percent_url'] = final_url.count('%')\n",
    "        features['qty_dot_domain'] = domain.count('.')\n",
    "        features['qty_hyphen_domain'] = domain.count('-')\n",
    "        features['qty_underline_domain'] = domain.count('_')\n",
    "        features['qty_at_domain'] = domain.count('@')\n",
    "        features['qty_vowels_domain'] = sum(1 for char in domain if char in 'aeiouAEIOU')\n",
    "        features['domain_length'] = len(domain)\n",
    "        features['domain_in_ip'] = 1 if re.fullmatch(r\"\\d+\\.\\d+\\.\\d+\\.\\d+\", domain) else 0\n",
    "        features['server_client_domain'] = 1 if 'server' in domain or 'client' in domain else 0\n",
    "\n",
    "    except Exception:\n",
    "        # Return a dictionary of zeros if any error occurs\n",
    "        return {col: 0 for col in LEXICAL_FEATURE_COLUMNS}\n",
    "    return features\n",
    "\n",
    "def create_feature_vector(url: str, all_feature_columns: list) -> np.ndarray:\n",
    "    \"\"\"Creates a full feature vector for a URL, filling missing values with 0.\"\"\"\n",
    "    lexical_feats = get_lexical_features(url)\n",
    "    feature_vector = np.array([lexical_feats.get(col, 0.0) for col in all_feature_columns], dtype=np.float32)\n",
    "    return feature_vector\n",
    "\n",
    "print(f\"Extracting {len(LEXICAL_FEATURE_COLUMNS)} lexical features from {len(df_public_raw)} URLs...\")\n",
    "tqdm.pandas(desc=\"Feature Extraction\")\n",
    "feature_vectors = df_public_raw['url'].progress_apply(lambda url: create_feature_vector(url, LEXICAL_FEATURE_COLUMNS))\n",
    "\n",
    "# Create the final feature matrix and label vector\n",
    "X_public = np.vstack(feature_vectors.values)\n",
    "y_public = df_public_raw['label'].values\n",
    "\n",
    "print(f\"\\nFeature extraction complete. X_public shape: {X_public.shape}\")\n",
    "\n",
    "# --- Feature Scaling ---\n",
    "print(\"\\nFitting RobustScaler on benign public training data...\")\n",
    "X_public_train_df = pd.DataFrame(X_public, columns=LEXICAL_FEATURE_COLUMNS)\n",
    "X_public_train, X_public_test, y_public_train, y_public_test = train_test_split(\n",
    "    X_public_train_df, y_public, test_size=0.3, random_state=SEED, stratify=y_public\n",
    ")\n",
    "scaler = RobustScaler().fit(X_public_train[y_public_train == 0])\n",
    "\n",
    "print(\"âœ… Scaler fitted and saved.\")\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ud7o1ZIKAzA9"
   },
   "source": [
    "## 6. Define and Pre-train the Autoencoder Model ðŸ§ \n",
    "This cell builds and trains the first of the two main models: the **Autoencoder (AE)**. Its job is to learn what a \"normal,\" safe URL looks like. This initial training is done using **only the benign (safe) links** from the live dataset. This way, it becomes an expert at spotting any URL that looks weird or anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KE3oBwYCAzA9",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Autoencoder hyperparameters\n",
    "AE_LAYER1 = 16 # Adjusted for smaller feature set\n",
    "AE_LAYER2 = 8\n",
    "AE_BOTTLENECK = 4\n",
    "AE_DROPOUT = 0.1\n",
    "LR_PRETRAIN = 1e-3\n",
    "BATCH_PRETRAIN = 512\n",
    "EPOCHS_PRETRAIN = 30\n",
    "\n",
    "def build_autoencoder(input_shape):\n",
    "    \"\"\"Builds the Keras Autoencoder model.\"\"\"\n",
    "    inp = keras.Input(shape=(input_shape,))\n",
    "    x = keras.layers.Dense(AE_LAYER1, activation='relu')(inp)\n",
    "    x = keras.layers.Dropout(AE_DROPOUT)(x)\n",
    "    x = keras.layers.Dense(AE_LAYER2, activation='relu')(x)\n",
    "    z = keras.layers.Dense(AE_BOTTLENECK, activation='relu', name='bottleneck')(x)\n",
    "    x = keras.layers.Dense(AE_LAYER2, activation='relu')(z)\n",
    "    x = keras.layers.Dense(AE_LAYER1, activation='relu')(x)\n",
    "    out = keras.layers.Dense(input_shape)(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    return model\n",
    "\n",
    "# Build and compile the model for pre-training\n",
    "pretrain_ae_model = build_autoencoder(X_public.shape[1])\n",
    "pretrain_ae_model.compile(optimizer=keras.optimizers.Adam(LR_PRETRAIN), loss='mse')\n",
    "\n",
    "print('--- Pre-training Autoencoder on New Public Dataset ---')\n",
    "X_public_train_scaled = scaler.transform(X_public_train)\n",
    "X_benign_public_train = X_public_train_scaled[y_public_train == 0]\n",
    "\n",
    "history = pretrain_ae_model.fit(\n",
    "    X_benign_public_train, X_benign_public_train,\n",
    "    epochs=EPOCHS_PRETRAIN,\n",
    "    batch_size=BATCH_PRETRAIN,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "print('\\nâœ… Pre-training complete.')\n",
    "pretrain_ae_model.save_weights('pretrained_ae_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBts0DBLAzA-"
   },
   "source": [
    "## 7. Fetch and Process User-Reported Data ðŸ“¥\n",
    "This cell connects to the Firebase database and **pulls down all fresh data** the browser extension has collected. This includes user feedback (links marked as \"phishing\" or \"safe\") and the graph data that shows relationships between users, posts, and domains. It then cleans this raw data, removes duplicates, and organizes it into a neat table for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26jrF0n2AzA-",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "def fetch_user_data(db_client):\n",
    "    \"\"\"Fetches and processes user reports and graph data from Firestore.\"\"\"\n",
    "    if not db_client:\n",
    "        print(\"Firestore client not initialized. Skipping data fetch.\")\n",
    "        return pd.DataFrame(), [], []\n",
    "\n",
    "    APP_ID = \"ads-phishing-link\"\n",
    "    REPORTS_PATH = f\"artifacts/{APP_ID}/private_user_reports\"\n",
    "    NODES_PATH = f\"artifacts/{APP_ID}/private/graph/nodes\"\n",
    "    EDGES_PATH = f\"artifacts/{APP_ID}/private/graph/edges\"\n",
    "\n",
    "    # Fetch Reports\n",
    "    print(f\"Fetching user reports from: {REPORTS_PATH}...\")\n",
    "    try:\n",
    "        # FIXED: Order by timestamp to process in a predictable, chronological order\n",
    "        reports_query = db_client.collection(REPORTS_PATH).order_by(\"timestamp\", direction=Query.DESCENDING)\n",
    "        report_docs = list(reports_query.stream())\n",
    "        print(f\"Found {len(report_docs)} total user reports.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching reports: {e}\")\n",
    "        report_docs = []\n",
    "\n",
    "    # Fetch Graph Data\n",
    "    print(f\"Fetching graph data...\")\n",
    "    try:\n",
    "        node_docs = list(db_client.collection(NODES_PATH).stream())\n",
    "        edge_docs = list(db_client.collection(EDGES_PATH).stream())\n",
    "        print(f\"Found {len(node_docs)} nodes and {len(edge_docs)} edges.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching graph data: {e}\")\n",
    "        node_docs, edge_docs = [], []\n",
    "\n",
    "    # Process reports into a DataFrame\n",
    "    processed_reports = []\n",
    "    for doc in report_docs:\n",
    "        d = doc.to_dict()\n",
    "        payload = d.get('payload', {})\n",
    "        # FIXED: Handle both 'url' and 'links' in payload\n",
    "        url = payload.get('url')\n",
    "        if not url:\n",
    "            links = payload.get('links')\n",
    "            if isinstance(links, list) and len(links) > 0:\n",
    "                url = links[0]\n",
    "\n",
    "        report = {\n",
    "            'url': url,\n",
    "            'postId': payload.get('postId'),\n",
    "            'type': d.get('type')\n",
    "        }\n",
    "        if not all(report.values()):\n",
    "            continue\n",
    "\n",
    "        report['label'] = 1 if report['type'] in ('true_positive', 'false_negative') else 0\n",
    "        processed_reports.append(report)\n",
    "\n",
    "    if not processed_reports:\n",
    "        print(\"No valid reports found.\")\n",
    "        return pd.DataFrame(), [], []\n",
    "\n",
    "    df_user = pd.DataFrame(processed_reports).drop_duplicates(subset=['url', 'postId']).reset_index(drop=True)\n",
    "    print(f\"Created a dataset of {len(df_user)} unique user-reported URL-post pairs.\")\n",
    "    print(\"User data label distribution:\")\n",
    "    print(df_user['label'].value_counts())\n",
    "\n",
    "    return df_user, node_docs, edge_docs\n",
    "\n",
    "# --- Firebase Authentication (if not already done) ---\n",
    "if 'db' not in locals() or db is None:\n",
    "    print(\"\\nPlease upload your Firebase service account JSON key file.\")\n",
    "    try:\n",
    "        uploaded = files.upload()\n",
    "        sa_path = next(iter(uploaded.keys()))\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
    "        db = firestore.Client()\n",
    "        print(\"\\nâœ… Firebase authentication configured.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Firebase authentication failed: {e}\")\n",
    "        db = None\n",
    "\n",
    "# Fetch all user data\n",
    "df_user_reports, node_docs, edge_docs = fetch_user_data(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "th7g94T7AzA-"
   },
   "source": [
    "## 8. Prepare Data for Cross-Validation and Fine-Tuning ðŸ“‹\n",
    "This code takes the cleaned user data from the previous step and **prepares it for the main training phase**. It uses the same feature engineering logic from section 5 to convert all user-reported URLs into numerical lists, ensuring they are in the exact same format as the public data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OmNC2J1AzA-",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "if not df_user_reports.empty:\n",
    "    # Create feature vectors for all user-reported URLs\n",
    "    # This step will now resolve shorteners as defined in the updated function\n",
    "    tqdm.pandas(desc=\"Feature Extraction (User Data)\")\n",
    "    user_feature_vectors = df_user_reports['url'].progress_apply(lambda url: create_feature_vector(url, LEXICAL_FEATURE_COLUMNS))\n",
    "    X_user = np.vstack(user_feature_vectors.values)\n",
    "\n",
    "    y_user = df_user_reports['label'].values\n",
    "    post_ids_user = df_user_reports['postId'].values\n",
    "\n",
    "    print(f\"User data prepared for cross-validation: X_user shape {X_user.shape}\")\n",
    "else:\n",
    "    print(\"User reports DataFrame is empty. Cannot proceed.\")\n",
    "    X_user, y_user, post_ids_user = [np.array([]) for _ in range(3)]\n",
    "    node_docs, edge_docs = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9YOG6awAzA-"
   },
   "source": [
    "## 9. The Cross-Validation and Fine-Tuning Loop ðŸ”\n",
    "This is the most important part of the notebook. It starts a loop to **train and test the model in a robust way**. \n",
    "\n",
    "- It **fine-tunes the Autoencoder (AE)** using all available safe links from the user training data for that round.\n",
    "- It **trains the Graph (GCN) model** using all user data for that round, but with class weights to handle imbalance.\n",
    "- It then tests both models on held-back data and finds the **best way to fuse their scores** for the highest accuracy.\n",
    "\n",
    "This process repeats 5 times, ensuring the final performance score is reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSigHiX0AzA-",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# --- GCN Model Definition ---\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_features, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "if len(X_user) > 0:\n",
    "    # --- Cross-Validation Setup ---\n",
    "    N_SPLITS = 5\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    # --- Hyperparameters ---\n",
    "    LR_FINETUNE = 1e-5\n",
    "    BATCH_FINETUNE = 16\n",
    "    EPOCHS_FINETUNE = 50\n",
    "    EPOCHS_GCN = 50\n",
    "\n",
    "    # --- Storage for results across folds ---\n",
    "    fold_results = []\n",
    "    best_thresholds = []\n",
    "    best_fusion_weights = []\n",
    "    all_confusion_matrices = []\n",
    "\n",
    "    # --- Build the full graph structure once ---\n",
    "    node_key_to_idx, idx_to_key, x_rows = {}, [], []\n",
    "    def add_node(key, doc):\n",
    "        if key in node_key_to_idx: return\n",
    "        idx = len(idx_to_key)\n",
    "        node_key_to_idx[key] = idx\n",
    "        idx_to_key.append(key)\n",
    "        node_type = (doc or {}).get(\"type\")\n",
    "        feats = [1.0, 0.0, 0.0] if node_type == \"user\" else [0.0, 1.0, 0.0] if node_type == \"domain\" else [0.0, 0.0, 1.0]\n",
    "        x_rows.append(feats)\n",
    "\n",
    "    for d in node_docs: add_node(d.id, d.to_dict())\n",
    "    edges = []\n",
    "    for d in edge_docs:\n",
    "        e = d.to_dict()\n",
    "        src, dst = e.get(\"src\"), e.get(\"dst\")\n",
    "        if src and dst and src in node_key_to_idx and dst in node_key_to_idx:\n",
    "            edges.append([node_key_to_idx[src], node_key_to_idx[dst]])\n",
    "            edges.append([node_key_to_idx[dst], node_key_to_idx[src]]) # Undirected\n",
    "\n",
    "    x_graph = torch.tensor(np.array(x_rows, dtype=np.float32)) if x_rows else torch.empty((0, 3))\n",
    "    edge_index_graph = torch.tensor(np.array(edges, dtype=np.int64)).t().contiguous() if edges else torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    # Map post IDs to their corresponding node indices\n",
    "    post_id_to_node_idx = {pid: node_key_to_idx.get(f\"post:{pid}\") for pid in np.unique(post_ids_user)}\n",
    "\n",
    "    # --- The Loop ---\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(skf.split(X_user, y_user)):\n",
    "        print(f\"\\n--- Starting Fold {fold_idx + 1}/{N_SPLITS} ---\")\n",
    "\n",
    "        # 1. Split data for this fold\n",
    "        X_train_fold, y_train_fold = X_user[train_indices], y_user[train_indices]\n",
    "        X_val_fold, y_val_fold = X_user[val_indices], y_user[val_indices]\n",
    "        posts_train_fold, posts_val_fold = post_ids_user[train_indices], post_ids_user[val_indices]\n",
    "\n",
    "        # ===================================\n",
    "        #     AUTOENCODER TRAINING\n",
    "        # ===================================\n",
    "\n",
    "        # 2. CORRECTED: Fine-tune AE only on the BENIGN data from the user training set for this fold\n",
    "        X_benign_finetune = X_train_fold[y_train_fold == 0]\n",
    "        X_benign_finetune_scaled = scaler.transform(X_benign_finetune)\n",
    "        X_val_scaled = scaler.transform(X_val_fold)\n",
    "\n",
    "        # 3. Build AE and load pre-trained weights\n",
    "        fine_tune_ae = build_autoencoder(X_user.shape[1])\n",
    "        fine_tune_ae.load_weights('pretrained_ae_weights.weights.h5')\n",
    "        fine_tune_ae.compile(optimizer=keras.optimizers.Adam(LR_FINETUNE), loss='mse')\n",
    "\n",
    "        # 4. Fine-tune AE\n",
    "        if len(X_benign_finetune_scaled) > 0:\n",
    "            fine_tune_ae.fit(X_benign_finetune_scaled, X_benign_finetune_scaled, epochs=EPOCHS_FINETUNE, batch_size=BATCH_FINETUNE, verbose=0, callbacks=[keras.callbacks.EarlyStopping(patience=5)])\n",
    "\n",
    "        # 5. Get AE reconstruction errors for the validation set\n",
    "        val_reconstructed = fine_tune_ae.predict(X_val_scaled, verbose=0)\n",
    "        val_errors_ae = np.mean(np.square(X_val_scaled - val_reconstructed), axis=1)\n",
    "\n",
    "        # 6. Find best AE threshold on validation set\n",
    "        precision, recall, thresholds = precision_recall_curve(y_val_fold, val_errors_ae)\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "        best_ae_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        best_thresholds.append(best_ae_threshold)\n",
    "        print(f\"Best AE Threshold found: {best_ae_threshold:.4f}\")\n",
    "\n",
    "        # ===================================\n",
    "        #         GCN TRAINING\n",
    "        # ===================================\n",
    "\n",
    "        # 7. Prepare graph data for this fold\n",
    "        y_nodes = torch.full((len(idx_to_key),), -1, dtype=torch.long)\n",
    "        for post_id, label in zip(post_ids_user, y_user):\n",
    "            node_idx = post_id_to_node_idx.get(post_id)\n",
    "            if node_idx is not None:\n",
    "                y_nodes[node_idx] = label\n",
    "\n",
    "        train_node_indices = [post_id_to_node_idx.get(pid) for pid in posts_train_fold if post_id_to_node_idx.get(pid) is not None]\n",
    "\n",
    "        train_mask = torch.zeros(len(idx_to_key), dtype=torch.bool)\n",
    "        if train_node_indices:\n",
    "            train_mask[torch.tensor(train_node_indices)] = True\n",
    "\n",
    "        graph_data = Data(x=x_graph, edge_index=edge_index_graph, y=y_nodes, train_mask=train_mask)\n",
    "\n",
    "        # 8. Train GCN\n",
    "        gcn_model = GCN(num_features=graph_data.x.size(1))\n",
    "        optimizer = optim.Adam(gcn_model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
    "\n",
    "        # Calculate class weights for imbalance\n",
    "        train_labels = graph_data.y[train_mask]\n",
    "        class_counts = torch.bincount(train_labels[train_labels >= 0], minlength=2)\n",
    "        if torch.all(class_counts > 0):\n",
    "            class_weights = 1. / class_counts.float()\n",
    "            class_weights = class_weights / class_weights.sum()\n",
    "            print(f\"Training GCN with class weights: {class_weights.numpy()}\")\n",
    "        else:\n",
    "            class_weights = None\n",
    "            print(\"Training GCN without class weights (one class missing in this training fold).\")\n",
    "\n",
    "        if train_mask.sum() > 0:\n",
    "            for epoch in range(EPOCHS_GCN):\n",
    "                gcn_model.train()\n",
    "                optimizer.zero_grad()\n",
    "                out = gcn_model(graph_data)\n",
    "                loss = F.nll_loss(out[train_mask], graph_data.y[train_mask], weight=class_weights)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # 9. Get GCN probabilities for all nodes\n",
    "        gcn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            all_node_logits = gcn_model(graph_data)\n",
    "            all_node_probs = all_node_logits.exp()[:, 1].cpu().numpy()\n",
    "\n",
    "        # ===================================\n",
    "        #     HYBRID FUSION & EVALUATION\n",
    "        # ===================================\n",
    "\n",
    "        # 10. Get AE and GCN scores for the validation set posts\n",
    "        df_val_fold = pd.DataFrame({'postId': posts_val_fold, 'ae_error': val_errors_ae, 'label': y_val_fold})\n",
    "        post_level_ae_scores = df_val_fold.groupby('postId')['ae_error'].max()\n",
    "        val_post_ids_unique = df_val_fold['postId'].unique()\n",
    "        val_labels_post_level_df = df_val_fold.groupby('postId')['label'].first()\n",
    "\n",
    "        # Align posts in validation set with posts in the graph\n",
    "        common_posts = [pid for pid in val_post_ids_unique if post_id_to_node_idx.get(pid) is not None]\n",
    "\n",
    "        if not common_posts:\n",
    "            print(\"Warning: No validation posts found in the graph for this fold. Skipping GCN/Hybrid evaluation.\")\n",
    "            fold_results.append({'f1_ae': f1_score(y_val_fold, (val_errors_ae > best_ae_threshold).astype(int)), 'f1_gcn': 0, 'f1_hybrid': 0, 'precision_hybrid': 0, 'recall_hybrid': 0, 'accuracy_hybrid': 0})\n",
    "            all_confusion_matrices.append(np.zeros((2,2), dtype=int))\n",
    "            continue\n",
    "\n",
    "        ae_scores_val = post_level_ae_scores.loc[common_posts].values\n",
    "        gcn_scores_val = np.array([all_node_probs[post_id_to_node_idx[pid]] for pid in common_posts])\n",
    "        val_labels_post_level = val_labels_post_level_df.loc[common_posts].values\n",
    "\n",
    "        # 11. Optimize Fusion Weight 'w'\n",
    "        best_w = 0.5\n",
    "        best_f1_fusion = -1\n",
    "        for w in np.linspace(0, 1, 21):\n",
    "            ae_prob = np.minimum(ae_scores_val / (best_ae_threshold * 2 + 1e-9), 1.0)\n",
    "            fused_scores = w * ae_prob + (1 - w) * gcn_scores_val\n",
    "            fused_preds = (fused_scores > 0.5).astype(int)\n",
    "            f1 = f1_score(val_labels_post_level, fused_preds, zero_division=0)\n",
    "            if f1 > best_f1_fusion:\n",
    "                best_f1_fusion = f1\n",
    "                best_w = w\n",
    "        best_fusion_weights.append(best_w)\n",
    "        print(f\"Best Fusion Weight (w for AE) found: {best_w:.2f}\")\n",
    "\n",
    "        # 12. Calculate and store final metrics for this fold\n",
    "        ae_preds_post_level = (ae_scores_val > best_ae_threshold).astype(int)\n",
    "        gcn_preds_post_level = (gcn_scores_val > 0.5).astype(int)\n",
    "        ae_prob_final = np.minimum(ae_scores_val / (best_ae_threshold * 2 + 1e-9), 1.0)\n",
    "        fused_scores_final = best_w * ae_prob_final + (1 - best_w) * gcn_scores_val\n",
    "        hybrid_preds_post_level = (fused_scores_final > 0.5).astype(int)\n",
    "\n",
    "        fold_metrics = {\n",
    "            'f1_ae': f1_score(val_labels_post_level, ae_preds_post_level, zero_division=0),\n",
    "            'f1_gcn': f1_score(val_labels_post_level, gcn_preds_post_level, zero_division=0),\n",
    "            'f1_hybrid': f1_score(val_labels_post_level, hybrid_preds_post_level, zero_division=0),\n",
    "            'precision_hybrid': precision_score(val_labels_post_level, hybrid_preds_post_level, zero_division=0),\n",
    "            'recall_hybrid': recall_score(val_labels_post_level, hybrid_preds_post_level, zero_division=0),\n",
    "            'accuracy_hybrid': accuracy_score(val_labels_post_level, hybrid_preds_post_level)\n",
    "        }\n",
    "        fold_results.append(fold_metrics)\n",
    "        all_confusion_matrices.append(confusion_matrix(val_labels_post_level, hybrid_preds_post_level))\n",
    "        print(f\"Hybrid Validation Metrics for Fold {fold_idx + 1}: {fold_metrics}\")\n",
    "else:\n",
    "    print(\"Cannot run cross-validation as there is no user data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRT40R3ZAzA_"
   },
   "source": [
    "## 10. Aggregate and Display Final Performance ðŸ“Š\n",
    "After the main loop is finished, this cell acts as the **reporter**. It gathers the results from all 5 rounds of testing and calculates the **average performance scores** (accuracy, precision, recall, and F1-score). It prints these scores in a table and also displays a **confusion matrix**, which is a simple chart that visually breaks down how many phishing links the model correctly caught, how many it missed, and how many times it made a false alarm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nDPDXflAzA_",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "if fold_results:\n",
    "    df_results = pd.DataFrame(fold_results)\n",
    "    print(\"\\n--- Cross-Validation Results Summary (F1-Scores) ---\")\n",
    "    print(df_results[['f1_ae', 'f1_gcn', 'f1_hybrid']])\n",
    "\n",
    "    print(\"\\n--- Average Performance Metrics (Â± Std Dev) --- \")\n",
    "    # Focus on the hybrid model's performance as per the thesis\n",
    "    hybrid_metrics = ['accuracy_hybrid', 'precision_hybrid', 'recall_hybrid', 'f1_hybrid']\n",
    "    mean_metrics = df_results[hybrid_metrics].mean()\n",
    "    std_metrics = df_results[hybrid_metrics].std()\n",
    "    summary_df = pd.concat([mean_metrics, std_metrics], axis=1)\n",
    "    summary_df.columns = ['Mean', 'Std Dev']\n",
    "    summary_df.index = ['Accuracy', 'Precision', 'Recall', 'F1-Score'] # Rename for clarity\n",
    "    print(summary_df)\n",
    "\n",
    "    # Summed Confusion Matrix\n",
    "    if all_confusion_matrices:\n",
    "        # Ensure all matrices are 2x2, padding if necessary\n",
    "        padded_matrices = []\n",
    "        for cm in all_confusion_matrices:\n",
    "            if cm.shape == (1, 1):\n",
    "                # This can happen if a fold only has one class\n",
    "                padded_cm = np.zeros((2, 2), dtype=int)\n",
    "                # Assuming the single class is the majority (negative) class\n",
    "                padded_cm[0, 0] = cm[0, 0]\n",
    "                padded_matrices.append(padded_cm)\n",
    "            elif cm.shape == (2,2):\n",
    "                padded_matrices.append(cm)\n",
    "\n",
    "        if padded_matrices:\n",
    "            summed_cm = np.sum(padded_matrices, axis=0)\n",
    "            print(\"\\n--- Summed Confusion Matrix (Across All Folds) ---\")\n",
    "            print(summed_cm)\n",
    "            # Optional: Plot the summed confusion matrix\n",
    "            plt.figure(figsize=(6, 5))\n",
    "            sns.heatmap(summed_cm, annot=True, fmt='d', cmap='Blues',\n",
    "                        xticklabels=['Predicted Benign', 'Predicted Phishing'],\n",
    "                        yticklabels=['True Benign', 'True Phishing'])\n",
    "            plt.title('Summed Confusion Matrix')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.show()\n",
    "\n",
    "    # Determine the final hyperparameters\n",
    "    final_threshold = np.mean(best_thresholds)\n",
    "    final_fusion_weight = np.mean(best_fusion_weights)\n",
    "    print(f\"\\nâœ… Final Optimized AE Threshold (Mean): {final_threshold:.6f}\")\n",
    "    print(f\"âœ… Final Optimized Fusion Weight (Mean): {final_fusion_weight:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo results to aggregate.\")\n",
    "    final_threshold = 0.5\n",
    "    final_fusion_weight = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEvmWuZYAzA_"
   },
   "source": [
    "## 11. Retrain Final Models on All User Data ðŸŽ“\n",
    "Now that the best settings and fusion weight have been discovered, this code performs **one final training run**. It uses **all of the available user data** to train the AE and GCN models one last time. This ensures the exported models are as smart and up-to-date as possible, having learned from every single piece of user feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-DUlpTVAzA_",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "if len(X_user) > 0:\n",
    "    # --- Final Autoencoder Training ---\n",
    "    print(\"\\n--- Training Final Autoencoder on ALL User Data ---\")\n",
    "    X_benign_user_final = X_user[y_user == 0]\n",
    "    X_benign_user_final_scaled = scaler.transform(X_benign_user_final)\n",
    "\n",
    "    final_ae_model = build_autoencoder(X_user.shape[1])\n",
    "    final_ae_model.load_weights('pretrained_ae_weights.weights.h5')\n",
    "    final_ae_model.compile(optimizer=keras.optimizers.Adam(LR_FINETUNE), loss='mse')\n",
    "\n",
    "    if len(X_benign_user_final_scaled) > 0:\n",
    "        final_ae_model.fit(X_benign_user_final_scaled, X_benign_user_final_scaled, epochs=EPOCHS_FINETUNE, batch_size=BATCH_FINETUNE, verbose=1, callbacks=[keras.callbacks.EarlyStopping(patience=5)])\n",
    "        print(\"\\nâœ… Final AE model training complete.\")\n",
    "    else:\n",
    "        print(\"No benign user data for final AE training. Using pre-trained model.\")\n",
    "        final_ae_model = pretrain_ae_model\n",
    "\n",
    "    # --- Final GCN Training ---\n",
    "    print(\"\\n--- Training Final GCN on ALL User Data ---\")\n",
    "    final_train_mask = torch.zeros(len(idx_to_key), dtype=torch.bool)\n",
    "    final_train_node_indices = [post_id_to_node_idx.get(pid) for pid in post_ids_user if post_id_to_node_idx.get(pid) is not None]\n",
    "    if final_train_node_indices:\n",
    "        final_train_mask[torch.tensor(final_train_node_indices)] = True\n",
    "\n",
    "    final_graph_data = Data(x=x_graph, edge_index=edge_index_graph, y=y_nodes, train_mask=final_train_mask)\n",
    "\n",
    "    final_gcn_model = GCN(num_features=final_graph_data.x.size(1))\n",
    "    optimizer = optim.Adam(final_gcn_model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
    "\n",
    "    final_train_labels = final_graph_data.y[final_train_mask]\n",
    "    final_class_counts = torch.bincount(final_train_labels[final_train_labels >= 0], minlength=2)\n",
    "    if torch.all(final_class_counts > 0):\n",
    "        final_class_weights = 1. / final_class_counts.float()\n",
    "        final_class_weights = final_class_weights / final_class_weights.sum()\n",
    "    else:\n",
    "        final_class_weights = None\n",
    "\n",
    "    if final_train_mask.sum() > 0:\n",
    "        for epoch in range(EPOCHS_GCN):\n",
    "            final_gcn_model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = final_gcn_model(final_graph_data)\n",
    "            loss = F.nll_loss(out[final_train_mask], final_graph_data.y[final_train_mask], weight=final_class_weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"\\nâœ… Final GCN model training complete.\")\n",
    "\n",
    "        # Generate final probabilities for all nodes\n",
    "        final_gcn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            final_all_node_logits = final_gcn_model(final_graph_data)\n",
    "            final_all_node_probs = final_all_node_logits.exp()[:, 1].cpu().numpy()\n",
    "        print(\"Generated final GCN probabilities for all nodes.\")\n",
    "    else:\n",
    "        print(\"No user data to train final GCN model.\")\n",
    "        final_gcn_model = None\n",
    "        final_all_node_probs = np.array([])\n",
    "else:\n",
    "    print(\"\\nNo user data. The final models will be the pre-trained public models.\")\n",
    "    final_ae_model = pretrain_ae_model\n",
    "    final_gcn_model = None\n",
    "    final_all_node_probs = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_73ajel0AzBA"
   },
   "source": [
    "## 12. Thesis Performance Evaluation ðŸ“\n",
    "This section gives the models their final exam. It evaluates the performance of the initial **pre-trained model** and the **final fine-tuned hybrid model** on the portion of the live dataset that was held back from training. This generates the specific scores and charts needed to report the results formally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TST3BIHBAzBA",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "def print_thesis_stats(model_name, y_true, y_pred, y_prob=None):\n",
    "    \"\"\"Prints evaluation statistics formatted for the thesis.\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n--- {model_name} Performance ---\")\n",
    "    print(f\"| Metric    | Score    |\")\n",
    "    print(f\"| :-------- | :------- |\")\n",
    "    print(f\"| Accuracy  | {accuracy:.4f}   |\")\n",
    "    print(f\"| Precision | {precision:.4f}   |\")\n",
    "    print(f\"| Recall    | {recall:.4f}   |\")\n",
    "    print(f\"| F1-Score  | {f1:.4f}   |\")\n",
    "    if y_prob is not None and len(np.unique(y_true)) > 1:\n",
    "        roc_auc = roc_auc_score(y_true, y_prob)\n",
    "        print(f\"| ROC-AUC   | {roc_auc:.4f}   |\")\n",
    "\n",
    "    print(f\"\\nConfusion Matrix for {model_name}:\")\n",
    "    print(cm)\n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Benign', 'Predicted Phishing'],\n",
    "                yticklabels=['True Benign', 'True Phishing'])\n",
    "    plt.title(f'Confusion Matrix: {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "# --- 1. Evaluate Pre-trained Autoencoder (Baseline) ---\n",
    "X_public_test_scaled = scaler.transform(X_public_test)\n",
    "baseline_reconstructed = pretrain_ae_model.predict(X_public_test_scaled, verbose=0)\n",
    "baseline_errors = np.mean(np.square(X_public_test_scaled - baseline_reconstructed), axis=1)\n",
    "\n",
    "# Use a simple percentile threshold for the baseline model as per the original thesis approach\n",
    "X_benign_public_train_scaled = scaler.transform(X_public_train)\n",
    "baseline_train_reconstructed = pretrain_ae_model.predict(X_benign_public_train_scaled, verbose=0)\n",
    "baseline_train_errors = np.mean(np.square(X_benign_public_train_scaled - baseline_train_reconstructed), axis=1)\n",
    "baseline_threshold = np.percentile(baseline_train_errors, 99) # High precision threshold\n",
    "baseline_preds = (baseline_errors > baseline_threshold).astype(int)\n",
    "\n",
    "print_thesis_stats(\"Pre-trained Autoencoder (Baseline)\", y_public_test, baseline_preds)\n",
    "\n",
    "# --- 2. Evaluate Final Fine-Tuned Hybrid Model ---\n",
    "if len(X_user) > 0:\n",
    "    final_reconstructed = final_ae_model.predict(X_public_test_scaled, verbose=0)\n",
    "    final_ae_errors = np.mean(np.square(X_public_test_scaled - final_reconstructed), axis=1)\n",
    "    final_ae_probs = final_ae_errors / (final_threshold + 1e-9)\n",
    "\n",
    "    # For public data, GCN score is neutral as these posts are not in the user graph\n",
    "    final_gcn_probs = np.full_like(final_ae_probs, 0.5)\n",
    "\n",
    "    final_fused_scores = final_fusion_weight * final_ae_probs + (1 - final_fusion_weight) * final_gcn_probs\n",
    "    final_hybrid_preds = (final_fused_scores > 0.5).astype(int)\n",
    "\n",
    "    print_thesis_stats(\"Final Hybrid Model\", y_public_test, final_hybrid_preds, y_prob=final_fused_scores)\n",
    "else:\n",
    "    print(\"\\nSkipping final hybrid model evaluation as no user data was available for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGnvHT92AzBA"
   },
   "source": [
    "## 13. Export Artifacts for Application ðŸ“¦\n",
    "This cell takes the final, fully trained models and all their necessary settings (like the scaler and the optimal threshold) and **saves them into a collection of files**. These files are the \"brains\" of the operation. They are the artifacts that the backend server will load to start making live predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHAgIICFAzBA",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "print(\"\\n--- Exporting Final Artifacts ---\")\n",
    "\n",
    "# 1. Save AE model, scaler, and threshold\n",
    "final_ae_model.save(\"phishing_autoencoder_model.keras\")\n",
    "print(\"Saved phishing_autoencoder_model.keras\")\n",
    "\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Saved scaler.pkl\")\n",
    "\n",
    "with open(\"autoencoder_threshold.txt\", \"w\") as f:\n",
    "    f.write(str(final_threshold))\n",
    "print(f\"Saved autoencoder_threshold.txt\")\n",
    "\n",
    "# 2. Save GCN model and post-to-node mapping\n",
    "if final_gcn_model:\n",
    "    torch.save(final_gcn_model.state_dict(), \"gnn_model.pth\")\n",
    "    print(\"Saved gnn_model.pth\")\n",
    "    with open(\"post_node_map.json\", \"w\") as f:\n",
    "        json.dump(post_id_to_node_idx, f)\n",
    "    print(\"Saved post_node_map.json\")\n",
    "    # Save the final GCN probabilities\n",
    "    if 'final_all_node_probs' in locals() and final_all_node_probs.size > 0:\n",
    "        np.save(\"gnn_probs.npy\", final_all_node_probs)\n",
    "        print(\"Saved gnn_probs.npy\")\n",
    "\n",
    "# 3. Save fusion configuration\n",
    "fusion_config = {\n",
    "    'ae_threshold': float(final_threshold),\n",
    "    'fusion_weight_ae': float(final_fusion_weight)\n",
    "}\n",
    "with open(\"fusion_config.json\", \"w\") as f:\n",
    "    json.dump(fusion_config, f)\n",
    "print(\"Saved fusion_config.json\")\n",
    "\n",
    "print(\"\\nâœ… All artifacts exported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yJz4iYyAzBA"
   },
   "source": [
    "## 14. Download Artifacts â¬‡ï¸\n",
    "This final cell is a simple convenience script. It **triggers a download prompt in the browser** for every artifact file created in the previous step. This makes it easy to get the finished model files from the training environment to a local computer so they can be uploaded to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UtVMo5zAzBA",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "print(\"Preparing to download artifacts...\")\n",
    "artifacts_to_download = [\n",
    "    \"phishing_autoencoder_model.keras\",\n",
    "    \"scaler.pkl\",\n",
    "    \"autoencoder_threshold.txt\",\n",
    "    \"gnn_model.pth\",\n",
    "    \"post_node_map.json\",\n",
    "    \"fusion_config.json\",\n",
    "    \"gnn_probs.npy\"\n",
    "]\n",
    "\n",
    "for artifact in artifacts_to_download:\n",
    "    if os.path.exists(artifact):\n",
    "        print(f\"Downloading {artifact}...\")\n",
    "        files.download(artifact)\n",
    "    else:\n",
    "        print(f\"Skipping {artifact} as it was not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}