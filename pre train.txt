{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Robust Hybrid Phishing Detection Trainer (AE + GCN) v3**\n",
        "\n",
        "This notebook implements the complete, robust training pipeline for the hybrid phishing detection system, combining a content-based Autoencoder (AE) with a structure-based Graph Convolutional Network (GCN), as described in the thesis.\n",
        "\n",
        "**Key Features:**\n",
        "1.  **Hybrid Model Architecture:** Fully integrates the AE and GCN components.\n",
        "2.  **Cross-Validation (`StratifiedKFold`):** Employs k-fold cross-validation for statistically reliable performance evaluation of both the individual models and the final fused system.\n",
        "3.  **Balanced Fine-Tuning:** Prevents catastrophic forgetting in the AE by using a balanced mix of user-reported data and public phishing data during fine-tuning.\n",
        "4.  **Intelligent Threshold & Weight Optimization:** \n",
        "    - Automatically finds the optimal **anomaly threshold** for the AE using Precision-Recall curve analysis.\n",
        "    - Automatically finds the optimal **fusion weight** (`w`) to combine AE and GCN scores by maximizing the F1-score on validation data.\n",
        "5.  **GCN Class Imbalance Handling:** The GCN is trained using class weights to counteract the typically imbalanced nature of user reports.\n",
        "6.  **Comprehensive Reporting:** Final performance is reported as an average across all folds for three models: **AE-Only**, **GCN-Only**, and the **Optimized Hybrid Model**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "#### **1. Setup and Installations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core data science, deep learning, and cloud libraries\n",
        "!pip -q install pandas scikit-learn tensorflow tldextract google-cloud-firestore matplotlib seaborn\n",
        "\n",
        "# Install PyTorch and PyTorch Geometric for the GNN\n",
        "!pip -q install torch torch_geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "#### **2. Imports and Initial Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tldextract\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# TensorFlow for Autoencoder\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# PyTorch for GCN\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "# Scikit-learn for preprocessing, cross-validation, and metrics\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, \n",
        "    confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
        ")\n",
        "\n",
        "# Google Cloud for data fetching\n",
        "from google.colab import files\n",
        "from google.cloud import firestore\n",
        "\n",
        "# --- Reproducibility ---\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "print(\"Libraries imported and seed set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "### **Part 1: Data Loading and Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **3. Load Public Dataset and Authenticate with Firebase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The public dataset contains 111 lexical features extracted from URLs\n",
        "# FIXED: Removed markdown formatting from the URL string\n",
        "URL = \"[https://raw.githubusercontent.com/GregaVrbancic/Phishing-Dataset/master/dataset_full.csv](https://raw.githubusercontent.com/GregaVrbancic/Phishing-Dataset/master/dataset_full.csv)\"\n",
        "print(\"Downloading public dataset...\")\n",
        "df_public = pd.read_csv(URL)\n",
        "\n",
        "# Separate features (X) from the label (y)\n",
        "y_public = df_public[\"phishing\"].astype(int)\n",
        "X_public = df_public.drop(columns=[\"phishing\"]).astype(np.float32)\n",
        "feature_cols = X_public.columns.tolist()\n",
        "\n",
        "print(f\"Public dataset loaded with {X_public.shape[0]} samples and {X_public.shape[1]} features.\")\n",
        "\n",
        "# --- Firebase Authentication ---\n",
        "print(\"\\nPlease upload your Firebase service account JSON key file.\")\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "    sa_path = next(iter(uploaded.keys()))\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
        "    db = firestore.Client()\n",
        "    print(\"\\n✅ Firebase authentication configured.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Firebase authentication failed: {e}\")\n",
        "    db = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **4. Feature Engineering and Scaling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lexical_features(url: str) -> dict:\n",
        "    \"\"\"Extracts basic lexical features from a URL string.\"\"\"\n",
        "    features = {}\n",
        "    try:\n",
        "        ext = tldextract.extract(url)\n",
        "        domain = f\"{ext.domain}.{ext.suffix}\"\n",
        "        hostname = f\"{ext.subdomain}.{domain}\" if ext.subdomain else domain\n",
        "        \n",
        "        features['qty_dot_url'] = url.count('.')\n",
        "        features['qty_hyphen_url'] = url.count('-')\n",
        "        features['qty_underline_url'] = url.count('_')\n",
        "        features['qty_slash_url'] = url.count('/')\n",
        "        features['qty_questionmark_url'] = url.count('?')\n",
        "        features['qty_equal_url'] = url.count('=')\n",
        "        features['qty_at_url'] = url.count('@')\n",
        "        features['qty_and_url'] = url.count('&')\n",
        "        features['qty_exclamation_url'] = url.count('!')\n",
        "        features['qty_space_url'] = url.count(' ')\n",
        "        features['qty_tilde_url'] = url.count('~')\n",
        "        features['qty_comma_url'] = url.count(',')\n",
        "        features['qty_plus_url'] = url.count('+')\n",
        "        features['qty_asterisk_url'] = url.count('*')\n",
        "        features['qty_hashtag_url'] = url.count('#')\n",
        "        features['qty_dollar_url'] = url.count('$')\n",
        "        features['qty_percent_url'] = url.count('%')\n",
        "        features['qty_dot_domain'] = domain.count('.')\n",
        "        features['qty_hyphen_domain'] = domain.count('-')\n",
        "        features['qty_underline_domain'] = domain.count('_')\n",
        "        features['qty_at_domain'] = domain.count('@')\n",
        "        features['qty_vowels_domain'] = sum(1 for char in domain if char in 'aeiouAEIOU')\n",
        "        features['domain_length'] = len(domain)\n",
        "        features['domain_in_ip'] = 1 if all(part.isdigit() for part in domain.split('.')) else 0\n",
        "        features['server_client_domain'] = 1 if 'server' in domain or 'client' in domain else 0\n",
        "        \n",
        "    except Exception:\n",
        "        return {}\n",
        "    return features\n",
        "\n",
        "def create_feature_vector(url: str, all_feature_columns: list) -> np.ndarray:\n",
        "    \"\"\"Creates a full feature vector for a URL, filling missing values with 0.\"\"\"\n",
        "    lexical_feats = get_lexical_features(url)\n",
        "    feature_vector = np.array([lexical_feats.get(col, 0.0) for col in all_feature_columns], dtype=np.float32)\n",
        "    return feature_vector\n",
        "\n",
        "# Split public data to get a set for fitting the scaler\n",
        "X_public_train, _, y_public_train, _ = train_test_split(\n",
        "    X_public, y_public, test_size=0.3, random_state=SEED, stratify=y_public\n",
        ")\n",
        "\n",
        "# --- Feature Scaling ---\n",
        "print(\"Fitting RobustScaler on benign public training data...\")\n",
        "scaler = RobustScaler().fit(X_public_train[y_public_train == 0])\n",
        "\n",
        "print(\"✅ Scaler fitted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "### **Part 2: Autoencoder Pre-training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **5. Define and Pre-train the Autoencoder Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Autoencoder hyperparameters (from thesis)\n",
        "AE_LAYER1 = 64\n",
        "AE_LAYER2 = 32\n",
        "AE_BOTTLENECK = 16\n",
        "AE_DROPOUT = 0.1\n",
        "LR_PRETRAIN = 1e-3\n",
        "BATCH_PRETRAIN = 512\n",
        "EPOCHS_PRETRAIN = 30\n",
        "\n",
        "def build_autoencoder(input_shape):\n",
        "    \"\"\"Builds the Keras Autoencoder model.\"\"\"\n",
        "    inp = keras.Input(shape=(input_shape,))\n",
        "    x = keras.layers.Dense(AE_LAYER1, activation='relu')(inp)\n",
        "    x = keras.layers.Dropout(AE_DROPOUT)(x)\n",
        "    x = keras.layers.Dense(AE_LAYER2, activation='relu')(x)\n",
        "    z = keras.layers.Dense(AE_BOTTLENECK, activation='relu', name='bottleneck')(x)\n",
        "    x = keras.layers.Dense(AE_LAYER2, activation='relu')(z)\n",
        "    x = keras.layers.Dense(AE_LAYER1, activation='relu')(x)\n",
        "    out = keras.layers.Dense(input_shape)(x)\n",
        "    model = keras.Model(inp, out)\n",
        "    return model\n",
        "\n",
        "pretrain_ae_model = build_autoencoder(X_public.shape[1])\n",
        "pretrain_ae_model.compile(optimizer=keras.optimizers.Adam(LR_PRETRAIN), loss='mse')\n",
        "\n",
        "print('--- Pre-training Autoencoder on Public Dataset ---')\n",
        "X_public_train_scaled = scaler.transform(X_public_train)\n",
        "X_benign_public_train = X_public_train_scaled[y_public_train == 0]\n",
        "\n",
        "history = pretrain_ae_model.fit(\n",
        "    X_benign_public_train, X_benign_public_train,\n",
        "    epochs=EPOCHS_PRETRAIN,\n",
        "    batch_size=BATCH_PRETRAIN,\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "print('\\n✅ Pre-training complete.')\n",
        "pretrain_ae_model.save_weights('pretrained_ae_weights.weights.h5')\n",
        "print('Pre-trained model weights saved.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "### **Part 3: User Data and Graph Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **6. Fetch and Process User-Reported Data**\n",
        "This section fetches all necessary data from Firestore: the user reports (for AE fine-tuning and GCN labeling) and the graph structure (nodes and edges for the GCN)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_user_data(db_client):\n",
        "    \"\"\"Fetches and processes user reports and graph data from Firestore.\"\"\"\n",
        "    if not db_client:\n",
        "        print(\"Firestore client not initialized. Skipping data fetch.\")\n",
        "        return pd.DataFrame(), [], []\n",
        "\n",
        "    APP_ID = \"ads-phishing-link\"\n",
        "    REPORTS_PATH = f\"artifacts/{APP_ID}/private_user_reports\"\n",
        "    NODES_PATH = f\"artifacts/{APP_ID}/private/graph/nodes\"\n",
        "    EDGES_PATH = f\"artifacts/{APP_ID}/private/graph/edges\"\n",
        "    \n",
        "    # Fetch Reports\n",
        "    print(f\"Fetching user reports from: {REPORTS_PATH}...\")\n",
        "    try:\n",
        "        report_docs = list(db_client.collection(REPORTS_PATH).stream())\n",
        "        print(f\"Found {len(report_docs)} total user reports.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error fetching reports: {e}\")\n",
        "        report_docs = []\n",
        "    \n",
        "    # Fetch Graph Data\n",
        "    print(f\"Fetching graph data...\")\n",
        "    try:\n",
        "        node_docs = list(db_client.collection(NODES_PATH).stream())\n",
        "        edge_docs = list(db_client.collection(EDGES_PATH).stream())\n",
        "        print(f\"Found {len(node_docs)} nodes and {len(edge_docs)} edges.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error fetching graph data: {e}\")\n",
        "        node_docs, edge_docs = [], []\n",
        "\n",
        "    # Process reports into a DataFrame\n",
        "    processed_reports = []\n",
        "    for doc in report_docs:\n",
        "        d = doc.to_dict()\n",
        "        payload = d.get('payload', {})\n",
        "        report = {\n",
        "            'url': payload.get('url'),\n",
        "            'postId': payload.get('postId'),\n",
        "            'type': d.get('type')\n",
        "        }\n",
        "        if not all(report.values()):\n",
        "            continue\n",
        "        \n",
        "        report['label'] = 1 if report['type'] in ('true_positive', 'false_negative') else 0\n",
        "        processed_reports.append(report)\n",
        "\n",
        "    if not processed_reports:\n",
        "        print(\"No valid reports found.\")\n",
        "        return pd.DataFrame(), [], []\n",
        "        \n",
        "    df_user = pd.DataFrame(processed_reports).drop_duplicates(subset=['url', 'postId']).reset_index(drop=True)\n",
        "    print(f\"Created a dataset of {len(df_user)} unique user-reported URL-post pairs.\")\n",
        "    print(\"User data label distribution:\")\n",
        "    print(df_user['label'].value_counts())\n",
        "    \n",
        "    return df_user, node_docs, edge_docs\n",
        "\n",
        "# Fetch all data\n",
        "df_user_reports, node_docs, edge_docs = fetch_user_data(db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **7. Prepare Data for Cross-Validation and Fine-Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df_user_reports.empty:\n",
        "    # Create feature vectors for all user-reported URLs\n",
        "    X_user = np.array([create_feature_vector(url, feature_cols) for url in df_user_reports['url']])\n",
        "    y_user = df_user_reports['label'].values\n",
        "    post_ids_user = df_user_reports['postId'].values\n",
        "\n",
        "    # Get phishing examples from the public dataset to prevent forgetting\n",
        "    X_public_phish = X_public[y_public == 1]\n",
        "    \n",
        "    print(f\"User data prepared for cross-validation: X_user shape {X_user.shape}\")\n",
        "else:\n",
        "    print(\"User reports DataFrame is empty. Cannot proceed.\")\n",
        "    X_user, y_user, post_ids_user, X_public_phish = [np.array([]) for _ in range(4)]\n",
        "    node_docs, edge_docs = [], []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "### **Part 4: Hybrid Model Cross-Validation, Fine-Tuning, and Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **8. The Cross-Validation and Fine-Tuning Loop**\n",
        "This is the core of the robust training process. We iterate through each fold, training both the AE and GCN on the training portion and evaluating them on the validation portion to find the optimal AE threshold and fusion weight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- GCN Model Definition ---\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, num_features, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(num_features, 16)\n",
        "        self.conv2 = GCNConv(16, num_classes)\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "if len(X_user) > 0:\n",
        "    # --- Cross-Validation Setup ---\n",
        "    N_SPLITS = 5\n",
        "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
        "\n",
        "    # --- Hyperparameters ---\n",
        "    LR_FINETUNE = 1e-5\n",
        "    BATCH_FINETUNE = 16\n",
        "    EPOCHS_FINETUNE = 50\n",
        "    EPOCHS_GCN = 50\n",
        "\n",
        "    # --- Storage for results across folds ---\n",
        "    fold_results = []\n",
        "    best_thresholds = []\n",
        "    best_fusion_weights = []\n",
        "    all_confusion_matrices = []\n",
        "    \n",
        "    # --- Build the full graph structure once ---\n",
        "    node_key_to_idx, idx_to_key, x_rows = {}, [], []\n",
        "    def add_node(key, doc):\n",
        "        if key in node_key_to_idx: return\n",
        "        idx = len(idx_to_key)\n",
        "        node_key_to_idx[key] = idx\n",
        "        idx_to_key.append(key)\n",
        "        node_type = (doc or {}).get(\"type\")\n",
        "        feats = [1.0, 0.0, 0.0] if node_type == \"user\" else [0.0, 1.0, 0.0] if node_type == \"domain\" else [0.0, 0.0, 1.0]\n",
        "        x_rows.append(feats)\n",
        "    \n",
        "    for d in node_docs: add_node(d.id, d.to_dict())\n",
        "    edges = []\n",
        "    for d in edge_docs:\n",
        "        e = d.to_dict()\n",
        "        src, dst = e.get(\"src\"), e.get(\"dst\")\n",
        "        if src and dst and src in node_key_to_idx and dst in node_key_to_idx:\n",
        "            edges.append([node_key_to_idx[src], node_key_to_idx[dst]])\n",
        "            edges.append([node_key_to_idx[dst], node_key_to_idx[src]]) # Undirected\n",
        "    \n",
        "    x_graph = torch.tensor(np.array(x_rows, dtype=np.float32)) if x_rows else torch.empty((0, 3))\n",
        "    edge_index_graph = torch.tensor(np.array(edges, dtype=np.int64)).t().contiguous() if edges else torch.empty((2, 0), dtype=torch.long)\n",
        "    \n",
        "    # Map post IDs to their corresponding node indices\n",
        "    post_id_to_node_idx = {pid: node_key_to_idx.get(f\"post:{pid}\") for pid in np.unique(post_ids_user)}\n",
        "    \n",
        "    # --- The Loop ---\n",
        "    for fold_idx, (train_indices, val_indices) in enumerate(skf.split(X_user, y_user)):\n",
        "        print(f\"\\n--- Starting Fold {fold_idx + 1}/{N_SPLITS} ---\")\n",
        "\n",
        "        # 1. Split data for this fold\n",
        "        X_train_fold, X_val_fold = X_user[train_indices], X_user[val_indices]\n",
        "        y_train_fold, y_val_fold = y_user[train_indices], y_user[val_indices]\n",
        "        posts_train_fold, posts_val_fold = post_ids_user[train_indices], post_ids_user[val_indices]\n",
        "\n",
        "        # ===================================\n",
        "        #      AUTOENCODER TRAINING\n",
        "        # ===================================\n",
        "        \n",
        "        # 2. Create the balanced fine-tuning dataset\n",
        "        num_user_phish = np.sum(y_train_fold == 1)\n",
        "        if num_user_phish > 0:\n",
        "            sample_indices = np.random.choice(len(X_public_phish), size=num_user_phish, replace=False)\n",
        "            X_public_phish_sample = X_public_phish.iloc[sample_indices]\n",
        "            X_finetune = np.vstack([X_train_fold, X_public_phish_sample])\n",
        "            y_finetune = np.concatenate([y_train_fold, np.ones(num_user_phish)])\n",
        "        else:\n",
        "            X_finetune = X_train_fold\n",
        "            y_finetune = y_train_fold\n",
        "        \n",
        "        X_finetune_scaled = scaler.transform(X_finetune)\n",
        "        X_val_scaled = scaler.transform(X_val_fold)\n",
        "\n",
        "        # 3. Build AE and load pre-trained weights\n",
        "        fine_tune_ae = build_autoencoder(X_user.shape[1])\n",
        "        fine_tune_ae.load_weights('pretrained_ae_weights.weights.h5')\n",
        "        fine_tune_ae.compile(optimizer=keras.optimizers.Adam(LR_FINETUNE), loss='mse')\n",
        "\n",
        "        # 4. Fine-tune AE on BENIGN data\n",
        "        X_benign_finetune = X_finetune_scaled[y_finetune == 0]\n",
        "        if len(X_benign_finetune) > 0:\n",
        "            fine_tune_ae.fit(X_benign_finetune, X_benign_finetune, epochs=EPOCHS_FINETUNE, batch_size=BATCH_FINETUNE, verbose=0, callbacks=[keras.callbacks.EarlyStopping(patience=5)])\n",
        "        \n",
        "        # 5. Get AE reconstruction errors for the validation set\n",
        "        val_reconstructed = fine_tune_ae.predict(X_val_scaled, verbose=0)\n",
        "        val_errors_ae = np.mean(np.square(X_val_scaled - val_reconstructed), axis=1)\n",
        "\n",
        "        # 6. Find best AE threshold on validation set\n",
        "        precision, recall, thresholds = precision_recall_curve(y_val_fold, val_errors_ae)\n",
        "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "        best_ae_threshold = thresholds[np.argmax(f1_scores)]\n",
        "        best_thresholds.append(best_ae_threshold)\n",
        "        print(f\"Best AE Threshold found: {best_ae_threshold:.4f}\")\n",
        "\n",
        "        # ===================================\n",
        "        #         GCN TRAINING\n",
        "        # ===================================\n",
        "        \n",
        "        # 7. Prepare graph data for this fold\n",
        "        y_nodes = torch.full((len(idx_to_key),), -1, dtype=torch.long)\n",
        "        for post_id, label in zip(post_ids_user, y_user):\n",
        "            node_idx = post_id_to_node_idx.get(post_id)\n",
        "            if node_idx is not None:\n",
        "                y_nodes[node_idx] = label\n",
        "                \n",
        "        train_node_indices = [post_id_to_node_idx.get(pid) for pid in posts_train_fold if post_id_to_node_idx.get(pid) is not None]\n",
        "        \n",
        "        train_mask = torch.zeros(len(idx_to_key), dtype=torch.bool)\n",
        "        train_mask[torch.tensor(train_node_indices)] = True\n",
        "        \n",
        "        graph_data = Data(x=x_graph, edge_index=edge_index_graph, y=y_nodes, train_mask=train_mask)\n",
        "\n",
        "        # 8. Train GCN\n",
        "        gcn_model = GCN(num_features=graph_data.x.size(1))\n",
        "        optimizer = optim.Adam(gcn_model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
        "        \n",
        "        # FIXED: Robustly calculate class weights for imbalance\n",
        "        train_labels = graph_data.y[train_mask]\n",
        "        class_counts = torch.bincount(train_labels[train_labels >= 0], minlength=2)\n",
        "        if torch.all(class_counts > 0):\n",
        "            class_weights = 1. / class_counts.float()\n",
        "            class_weights = class_weights / class_weights.sum()\n",
        "            print(f\"Training GCN with class weights: {class_weights.numpy()}\")\n",
        "        else:\n",
        "            class_weights = None\n",
        "            print(\"Training GCN without class weights (one class missing in this training fold).\")\n",
        "        \n",
        "        for epoch in range(EPOCHS_GCN):\n",
        "            gcn_model.train()\n",
        "            optimizer.zero_grad()\n",
        "            out = gcn_model(graph_data)\n",
        "            loss = F.nll_loss(out[train_mask], graph_data.y[train_mask], weight=class_weights)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 9. Get GCN probabilities for all nodes\n",
        "        gcn_model.eval()\n",
        "        with torch.no_grad():\n",
        "            all_node_logits = gcn_model(graph_data)\n",
        "            all_node_probs = all_node_logits.exp()[:, 1].cpu().numpy()\n",
        "\n",
        "        # ===================================\n",
        "        #    HYBRID FUSION & EVALUATION\n",
        "        # ===================================\n",
        "        \n",
        "        # 10. Get AE and GCN scores for the validation set posts\n",
        "        df_val_fold = pd.DataFrame({'postId': posts_val_fold, 'ae_error': val_errors_ae, 'label': y_val_fold})\n",
        "        post_level_ae_scores = df_val_fold.groupby('postId')['ae_error'].max()\n",
        "        val_post_ids_unique = df_val_fold['postId'].unique()\n",
        "        val_labels_post_level_df = df_val_fold.groupby('postId')['label'].first()\n",
        "\n",
        "        # FIXED: Safely align posts in validation set with posts in the graph\n",
        "        common_posts = [pid for pid in val_post_ids_unique if post_id_to_node_idx.get(pid) is not None]\n",
        "\n",
        "        if not common_posts:\n",
        "            print(\"Warning: No validation posts found in the graph for this fold. Skipping GCN/Hybrid evaluation.\")\n",
        "            fold_results.append({'f1_ae': f1_score(y_val_fold, (val_errors_ae > best_ae_threshold).astype(int)), 'f1_gcn': 0, 'f1_hybrid': 0, 'precision_hybrid': 0, 'recall_hybrid': 0, 'accuracy_hybrid': 0})\n",
        "            all_confusion_matrices.append(np.zeros((2,2), dtype=int))\n",
        "            continue\n",
        "        \n",
        "        ae_scores_val = post_level_ae_scores.loc[common_posts].values\n",
        "        gcn_scores_val = np.array([all_node_probs[post_id_to_node_idx[pid]] for pid in common_posts])\n",
        "        val_labels_post_level = val_labels_post_level_df.loc[common_posts].values\n",
        "        \n",
        "        # 11. Optimize Fusion Weight 'w'\n",
        "        best_w = 0.5\n",
        "        best_f1_fusion = -1\n",
        "        for w in np.linspace(0, 1, 21):\n",
        "            ae_prob = val_errors_ae / (best_ae_threshold + 1e-9) # Normalize AE error to a probability-like score\n",
        "            fused_scores = w * ae_prob + (1 - w) * gcn_scores_val\n",
        "            fused_preds = (fused_scores > 0.5).astype(int)\n",
        "            f1 = f1_score(val_labels_post_level, fused_preds)\n",
        "            if f1 > best_f1_fusion:\n",
        "                best_f1_fusion = f1\n",
        "                best_w = w\n",
        "        best_fusion_weights.append(best_w)\n",
        "        print(f\"Best Fusion Weight (w for AE) found: {best_w:.2f}\")\n",
        "\n",
        "        # 12. Calculate and store final metrics for this fold\n",
        "        ae_preds_post_level = (ae_scores_val > best_ae_threshold).astype(int)\n",
        "        gcn_preds_post_level = (gcn_scores_val > 0.5).astype(int)\n",
        "        ae_prob_final = ae_scores_val / (best_ae_threshold + 1e-9)\n",
        "        fused_scores_final = best_w * ae_prob_final + (1 - best_w) * gcn_scores_val\n",
        "        hybrid_preds_post_level = (fused_scores_final > 0.5).astype(int)\n",
        "        \n",
        "        fold_metrics = {\n",
        "            'f1_ae': f1_score(val_labels_post_level, ae_preds_post_level, zero_division=0),\n",
        "            'f1_gcn': f1_score(val_labels_post_level, gcn_preds_post_level, zero_division=0),\n",
        "            'f1_hybrid': f1_score(val_labels_post_level, hybrid_preds_post_level, zero_division=0),\n",
        "            'precision_hybrid': precision_score(val_labels_post_level, hybrid_preds_post_level, zero_division=0),\n",
        "            'recall_hybrid': recall_score(val_labels_post_level, hybrid_preds_post_level, zero_division=0),\n",
        "            'accuracy_hybrid': accuracy_score(val_labels_post_level, hybrid_preds_post_level)\n",
        "        }\n",
        "        fold_results.append(fold_metrics)\n",
        "        all_confusion_matrices.append(confusion_matrix(val_labels_post_level, hybrid_preds_post_level))\n",
        "        print(f\"Hybrid Validation Metrics for Fold {fold_idx + 1}: {fold_metrics}\")\n",
        "else:\n",
        "    print(\"Cannot run cross-validation as there is no user data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **9. Aggregate and Display Final Performance**\n",
        "After completing all folds, we calculate the average and standard deviation of the performance metrics, and sum the confusion matrices to get an overall view of the hybrid model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if fold_results:\n",
        "    df_results = pd.DataFrame(fold_results)\n",
        "    print(\"\\n--- Cross-Validation Results Summary (F1-Scores) ---\")\n",
        "    print(df_results[['f1_ae', 'f1_gcn', 'f1_hybrid']])\n",
        "\n",
        "    print(\"\\n--- Average Performance Metrics (± Std Dev) --- \")\n",
        "    # Focus on the hybrid model's performance as per the thesis\n",
        "    hybrid_metrics = ['accuracy_hybrid', 'precision_hybrid', 'recall_hybrid', 'f1_hybrid']\n",
        "    mean_metrics = df_results[hybrid_metrics].mean()\n",
        "    std_metrics = df_results[hybrid_metrics].std()\n",
        "    summary_df = pd.concat([mean_metrics, std_metrics], axis=1)\n",
        "    summary_df.columns = ['Mean', 'Std Dev']\n",
        "    summary_df.index = ['Accuracy', 'Precision', 'Recall', 'F1-Score'] # Rename for clarity\n",
        "    print(summary_df)\n",
        "\n",
        "    # Summed Confusion Matrix\n",
        "    if all_confusion_matrices:\n",
        "        # Ensure all matrices are 2x2, padding if necessary\n",
        "        padded_matrices = []\n",
        "        for cm in all_confusion_matrices:\n",
        "            if cm.shape == (1, 1):\n",
        "                # This can happen if a fold only has one class\n",
        "                padded_cm = np.zeros((2, 2), dtype=int)\n",
        "                # Assuming the single class is the majority (negative) class\n",
        "                padded_cm[0, 0] = cm[0, 0]\n",
        "                padded_matrices.append(padded_cm)\n",
        "            elif cm.shape == (2,2):\n",
        "                padded_matrices.append(cm)\n",
        "        \n",
        "        if padded_matrices:\n",
        "            summed_cm = np.sum(padded_matrices, axis=0)\n",
        "            print(\"\\n--- Summed Confusion Matrix (Across All Folds) ---\")\n",
        "            print(summed_cm)\n",
        "            # Optional: Plot the summed confusion matrix\n",
        "            plt.figure(figsize=(6, 5))\n",
        "            sns.heatmap(summed_cm, annot=True, fmt='d', cmap='Blues', \n",
        "                        xticklabels=['Predicted Benign', 'Predicted Phishing'], \n",
        "                        yticklabels=['True Benign', 'True Phishing'])\n",
        "            plt.title('Summed Confusion Matrix')\n",
        "            plt.xlabel('Predicted Label')\n",
        "            plt.ylabel('True Label')\n",
        "            plt.show()\n",
        "    \n",
        "    # Determine the final hyperparameters\n",
        "    final_threshold = np.mean(best_thresholds)\n",
        "    final_fusion_weight = np.mean(best_fusion_weights)\n",
        "    print(f\"\\n✅ Final Optimized AE Threshold (Mean): {final_threshold:.6f}\")\n",
        "    print(f\"✅ Final Optimized Fusion Weight (Mean): {final_fusion_weight:.2f}\")\n",
        "else:\n",
        "    print(\"\\nNo results to aggregate.\")\n",
        "    final_threshold = 0.5\n",
        "    final_fusion_weight = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "### **Part 5: Final Model Training and Artifact Export**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **10. Retrain Final Models on All User Data**\n",
        "With the hyperparameters validated, we now train the final AE and GCN models on the **entire user dataset** to ensure they have learned from all available information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(X_user) > 0:\n",
        "    # --- Final Autoencoder Training ---\n",
        "    print(\"\\n--- Training Final Autoencoder on ALL User Data ---\")\n",
        "    num_user_phish_total = np.sum(y_user == 1)\n",
        "    if num_user_phish_total > 0:\n",
        "        sample_indices_final = np.random.choice(len(X_public_phish), size=num_user_phish_total, replace=False)\n",
        "        X_public_phish_sample_final = X_public_phish.iloc[sample_indices_final]\n",
        "        X_final_train_ae = np.vstack([X_user, X_public_phish_sample_final])\n",
        "        y_final_train_ae = np.concatenate([y_user, np.ones(num_user_phish_total)])\n",
        "    else:\n",
        "        X_final_train_ae = X_user\n",
        "        y_final_train_ae = y_user\n",
        "    \n",
        "    X_final_train_scaled_ae = scaler.transform(X_final_train_ae)\n",
        "    X_benign_final_train_ae = X_final_train_scaled_ae[y_final_train_ae == 0]\n",
        "\n",
        "    final_ae_model = build_autoencoder(X_user.shape[1])\n",
        "    final_ae_model.load_weights('pretrained_ae_weights.weights.h5')\n",
        "    final_ae_model.compile(optimizer=keras.optimizers.Adam(LR_FINETUNE), loss='mse')\n",
        "\n",
        "    if len(X_benign_final_train_ae) > 0:\n",
        "        final_ae_model.fit(X_benign_final_train_ae, X_benign_final_train_ae, epochs=EPOCHS_FINETUNE, batch_size=BATCH_FINETUNE, verbose=1, callbacks=[keras.callbacks.EarlyStopping(patience=5)])\n",
        "        print(\"\\n✅ Final AE model training complete.\")\n",
        "    else:\n",
        "        print(\"No benign user data for final AE training. Using pre-trained model.\")\n",
        "        final_ae_model = pretrain_ae_model\n",
        "        \n",
        "    # --- Final GCN Training ---\n",
        "    print(\"\\n--- Training Final GCN on ALL User Data ---\")\n",
        "    final_train_mask = torch.zeros(len(idx_to_key), dtype=torch.bool)\n",
        "    final_train_node_indices = [post_id_to_node_idx.get(pid) for pid in post_ids_user if post_id_to_node_idx.get(pid) is not None]\n",
        "    final_train_mask[torch.tensor(final_train_node_indices)] = True\n",
        "    \n",
        "    final_graph_data = Data(x=x_graph, edge_index=edge_index_graph, y=y_nodes, train_mask=final_train_mask)\n",
        "    \n",
        "    final_gcn_model = GCN(num_features=final_graph_data.x.size(1))\n",
        "    optimizer = optim.Adam(final_gcn_model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
        "    \n",
        "    final_train_labels = final_graph_data.y[final_train_mask]\n",
        "    final_class_counts = torch.bincount(final_train_labels[final_train_labels >= 0], minlength=2)\n",
        "    if torch.all(final_class_counts > 0):\n",
        "        final_class_weights = 1. / final_class_counts.float()\n",
        "        final_class_weights = final_class_weights / final_class_weights.sum()\n",
        "    else:\n",
        "        final_class_weights = None\n",
        "\n",
        "    for epoch in range(EPOCHS_GCN):\n",
        "        final_gcn_model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = final_gcn_model(final_graph_data)\n",
        "        loss = F.nll_loss(out[final_train_mask], final_graph_data.y[final_train_mask], weight=final_class_weights)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(\"\\n✅ Final GCN model training complete.\")\n",
        "else:\n",
        "    print(\"\\nNo user data. The final models will be the pre-trained public models.\")\n",
        "    final_ae_model = pretrain_ae_model\n",
        "    final_gcn_model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **11. Export Artifacts for Application**\n",
        "Finally, we save all the essential artifacts required by your backend application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Exporting Final Artifacts ---\")\n",
        "\n",
        "# 1. Save AE model, scaler, and threshold\n",
        "final_ae_model.save(\"phishing_autoencoder_model.keras\")\n",
        "print(\"Saved phishing_autoencoder_model.keras\")\n",
        "\n",
        "with open(\"scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(\"Saved scaler.pkl\")\n",
        "\n",
        "with open(\"autoencoder_threshold.txt\", \"w\") as f:\n",
        "    f.write(str(final_threshold))\n",
        "print(f\"Saved autoencoder_threshold.txt\")\n",
        "\n",
        "# 2. Save GCN model and post-to-node mapping\n",
        "if final_gcn_model:\n",
        "    torch.save(final_gcn_model.state_dict(), \"gnn_model.pth\")\n",
        "    print(\"Saved gnn_model.pth\")\n",
        "    with open(\"post_node_map.json\", \"w\") as f:\n",
        "        json.dump(post_id_to_node_idx, f)\n",
        "    print(\"Saved post_node_map.json\")\n",
        "\n",
        "# 3. Save fusion configuration\n",
        "fusion_config = {\n",
        "    'ae_threshold': final_threshold,\n",
        "    'fusion_weight_ae': final_fusion_weight\n",
        "}\n",
        "with open(\"fusion_config.json\", \"w\") as f:\n",
        "    json.dump(fusion_config, f)\n",
        "print(\"Saved fusion_config.json\")\n",
        "\n",
        "print(\"\\n✅ All artifacts exported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **12. Download Artifacts**\n",
        "This final cell will prompt you to download all the necessary files to your local machine. You can then upload these to your backend server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Preparing to download artifacts...\")\n",
        "artifacts_to_download = [\n",
        "    \"phishing_autoencoder_model.keras\",\n",
        "    \"scaler.pkl\",\n",
        "    \"autoencoder_threshold.txt\",\n",
        "    \"gnn_model.pth\",\n",
        "    \"post_node_map.json\",\n",
        "    \"fusion_config.json\"\n",
        "]\n",
        "\n",
        "for artifact in artifacts_to_download:\n",
        "    if os.path.exists(artifact):\n",
        "        print(f\"Downloading {artifact}...\")\n",
        "        files.download(artifact)\n",
        "    else:\n",
        "        print(f\"Skipping {artifact} as it was not found.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}