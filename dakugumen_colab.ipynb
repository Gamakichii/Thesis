{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (Colab) - run once\n",
        "%pip install -q tensorflow==2.13.0 keras==2.13.1 torch==2.3.1 torchvision==0.18.1 --index-url https://download.pytorch.org/whl/cpu\n",
        "%pip install -q torch_geometric -f https://data.pyg.org/whl/torch-2.3.1+cpu.html\n",
        "%pip install -q python-dotenv requests matplotlib seaborn scikit-learn pandas tldextract google-cloud-firestore\n",
        "print('Finished pip installs')\n",
        "\n",
        "# If a single command fails, re-run this cell -- Colab will show the failing line.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal imports and deterministic setup (small cell so it can be re-run after failures)\n",
        "import os, json, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "SEED = int(os.environ.get('TRAIN_SEED', '42'))\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "try:\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print('Imports done, seed=', SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thesis training stats printer\n",
        "# Tries to load common stats JSONs, otherwise falls back to parsing the Pre_Train_and_Fine_Tune_AE_+_GCN.ipynb notebook\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "\n",
        "def print_thesis_stats(nb_path='Pre_Train_and_Fine_Tune_AE_+_GCN.ipynb'):\n",
        "    candidates = ['ae_gcn_training_stats.json','training_stats.json','training_provenance.json','training_provenance.json']\n",
        "    for c in candidates:\n",
        "        if os.path.exists(c):\n",
        "            try:\n",
        "                with open(c,'r') as f:\n",
        "                    data = json.load(f)\n",
        "                print(f\"Loaded stats from {c}:\")\n",
        "                print(json.dumps(data, indent=2))\n",
        "                return\n",
        "            except Exception as e:\n",
        "                print('Failed to load', c, e)\n",
        "    # fallback: parse the notebook for numeric metrics\n",
        "    if os.path.exists(nb_path):\n",
        "        try:\n",
        "            with open(nb_path,'r', encoding='utf-8') as f:\n",
        "                nb = json.load(f)\n",
        "            text = '\\n'.join(''.join(cell.get('source',[])) for cell in nb.get('cells',[]))\n",
        "            patterns = [r'(?i)(ae\\s*loss\\s*[:=]\\s*\\d+\\.?\\d*)', r'(?i)(gcn\\s*loss\\s*[:=]\\s*\\d+\\.?\\d*)', r'(?i)(auc\\s*[:=]\\s*\\d+\\.?\\d*)', r'(?i)(accuracy\\s*[:=]\\s*\\d+\\.?\\d*)', r'(?i)(threshold\\s*[:=]\\s*\\d+\\.?\\d*)', r'(?i)(final\\s*loss\\s*[:=]\\s*\\d+\\.?\\d*)']\n",
        "            found = False\n",
        "            for p in patterns:\n",
        "                for m in re.findall(p, text):\n",
        "                    print(m)\n",
        "                    found = True\n",
        "            if not found:\n",
        "                print('No clear metric patterns found in', nb_path)\n",
        "        except Exception as e:\n",
        "            print('Failed to parse notebook', e)\n",
        "    else:\n",
        "        print('No stats JSON or notebook found; run training cell first to generate stats')\n",
        "\n",
        "# Call the helper so it prints immediately when run\n",
        "print_thesis_stats()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Declare model type (hybrid AE + GCN) so subsequent cells can reference it\n",
        "model_type = 'AE+GCN'\n",
        "print('Using hybrid model:', model_type)\n",
        "\n",
        "# When training cells run, ensure they print thesis-specific stats (losses, AUC, thresholds) to\n",
        "# either a JSON file named 'ae_gcn_training_stats.json' or to the notebook output so the above cell can\n",
        "# extract them for reporting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dakugumen Colab: Reproducible pipeline for AE + GCN hybrid phishing detector\n",
        "# This notebook installs dependencies, captures provenance, loads data, trains AE/GCN, trains a supervised\n",
        "# classifier, evaluates, and packages artifacts for deployment. It is designed to match the thesis methods.\n",
        "\n",
        "# ======= 0) Install (optional - run once) =======\n",
        "# Uncomment to install (Colab) - may take a while\n",
        "# !pip install -q tensorflow==2.13.0 keras==2.13.1 torch==2.3.1 torchvision==0.18.1 --index-url https://download.pytorch.org/whl/cpu\n",
        "# !pip install -q torch_geometric -f https://data.pyg.org/whl/torch-2.3.1+cpu.html\n",
        "# !pip install -q python-dotenv requests matplotlib seaborn scikit-learn pandas tldextract google-cloud-firestore\n",
        "\n",
        "# ======= 1) Imports & deterministic setup =======\n",
        "import os, json, hashlib, subprocess, shutil, zipfile, io\n",
        "from datetime import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tldextract\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ML frameworks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print('Python, numpy, pandas, TF versions:')\n",
        "print('python:', subprocess.check_output(['python','--version']).decode().strip())\n",
        "print('numpy', np.__version__, 'pandas', pd.__version__, 'tensorflow', tf.__version__)\n",
        "\n",
        "# Deterministic seed\n",
        "SEED = int(os.environ.get('TRAIN_SEED', '42'))\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "try:\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# provenance helper\n",
        "def save_provenance(extra=None, path='training_provenance.json'):\n",
        "    prov = {'seed': SEED, 'timestamp': datetime.utcnow().isoformat()+'Z'}\n",
        "    try:\n",
        "        prov['pip_freeze'] = subprocess.check_output(['python','-m','pip','freeze']).decode()\n",
        "    except Exception:\n",
        "        prov['pip_freeze'] = None\n",
        "    try:\n",
        "        prov['git_commit'] = subprocess.check_output(['git','rev-parse','--short','HEAD']).decode().strip()\n",
        "    except Exception:\n",
        "        prov['git_commit'] = None\n",
        "    if extra: prov.update(extra)\n",
        "    with open(path, 'w') as f:\n",
        "        json.dump(prov, f, indent=2)\n",
        "    print('Saved provenance to', path)\n",
        "\n",
        "# ======= 2) Data loading (public dataset) =======\n",
        "# Public dataset used in thesis (example). Replace URL if needed.\n",
        "PUBLIC_CSV = 'https://raw.githubusercontent.com/GregaVrbancic/Phishing-Dataset/master/dataset_full.csv'\n",
        "print('Downloading public dataset...')\n",
        "df_base = pd.read_csv(PUBLIC_CSV)\n",
        "print('Public dataset', df_base.shape)\n",
        "\n",
        "# Save initial provenance\n",
        "save_provenance({'public_rows': int(len(df_base))})\n",
        "\n",
        "# ======= 3) Feature extraction & preprocessing (shared function) =======\n",
        "from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode\n",
        "import re\n",
        "\n",
        "# Use same feature code as app.py\n",
        "def normalize_url_for_features(raw_url: str) -> str:\n",
        "    try:\n",
        "        p = urlparse(raw_url or '')\n",
        "        if p.query:\n",
        "            q = parse_qsl(p.query, keep_blank_values=True)\n",
        "            q_filtered = [(k,v) for k,v in q if not (k.lower()=='fbclid' or k.lower().startswith('utm_'))]\n",
        "            p = p._replace(query=urlencode(q_filtered))\n",
        "        return urlunparse(p)\n",
        "    except Exception:\n",
        "        return raw_url or ''\n",
        "\n",
        "# Stabilize features (log/clip) - same as notebook and app\n",
        "import math\n",
        "\n",
        "def stabilize_features_map(feat_dict: dict):\n",
        "    feat_dict['qty_questionmark_url'] = min(max(int(feat_dict.get('qty_questionmark_url',0)),0),3)\n",
        "    feat_dict['qty_slash_url'] = min(max(int(feat_dict.get('qty_slash_url',0)),0),10)\n",
        "    feat_dict['qty_dot_url'] = min(max(int(feat_dict.get('qty_dot_url',0)),0),6)\n",
        "    feat_dict['qty_hyphen_url'] = min(max(int(feat_dict.get('qty_hyphen_url',0)),0),10)\n",
        "    feat_dict['url_shortened'] = 1.0 if feat_dict.get('url_shortened',0) else 0.0\n",
        "    for key in ['file_length','directory_length','params_length','time_domain_activation','time_domain_expiration','ttl_hostname','asn_ip','time_response']:\n",
        "        if key in feat_dict and feat_dict[key] is not None:\n",
        "            try:\n",
        "                feat_dict[key] = float(math.log1p(max(0.0,float(feat_dict[key]))))\n",
        "            except Exception:\n",
        "                pass\n",
        "    return feat_dict\n",
        "\n",
        "# Compute lexical features (subset - must match feature_cols ordering)\n",
        "def compute_lexical(u: str):\n",
        "    u = normalize_url_for_features(u)\n",
        "    parts = tldextract.extract(u)\n",
        "    domain = '.'.join([p for p in [parts.subdomain, parts.domain, parts.suffix] if p])\n",
        "    path_q = u.split(domain,1)[-1] if domain and domain in u else ''\n",
        "    feats = {\n",
        "        'qty_dot_url': u.count('.'), 'qty_hyphen_url': u.count('-'), 'qty_underline_url': u.count('_'),\n",
        "        'qty_slash_url': u.count('/'), 'qty_questionmark_url': u.count('?'), 'qty_equal_url': u.count('='),\n",
        "        'qty_at_url': u.count('@'), 'qty_and_url': u.count('&'), 'qty_exclamation_url': u.count('!'),\n",
        "        'qty_space_url': u.count(' '), 'qty_tilde_url': u.count('~'), 'qty_comma_url': u.count(','),\n",
        "        'qty_plus_url': u.count('+'), 'qty_asterisk_url': u.count('*'), 'qty_hashtag_url': u.count('#'),\n",
        "        'qty_dollar_url': u.count('$'), 'qty_percent_url': u.count('%'), 'length_url': len(u)\n",
        "    }\n",
        "    # directory/file breakdown approx\n",
        "    directory = path_q.rsplit('/',1)[0] if '/' in path_q else ''\n",
        "    filepart = path_q.rsplit('/',1)[-1] if '/' in path_q else path_q\n",
        "    for prefix,s in [('directory',directory),('file',filepart)]:\n",
        "        feats[f'qty_dot_{prefix}'] = s.count('.')\n",
        "        feats[f'qty_hyphen_{prefix}'] = s.count('-')\n",
        "        feats[f'qty_underline_{prefix}'] = s.count('_')\n",
        "        feats[f'qty_slash_{prefix}'] = s.count('/')\n",
        "        feats[f'qty_questionmark_{prefix}'] = s.count('?')\n",
        "        feats[f'qty_equal_{prefix}'] = s.count('=')\n",
        "        feats[f'qty_at_{prefix}'] = s.count('@')\n",
        "        feats[f'qty_and_{prefix}'] = s.count('&')\n",
        "        feats[f'{prefix}_length'] = len(s)\n",
        "    # enrich with defaults\n",
        "    feats = stabilize_features_map(feats)\n",
        "    return feats\n",
        "\n",
        "# Build feature dataframe for initial public features if not present\n",
        "if 'feature_cols' not in globals():\n",
        "    # attempt to infer feature columns from df_base\n",
        "    feature_cols = [c for c in df_base.columns if c!='phishing']\n",
        "\n",
        "# ======= Save feature names for audit =======\n",
        "with open('feature_names.json','w') as f:\n",
        "    json.dump(feature_cols, f)\n",
        "print('Saved feature_names.json')\n",
        "\n",
        "# The rest of the notebook contains cells to train AE/GCN and produce artifacts (already present in this notebook)\n",
        "print('Preprocessing utilities ready. Continue to run remaining cells in the notebook to train models.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Data split & scaler fit (idempotent) ===\n",
        "# Loads public dataset, splits, and fits scaler on benign training set; saves scaler as scaler.pkl and scaler_final.pkl\n",
        "import os, json, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "\n",
        "PUBLIC_CSV = os.environ.get('PUBLIC_CSV', 'https://raw.githubusercontent.com/GregaVrbancic/Phishing-Dataset/master/dataset_full.csv')\n",
        "print('Loading public dataset from', PUBLIC_CSV)\n",
        "df_public = pd.read_csv(PUBLIC_CSV)\n",
        "print('Public data shape', df_public.shape)\n",
        "\n",
        "if 'phishing' not in df_public.columns:\n",
        "    raise RuntimeError('Public CSV must contain \"phishing\" label column')\n",
        "\n",
        "X = df_public.drop(columns=['phishing']).astype(float)\n",
        "y = df_public['phishing'].astype(int).values\n",
        "feature_cols = X.columns.tolist()\n",
        "\n",
        "# Splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state=SEED)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, stratify=y_train, random_state=SEED)\n",
        "print('Data splits:', X_train.shape, X_val.shape, X_test.shape)\n",
        "\n",
        "# Fit scaler on benign portion of training set (per thesis)\n",
        "use_standard = os.environ.get('STANDARD_SCALER','0').lower() in ('1','true','yes')\n",
        "if use_standard:\n",
        "    scaler = StandardScaler().fit(X_train[y_train==0])\n",
        "    scaler_type = 'StandardScaler'\n",
        "else:\n",
        "    scaler = RobustScaler().fit(X_train[y_train==0])\n",
        "    scaler_type = 'RobustScaler'\n",
        "\n",
        "# Transform\n",
        "X_train_s = scaler.transform(X_train)\n",
        "X_val_s = scaler.transform(X_val)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "# Save scaler for backend compatibility\n",
        "with open('scaler.pkl','wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "with open('scaler_final.pkl','wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print('Saved scaler.pkl and scaler_final.pkl (type=', scaler_type, ')')\n",
        "\n",
        "# Persist feature names\n",
        "with open('feature_names.json','w') as f:\n",
        "    json.dump(feature_cols, f)\n",
        "print('Saved feature_names.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Autoencoder model definition and training (pretrain + fine-tune) ===\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "input_dim = X_train_s.shape[1]\n",
        "print('Building AE with input_dim=', input_dim)\n",
        "\n",
        "def build_autoencoder(input_dim, latent_dim=None):\n",
        "    if latent_dim is None:\n",
        "        latent_dim = max(8, input_dim // 4)\n",
        "    inp = keras.Input(shape=(input_dim,))\n",
        "    x = layers.Dense(max(64, input_dim//2), activation='relu')(inp)\n",
        "    x = layers.Dense(latent_dim, activation='relu')(x)\n",
        "    x = layers.Dense(max(64, input_dim//2), activation='relu')(x)\n",
        "    out = layers.Dense(input_dim, activation='linear')(x)\n",
        "    model = keras.Model(inp, out, name='autoencoder')\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
        "    return model\n",
        "\n",
        "ae = build_autoencoder(input_dim)\n",
        "\n",
        "epochs_pre = int(os.environ.get('AE_PRE_EPOCHS','10'))\n",
        "epochs_fine = int(os.environ.get('AE_FINE_EPOCHS','5'))\n",
        "batch_size = int(os.environ.get('AE_BATCH', '256'))\n",
        "\n",
        "# Pretrain on all public training data (or only benign if preferred)\n",
        "print('Pretraining AE...')\n",
        "try:\n",
        "    hist_pre = ae.fit(X_train_s, X_train_s, validation_data=(X_val_s, X_val_s), epochs=epochs_pre, batch_size=batch_size, verbose=2)\n",
        "    print('Pretrain complete')\n",
        "except Exception as e:\n",
        "    print('Pretrain failed:', e)\n",
        "\n",
        "# Fine-tune on benign samples only (if any)\n",
        "try:\n",
        "    benign_idx = np.where(y_train==0)[0]\n",
        "    if len(benign_idx) > 0:\n",
        "        print('Fine-tuning on', len(benign_idx), 'benign samples')\n",
        "        hist_fine = ae.fit(X_train_s[benign_idx], X_train_s[benign_idx], validation_data=(X_val_s[y_val==0], X_val_s[y_val==0]) if any(y_val==0) else (X_val_s, X_val_s), epochs=epochs_fine, batch_size=batch_size, verbose=2)\n",
        "    else:\n",
        "        print('No benign samples for fine-tuning; skipping')\n",
        "except Exception as e:\n",
        "    print('Fine-tune failed:', e)\n",
        "\n",
        "print('AE training finished')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Compute reconstruction threshold, save AE model and threshold ===\n",
        "import numpy as np\n",
        "\n",
        "# Compute reconstruction errors on validation benign set\n",
        "val_benign = X_val_s[y_val==0]\n",
        "if val_benign.shape[0] == 0:\n",
        "    val_benign = X_val_s\n",
        "recons = ae.predict(val_benign)\n",
        "errors = np.mean(np.square(val_benign - recons), axis=1)\n",
        "thr = float(np.mean(errors) + 3.0 * np.std(errors))\n",
        "print('Computed AE threshold:', thr)\n",
        "\n",
        "# Save model and threshold\n",
        "try:\n",
        "    ae.save('phishing_autoencoder_model.keras')\n",
        "    print('Saved phishing_autoencoder_model.keras')\n",
        "except Exception as e:\n",
        "    print('Failed to save AE model:', e)\n",
        "\n",
        "with open('autoencoder_threshold.txt','w') as f:\n",
        "    f.write(str(thr))\n",
        "print('Saved autoencoder_threshold.txt')\n",
        "\n",
        "# Ensure scaler files exist\n",
        "import pickle\n",
        "if not os.path.exists('scaler.pkl'):\n",
        "    with open('scaler.pkl','wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "if not os.path.exists('scaler_final.pkl'):\n",
        "    with open('scaler_final.pkl','wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "print('Ensured scaler.pkl and scaler_final.pkl exist')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === GCN artifacts creation & training (adapted from thesis notebook) ===\n",
        "# Builds a heterogeneous graph from user reports (posts/domains/users), trains a small GCN to predict\n",
        "# phishing probability per post node, and saves `gnn_probs.npy` and `post_node_map.json` for the backend.\n",
        "import os, json\n",
        "import numpy as np\n",
        "\n",
        "gnn_probs_path = 'gnn_probs.npy'\n",
        "post_node_map_path = 'post_node_map.json'\n",
        "\n",
        "def _write_neutral_artifacts():\n",
        "    probs = np.full(1, 0.5)\n",
        "    np.save(gnn_probs_path, probs)\n",
        "    with open(post_node_map_path, 'w') as f:\n",
        "        json.dump({}, f)\n",
        "    print('Wrote neutral GCN artifacts')\n",
        "\n",
        "try:\n",
        "    # Prefer real training if user-provided reports dataframe exists\n",
        "    if 'df_user' in globals() and not df_user.empty:\n",
        "        print('Building graph from df_user with', len(df_user), 'rows')\n",
        "        try:\n",
        "            import torch\n",
        "            import torch.nn.functional as F\n",
        "            from torch import nn, optim\n",
        "            from torch_geometric.data import Data\n",
        "            from torch_geometric.nn import GCNConv\n",
        "        except Exception as e:\n",
        "            print('Required torch/torch_geometric not installed or failed to import:', e)\n",
        "            _write_neutral_artifacts()\n",
        "            raise\n",
        "\n",
        "        # Build node index mapping: create unique node keys for posts, domains, users\n",
        "        idx_to_key = []\n",
        "        key_to_idx = {}\n",
        "        def add_node(key):\n",
        "            if key in key_to_idx:\n",
        "                return key_to_idx[key]\n",
        "            i = len(idx_to_key)\n",
        "            idx_to_key.append(key)\n",
        "            key_to_idx[key] = i\n",
        "            return i\n",
        "\n",
        "        edges = []\n",
        "        post_keys = []\n",
        "        labels_by_post = {}\n",
        "\n",
        "        # Helper to extract domain\n",
        "        from urllib.parse import urlparse\n",
        "        def domain_of(url):\n",
        "            try:\n",
        "                p = urlparse(url)\n",
        "                host = p.netloc or p.path\n",
        "                return host.lower()\n",
        "            except Exception:\n",
        "                return ''\n",
        "\n",
        "        # Build nodes and edges from df_user. Expect df_user to have 'post_id', 'url', 'user_id', 'label' (0/1)\n",
        "        for _, r in df_user.iterrows():\n",
        "            post_id = str(r.get('post_id') or r.get('postId') or r.get('id') or '')\n",
        "            url = r.get('url') or r.get('link') or ''\n",
        "            user_id = str(r.get('userId') or r.get('user_id') or r.get('user') or 'anon')\n",
        "            label = r.get('label') if 'label' in r.index else None\n",
        "\n",
        "            if not post_id and not url:\n",
        "                continue\n",
        "            # Node keys\n",
        "            post_key = f'post:{post_id}' if post_id else f'post_url:{url}'\n",
        "            dom = domain_of(url)\n",
        "            domain_key = f'domain:{dom}' if dom else None\n",
        "            user_key = f'user:{user_id}'\n",
        "\n",
        "            pidx = add_node(post_key)\n",
        "            post_keys.append(post_key)\n",
        "            if domain_key:\n",
        "                didx = add_node(domain_key)\n",
        "                edges.append((pidx, didx))\n",
        "                edges.append((didx, pidx))\n",
        "            uidx = add_node(user_key)\n",
        "            edges.append((uidx, pidx))\n",
        "            edges.append((pidx, uidx))\n",
        "\n",
        "            # record label if present\n",
        "            try:\n",
        "                if label is not None and (int(label) == 0 or int(label) == 1):\n",
        "                    labels_by_post[post_key] = int(label)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        if len(idx_to_key) == 0:\n",
        "            print('No graph nodes constructed from df_user; writing neutral artifacts')\n",
        "            _write_neutral_artifacts()\n",
        "        else:\n",
        "            # Prepare node features: for post nodes use scaled lexical features (if url present), else zeros\n",
        "            num_nodes = len(idx_to_key)\n",
        "            feat_dim = X_train_s.shape[1] if 'X_train_s' in globals() else len(feature_cols)\n",
        "            x = np.zeros((num_nodes, feat_dim), dtype=float)\n",
        "            y_nodes = np.zeros((num_nodes,), dtype=float)\n",
        "            labeled_mask = np.zeros((num_nodes,), dtype=bool)\n",
        "\n",
        "            for i, key in enumerate(idx_to_key):\n",
        "                if key.startswith('post:') or key.startswith('post_url:'):\n",
        "                    # try to recover URL from df_user mapping\n",
        "                    # Search df_user for matching post_key\n",
        "                    url = None\n",
        "                    if key.startswith('post:'):\n",
        "                        pid = key.split(':',1)[1]\n",
        "                        matches = df_user[(df_user.get('post_id')==pid) | (df_user.get('postId')==pid)] if 'post_id' in df_user.columns or 'postId' in df_user.columns else df_user[df_user.index==int(pid)] if pid.isdigit() else df_user[df_user['url'].str.contains(pid, na=False)]\n",
        "                        if not matches.empty:\n",
        "                            url = matches.iloc[0].get('url')\n",
        "                    else:\n",
        "                        # post_url key\n",
        "                        # try to find row containing the url\n",
        "                        candidates = df_user[df_user['url'].notnull() & df_user['url'].str.contains(key.split(':',1)[1], na=False)] if 'url' in df_user.columns else None\n",
        "                        if candidates is not None and not candidates.empty:\n",
        "                            url = candidates.iloc[0].get('url')\n",
        "                    if url and 'scaler' in globals():\n",
        "                        try:\n",
        "                            from numpy import array\n",
        "                            feats = np.array(list(compute_lexical(url).values()), dtype=float)\n",
        "                            # Align features to scaler's expected feature order if possible\n",
        "                            # Attempt to transform via scaler by building DataFrame\n",
        "                            import pandas as pd\n",
        "                            df_tmp = pd.DataFrame([compute_lexical(url)])\n",
        "                            cols_expected = list(getattr(scaler, 'feature_names_in_', []))\n",
        "                            if cols_expected:\n",
        "                                row_vals = [float(df_tmp.iloc[0][c]) if c in df_tmp.columns else 0.0 for c in cols_expected]\n",
        "                                x[i] = np.array(row_vals)\n",
        "                            else:\n",
        "                                # fallback: trim/pad to feat_dim\n",
        "                                arr = np.array(list(df_tmp.iloc[0].values()), dtype=float)\n",
        "                                if arr.size >= feat_dim:\n",
        "                                    x[i] = arr[:feat_dim]\n",
        "                                else:\n",
        "                                    tmp = np.zeros(feat_dim, dtype=float)\n",
        "                                    tmp[:arr.size] = arr\n",
        "                                    x[i] = tmp\n",
        "                        except Exception:\n",
        "                            x[i] = np.zeros((feat_dim,), dtype=float)\n",
        "                    else:\n",
        "                        x[i] = np.zeros((feat_dim,), dtype=float)\n",
        "                else:\n",
        "                    # domain/user nodes: zeros\n",
        "                    x[i] = np.zeros((feat_dim,), dtype=float)\n",
        "\n",
        "                # labels\n",
        "                if key in labels_by_post:\n",
        "                    y_nodes[i] = labels_by_post[key]\n",
        "                    labeled_mask[i] = True\n",
        "\n",
        "            # Build edge_index\n",
        "            if edges:\n",
        "                import torch\n",
        "                edge_index = torch.tensor(np.array(edges, dtype=np.int64).T, dtype=torch.long).contiguous()\n",
        "            else:\n",
        "                import torch\n",
        "                edge_index = torch.empty((2,0), dtype=torch.long)\n",
        "\n",
        "            # Train/test masks for labeled nodes\n",
        "            labeled_idx = np.where(labeled_mask)[0]\n",
        "            if labeled_idx.size == 0:\n",
        "                print('No labeled nodes available for supervised GCN training; skipping training and writing neutral artifacts')\n",
        "                _write_neutral_artifacts()\n",
        "            else:\n",
        "                # create boolean masks\n",
        "                rng = np.random.RandomState(SEED)\n",
        "                perm = rng.permutation(labeled_idx)\n",
        "                n_train = max(1, int(0.8 * len(perm)))\n",
        "                train_idx = perm[:n_train]\n",
        "                test_idx = perm[n_train:]\n",
        "\n",
        "                train_mask = np.zeros((num_nodes,), dtype=bool)\n",
        "                test_mask = np.zeros((num_nodes,), dtype=bool)\n",
        "                train_mask[train_idx] = True\n",
        "                test_mask[test_idx] = True\n",
        "\n",
        "                # Build PyG Data\n",
        "                import torch\n",
        "                data = Data(x=torch.tensor(x, dtype=torch.float), edge_index=edge_index, y=torch.tensor(y_nodes, dtype=torch.float), train_mask=torch.tensor(train_mask), test_mask=torch.tensor(test_mask))\n",
        "\n",
        "                # Define simple GCN\n",
        "                class GCN(nn.Module):\n",
        "                    def __init__(self, num_features, num_classes=1):\n",
        "                        super(GCN, self).__init__()\n",
        "                        self.conv1 = GCNConv(num_features, 16)\n",
        "                        self.conv2 = GCNConv(16, num_classes)\n",
        "                    def forward(self, d):\n",
        "                        x, ei = d.x, d.edge_index\n",
        "                        x = F.relu(self.conv1(x, ei))\n",
        "                        x = self.conv2(x, ei).squeeze(-1)\n",
        "                        return x\n",
        "\n",
        "                # Instantiate and train\n",
        "                num_features = x.shape[1]\n",
        "                gnn_model = GCN(num_features=num_features)\n",
        "                optimizer = optim.Adam(gnn_model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
        "                criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "                epochs = int(os.environ.get('GCN_EPOCHS', '50'))\n",
        "                print('Training GCN for', epochs, 'epochs on', int(train_mask.sum()), 'train labeled nodes')\n",
        "                for ep in range(epochs):\n",
        "                    gnn_model.train()\n",
        "                    optimizer.zero_grad()\n",
        "                    out = gnn_model(data)\n",
        "                    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    if ep % 10 == 0 or ep == epochs-1:\n",
        "                        gnn_model.eval()\n",
        "                        with torch.no_grad():\n",
        "                            logits = gnn_model(data)\n",
        "                            preds = torch.sigmoid(logits[data.test_mask]).cpu().numpy() if data.test_mask.any() else np.array([])\n",
        "                            print(f'GCN epoch {ep}/{epochs} loss={loss.item():.4f} test_pos_mean={(preds.mean() if preds.size>0 else float(\"nan\"))}')\n",
        "\n",
        "                # Save model weights and node probs for post nodes\n",
        "                try:\n",
        "                    torch.save(gnn_model.state_dict(), 'gnn_model.pth')\n",
        "                except Exception as e:\n",
        "                    print('Warning: failed to save gnn_model.pth:', e)\n",
        "\n",
        "                # Compute probability per node\n",
        "                gnn_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    logits_all = gnn_model(data)\n",
        "                    probs_all = torch.sigmoid(logits_all).cpu().numpy()\n",
        "\n",
        "                # Save post node map and probs (extract only post nodes mapping to post IDs)\n",
        "                post_node_map = {}\n",
        "                post_probs = []\n",
        "                for i, key in enumerate(idx_to_key):\n",
        "                    if key.startswith('post:') or key.startswith('post_url:'):\n",
        "                        # key format: post:ID or post_url:URL\n",
        "                        # map original post id where available (post:ID) else use URL hash\n",
        "                        if key.startswith('post:'):\n",
        "                            pid = key.split(':',1)[1]\n",
        "                            post_node_map[str(pid)] = i\n",
        "                        else:\n",
        "                            # fallback id is the index-based key\n",
        "                            post_node_map[key] = i\n",
        "                        post_probs.append(float(probs_all[i]))\n",
        "\n",
        "                # Save artifacts\n",
        "                np.save(gnn_probs_path, np.array(post_probs, dtype=float))\n",
        "                with open(post_node_map_path, 'w') as f:\n",
        "                    json.dump(post_node_map, f)\n",
        "                print('Saved trained GCN artifacts:', gnn_probs_path, post_node_map_path)\n",
        "    else:\n",
        "        print('No df_user available; writing neutral GCN artifacts')\n",
        "        _write_neutral_artifacts()\n",
        "except Exception as e:\n",
        "    print('GCN pipeline failed:', e)\n",
        "    _write_neutral_artifacts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Supervised classifier training, evaluation, and stats export ===\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "# Build content_score using AE errors on the training set\n",
        "recon_tr = ae.predict(X_train_s)\n",
        "errors_tr = np.mean(np.square(X_train_s - recon_tr), axis=1)\n",
        "thr_used = float(open('autoencoder_threshold.txt').read())\n",
        "content_scores_tr = np.minimum(errors_tr / (thr_used * 2.0), 1.0)\n",
        "\n",
        "# Use neutral GCN probabilities unless map exists\n",
        "try:\n",
        "    with open('post_node_map.json') as f:\n",
        "        post_map = json.load(f)\n",
        "    gcn_p_tr = np.full(len(content_scores_tr), 0.5)\n",
        "except Exception:\n",
        "    gcn_p_tr = np.full(len(content_scores_tr), 0.5)\n",
        "\n",
        "clf_X = np.hstack([content_scores_tr.reshape(-1,1), gcn_p_tr.reshape(-1,1), X_train_s])\n",
        "clf_y = y_train\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(clf_X, clf_y)\n",
        "\n",
        "# Save classifier and meta\n",
        "with open('phishing_classifier.pkl','wb') as f:\n",
        "    pickle.dump(clf, f)\n",
        "meta = {'feature_names': ['content_score','gcn_prob'] + feature_cols, 'model': 'LogisticRegression'}\n",
        "with open('classifier_meta.json','w') as f:\n",
        "    json.dump(meta, f)\n",
        "print('Saved phishing_classifier.pkl and classifier_meta.json')\n",
        "\n",
        "# Evaluate on test set\n",
        "recon_te = ae.predict(X_test_s)\n",
        "errors_te = np.mean(np.square(X_test_s - recon_te), axis=1)\n",
        "content_scores_te = np.minimum(errors_te / (thr_used * 2.0), 1.0)\n",
        "gcn_p_te = np.full(len(content_scores_te), 0.5)\n",
        "clf_X_te = np.hstack([content_scores_te.reshape(-1,1), gcn_p_te.reshape(-1,1), X_test_s])\n",
        "probs = clf.predict_proba(clf_X_te)[:,1]\n",
        "auc = float(roc_auc_score(y_test, probs))\n",
        "acc = float(accuracy_score(y_test, probs > 0.5))\n",
        "\n",
        "stats = {'ae_threshold': thr_used, 'ae_pretrain_epochs': epochs_pre, 'ae_finetune_epochs': epochs_fine, 'classifier_auc': auc, 'classifier_accuracy': acc}\n",
        "with open('ae_gcn_training_stats.json','w') as f:\n",
        "    json.dump(stats, f, indent=2)\n",
        "\n",
        "print('Training stats saved to ae_gcn_training_stats.json')\n",
        "print(json.dumps(stats, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Export & zip artifacts for deployment ===\n",
        "# Zips backend artifacts into `artifacts_for_deploy.zip` and attempts to download via Colab's files.download\n",
        "import os, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "artifacts = [\n",
        "    'phishing_autoencoder_model.keras', 'scaler.pkl', 'scaler_final.pkl', 'autoencoder_threshold.txt',\n",
        "    'post_node_map.json', 'gnn_probs.npy', 'gnn_model.pth',\n",
        "    'phishing_classifier.pkl', 'classifier_meta.json', 'ae_gcn_training_stats.json',\n",
        "    'artifact_manifest.json', 'MODEL_CARD.md', 'feature_names.json'\n",
        "]\n",
        "zip_name = 'artifacts_for_deploy.zip'\n",
        "\n",
        "with shutil.ZipFile(zip_name, 'w') as zf:\n",
        "    for a in artifacts:\n",
        "        if os.path.exists(a):\n",
        "            zf.write(a)\n",
        "            print('Added to zip:', a)\n",
        "        else:\n",
        "            print('Missing (skipped):', a)\n",
        "\n",
        "print('\\nBuilt', zip_name)\n",
        "\n",
        "# Attempt Colab download if available\n",
        "try:\n",
        "    from google.colab import files\n",
        "    try:\n",
        "        files.download(zip_name)\n",
        "    except Exception as e:\n",
        "        print('Colab files.download failed:', e)\n",
        "        print('You can download the zip from the Files sidebar or use gdown / drive integration.')\n",
        "except Exception:\n",
        "    print('Not running in Colab or google.colab.files not available — zip saved in notebook working directory.')\n",
        "\n",
        "print('\\nArtifacts ready. Copy `artifacts_for_deploy.zip` to your backend and POST /reload_models or upload files directly to the server directory.')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
